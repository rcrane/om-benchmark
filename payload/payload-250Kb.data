.  Finally, the
action of formant filters can be synthesized in the time domain.  This gives
eight possibilities which are summarized in Table 5.2.
.RF
.in +0.5i
.ta 2.1i +2.0i
.nr x1 (\w'Analogue'/2)
.nr x2 (\w'Digital'/2)
	\h'-\n(x1u'Analogue	\h'-\n(x2u'Digital
.nr x0 2.0i+(\w'Liljencrants (1968)'/2)+(\w'Morris and Paillet (1972)'/2)
.nr x3 (\w'Liljencrants (1968)'/2)
	\h'-\n(x3u'\l'\n(x0u\(ul'
.sp
.nr x1 (\w'Rice (1976)'/2)
.nr x2 (\w'Rabiner \fIet al\fR'/2)
Series	\h'-\n(x1u'Rice (1976)	\h'-\n(x2u'Rabiner \fIet al\fR
.nr x1 (\w'Liljencrants (1968)'/2)
.nr x2 (\w'Holmes (1973)'/2)
Parallel	\h'-\n(x1u'Liljencrants (1968)	\h'-\n(x2u'Holmes (1973)
.nr x1 (\w'unpublished'/2)
.nr x2 (\w'unpublished'/2
Time-domain	\h'-\n(x1u'unpublished	\h'-\n(x2u'unpublished
.nr x1 (\w'\(em'/2)
.nr x2 (\w'Morris and Paillet (1972)'/2)
High-order filter	\h'-\n(x1u'\(em	\h'-\n(x2u'Morris and Paillet (1972)
	\h'-\n(x3u'\l'\n(x0u\(ul'
.ta 0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i
.in-0.5i
.FG "Table 5.2  Implementation options for resonance speech synthesizers"
.[
Rice 1976 Byte
.]
.[
Rabiner Jackson Schafer Coker 1971
.]
.[
Liljencrants 1968
.]
.[
Holmes 1973 Influence of glottal waveform on naturalness
.]
.[
Morris and Paillet 1972
.]
All but one have certainly been used as the basis for synthesis, and
the table includes reference to published descriptions.
.pp
Each method has advantages and disadvantages.  Series decomposition obviates
the need for control over the amplitudes of individual formants, but does
not allow synthesis of sounds which use the nasal tract as well as the oral
one; for these are in parallel.  Analogue implementation of series synthesizers
is complicated by the need for higher-pole correction, and the fact that
the gains at different frequencies can vary widely throughout the system.
Higher-pole correction is not so important for digital synthesizers.
Parallel decomposition eliminates some of these problems:  higher-pole correction
can be implemented individually for each formant.  However, the formant
amplitudes must be controlled rather precisely to simulate the vocal tract,
which is essentially serial.
Time-domain synthesis is associated with low hardware costs but does not
easily allow proper control over the excitation sources.  In particular,
it cannot simulate dynamical movement of the spectrum during aspiration.
Implementation of the entire vocal tract model as a single high-order filter,
without breaking it down into individual formants in series or parallel,
is attractive from the computational point of view because less arithmetic
operations are required.  It is best analysed in terms of linear predictive
coding, which is the subject of the next chapter.
.sh "5.6  References"
.LB "nnnn"
.[
$LIST$
.]
.LE "nnnn"
.sh "5.7  Further reading"
.pp
Historically-minded readers should look at the early speech synthesizer
designed by Lawrence (1953).
This and other classic papers on the subject
are reprinted in Flanagan and Rabiner (1973).
A good description of a quite sophisticated parallel synthesizer can
be found in Holmes (1973), above, and another of a switchable
series/parallel one in Klatt (1980), who even includes a listing of
the Fortran program that implements it.
Here are some useful books on speech synthesizers.
.LB "nn"
.\"Fant-1960-1
.]-
.ds [A Fant, G.
.ds [D 1960
.ds [T Acoustic theory of speech production
.ds [I Mouton
.ds [C The Hague
.nr [T 0
.nr [A 1
.nr [O 0
.][ 2 book
.in+2n
Fant really started the study of the vocal tract as an acoustic system,
and this book marks the beginning of modern speech synthesis.
.in-2n
.\"Flanagan-1972-1
.]-
.ds [A Flanagan, J.L.
.ds [D 1972
.ds [T Speech analysis, synthesis, and perception (2nd, expanded, edition)
.ds [I Springer Verlag
.ds [C Berlin
.nr [T 0
.nr [A 1
.nr [O 0
.][ 2 book
.in+2n
This book is the speech researcher's bible, and like the bible, it's not
all that easy to read.
However, it is an essential reference source for speech acoustics and
speech synthesis (as well as for human speech perception).
.in-2n
.\"Flanagan-1973-2
.]-
.ds [A Flanagan, J.L.
.as [A " and Rabiner, L.R.(Editors)
.ds [D 1973
.ds [T Speech synthesis
.ds [I Dowsen, Hutchinson and Ross
.ds [C Stroudsburg, Pennsylvania
.nr [T 0
.nr [A 0
.nr [O 0
.][ 2 book
.in+2n
I recommended this book at the end of Chapter 1 as a collection of
classic papers on the subject of speech synthesis and synthesizers.
.in-2n
.\"Holmes-1972-3
.]-
.ds [A Holmes, J.N.
.ds [D 1972
.ds [T Speech synthesis
.ds [I Mills and Boom
.ds [C London
.nr [T 0
.nr [A 1
.nr [O 0
.][ 2 book
.in+2n
This little book, by one of Britain's foremost workers in the field,
introduces the subject of speech synthesis and speech synthesizers.
It has a particularly good discussion of parallel synthesizers.
.in-2n
.LE "nn"
.EQ
delim $$
.EN
.CH "6  LINEAR PREDICTION OF SPEECH"
.ds RT "Linear prediction of speech
.ds CX "Principles of computer speech
.pp
The speech coding techniques which were discussed in Chapter 3 operate
in the time domain, while the analysis and synthesis techniques
of Chapters 4 and 5 are
based in the frequency domain.  Linear prediction is a relatively
new method of speech analysis-synthesis,
introduced in the early 1970's and used
extensively since then, which is primarily a time-domain coding method
but can be used to give frequency-domain parameters like formant
frequency, bandwidth, and amplitude.
.pp
It has several advantages over other speech analysis techniques, and is
likely to become increasingly dominant in speech output systems.
As well as bridging the gap between time- and frequency-domain techniques, it
is of equal value for both speech storage and speech synthesis, and forms
an extremely convenient basis for speech-output systems which use high-quality
stored speech for routine messages and synthesis from phonetics or text
for unusual or exceptional conditions.  Linear prediction can be used to
separate the excitation source properties of pitch and amplitude from the
vocal tract filter which governs phoneme articulation, or, in other words,
to separate much of the prosodic from the segmental information.
Hence it makes it easy to use stored segmentals with synthetic prosody,
which is just what is needed to enhance the flexibility of stored speech by
providing overall intonation contours for utterances formed by word
concatenation (see Chapter 7).
.pp
The frequency-domain analysis technique
of Fourier transformation necessarily involves approximation because it
applies only to periodic waveforms, and so the artificial operation
of windowing is required to suppress the aperiodicity of real
speech.  In contrast, the linear predictive technique, being a time-domain
method, can \(em in certain forms \(em deal more rationally with aperiodic
signals.
.pp
The basic idea of linear predictive coding is exactly the same as
one form of adaptive differential pulse code modulation which
was introduced briefly in Chapter 3.  There it was noted that a speech
sample $x(n)$ can be predicted quite closely by the previous sample
$x(n-1)$.  The prediction can be improved by multiplying the previous
sample by a number, say $a sub 1$, which is adapted on a syllabic
time-scale.  This can be utilized for speech coding by transmitting
only the prediction error
.LB
.EQ
e(n)~=~~x(n)~-~a sub 1 x(n-1),
.EN
.LE
and using it (and the value of $a sub 1$) to reconstitute the signal
$x(n)$ at the receiver.  It is worthwhile noting that
exactly the same relationship was used for digital
preemphasis in Chapter 4, with the value of $a sub 1$
being constant at about 0.9 \(em although
the possibility of adapting it to take into account the difference
between voiced and unvoiced speech was discussed.
.pp
An obvious extension is to use several past values of the signal to form
the prediction, instead of just one.  Different multipliers for each would
be needed, so that the prediction error could be written as
.LB
.EQ
e(n)~~ mark =~~x(n)~-~a sub 1 x(n-1)~-~a sub 2 x(n-2)~-~...~-~a sub p x(n-p)
.EN
.sp
.EQ
lineup =~~x(n)~-~~sum from k=1 to p ~a sub k x(n-k).
.EN
.LE
The multipliers $a sub k$ should be adapted to minimize the error signal,
and we will consider how to do this in the next section.  It turns out
that they must be re-calculated and transmitted on a time-scale that is
rather faster than syllabic but much slower than
the basic sampling rate:  intervals
of 10\-25\ msec are usually used (compare this with the 125\ $mu$sec sampling
rate for telephone-quality speech).
A configuration for high-order adaptive differential
pulse code modulation is shown in Figure 6.1.
.FC "Figure 6.1"
.pp
Figure 6.2 shows typical time waveforms for each of the ten coefficients
over a 1-second stretch of speech.
.FC "Figure 6.2"
Notice that they vary much more slowly than, say, the speech waveform of
Figure 3.5.
.pp
Turning the above relationship into $z$-transforms gives
.LB
.EQ
E(z)~~=~~X(z)~-~~sum from k=1 to p ~a sub k z sup -k ~X(z)~~=~~(1~-~~
sum from k=1 to p ~a sub k z sup -k )~X(z).
.EN
.LE
Rewriting the speech signal in terms of the error,
.LB
.EQ
X(z)~~=~~1 over {1~-~~ sum ~a sub k z sup -k }~.~E(z) .
.EN
.LE
.pp
Now let us bring together some facts from the previous chapter which will
allow the time-domain technique of linear prediction to be interpreted
in terms of the frequency-domain formant model of speech.  Recall that speech
can be viewed as an excitation source passing through a vocal tract filter,
followed by another filter to model the effect of radiation from the lips.
The overall spectral levels can be reassigned as in Figure 5.1 so that
the excitation source has a 0\ dB/octave spectral profile, and hence is
essentially impulsive.
Considering the vocal tract filter as a series connection
of digital formant filters, its transfer function is the product of terms like
.LB
.EQ
1 over {1~-~b sub 1 z sup -1 ~+~b sub 2 z sup -2}~ ,
.EN
.LE
where $b sub 1$ and $b sub 2$ control the position and bandwidth of the formant resonances.
The \-6\ dB/octave spectral compensation can be modelled by the
first-order digital filter
.LB
.EQ
1 over {1~-~bz sup -1}~ .
.EN
.LE
The product of all these terms, when multiplied out, will have the
form
.LB
.EQ
1 over {1~-~c sub 1 z sup -1 ~-~c sub 2 z sup -2 ~-~...~-~
c sub q z sup -q }~ ,
.EN
.LE
where $q$ is twice the number of formants plus one, and the $c$'s are calculated
from the positions and bandwidths of the formant resonances and the spectral
compensation parameter.  Hence
the $z$-transform of the speech is
.LB
.EQ
X(z)~=~~1 over {1~-~~ sum from k=1 to q ~c sub k z sup -k }~.~I(z) ,
.EN
.LE
where $I(z)$ is the transform of the impulsive excitation.
.pp
This is remarkably similar to the linear prediction relation given earlier!  If
$p$ and $q$ are the same, then the linear predictive coefficients $a sub k$
form a $p$'th order polynomial which is the same as that obtained by multiplying
together the second-order polynomials representing the individual formants
(together with the first-order one for spectral compensation).
Furthermore, the predictive error $E(z)$ can be identified with the
impulsive excitation $I(z)$.  This raises the very interesting
possibility of parametrizing the error signal by its frequency and
amplitude \(em two relatively slowly-varying quantities \(em instead of
transmitting it sample-by-sample (at an 8\ kHz rate).  This is how
linear prediction separates out the excitation properties of the source
from the vocal tract filter:  the source parameters can be derived
from the error signal and the vocal tract filter is represented by
the linear predictive coefficients.
Figure 6.3 shows how this can be used for speech transmission.
.FC "Figure 6.3"
Note that
.ul
no
signals need now be transmitted at the speech sampling rate; for the
source parameters vary relatively slowly.  This leads to an extremely
low data rate.
.pp
Practical linear predictive coding schemes operate with a value of $p$ between
10 and 15, corresponding approximately to 4-formant and 7-formant synthesis
respectively.  The $a sub k$'s are re-calculated every 10 to 25\ msec, and
transmitted to the receiver.  Also, the pitch and amplitude
of the speech are estimated and transmitted at the same rate.
If the speech
is unvoiced, there is no pitch value:  an "unvoiced flag" is
transmitted instead.
Because the linear predictive coefficients are intimately related to
formant frequencies and bandwidths, a "frame rate" in the region
of 10 to 25\ msec is appropriate because this approximates the maximum rate
at which acoustic events happen in speech production.
.pp
At the receiver, the excitation waveform
is reconstituted.
For voiced speech, it is impulsive at the specified
frequency and with the specified amplitude, while for unvoiced speech it
is random, with the specified amplitude.  This signal $e(n)$, together
with the transmitted parameters $a sub 1$, ..., $a sub p$, is used
to regenerate the speech waveform by
.LB
.EQ
x(n)~=~~e(n)~+~~sum from k=1 to p ~a sub k x(n-k) ,
.EN
.LE
\(em which is the inverse of the transmitter's formula for calculating $e(n)$,
namely
.LB
.EQ
e(n)~=~~x(n)~-~~sum from k=1 to p ~a sub k x(n-k) .
.EN
.LE
This relies on knowing the past $p$ values of the speech samples.
Many systems set these past values to zero at the beginning of each pitch
cycle.
.pp
Linear prediction can also be used for speech analysis, rather than
for speech coding, as shown in Figure 6.4.
.FC "Figure 6.4"
Instead of transmitting the coefficients $a sub k$,
they are used to determine the formant positions and bandwidths.
We saw above that the polynomial
.LB
.EQ
1~-~a sub 1 z sup -1 ~-~a sub 2 z sup -2 ~-~...~-~a sub p z sup -p ,
.EN
.LE
when factored into a product of second-order terms, gives the formant
characteristics (as well as the spectral compensation term).
Factoring is equivalent to finding the complex roots of the polynomial,
and this is fairly demanding computationally \(em especially if done at
a high rate.  Consequently, peak-picking algorithms are sometimes
used instead.  The absolute value of the polynomial gives the
frequency spectrum of the vocal tract filter, and the formants
appear as peaks \(em just as they do in cepstrally smoothed speech
(see Chapter 4).
.pp
The chief deficiency in the linear predictive method, whether it
is used for speech coding or for speech analysis, is that \(em like a series
synthesizer \(em it
implements an all-pole model of the vocal tract.
We mentioned in Chapter 5 that this is rather simplistic,
especially for nasalized sounds which involve a cavity in parallel
with the oral one.  Some research has been done on incorporating zeros
into a linear predictive model, but it complicates the problem of
calculating the parameters enormously.  For most purposes people seem
to be able to live with the limitations of the all-pole model.
.sh "6.1  Linear predictive analysis"
.pp
The key problem in linear predictive coding is to determine the values
of the coefficients $a sub 1$, ..., $a sub p$.
If the error signal is to be transmitted on a sample-by-sample basis,
as it is in adaptive differential pulse code modulation, then it can be most
economically encoded if its mean power is as small as possible.
Thus the coefficients are chosen to minimize
.LB
.EQ
sum ~e(n) sup 2
.EN
.LE
over some period of time.
The period of time used is related to the frame rate at which the
coefficients are transmitted or stored, although there is no need
to make it exactly the same as one frame interval.  As mentioned above,
the frame size
is usually chosen to be in the region of 10 to 25\ msec.  Some
schemes minimize the error signal over as few as 30 samples
(corresponding to 3\ msec at a 10\ kHz sampling rate).  Others take
longer; up to 250 samples (25\ msec).
.pp
However, if the error signal is to be considered as impulsive and
parametrized by its frequency and amplitude before transmission,
or if the coefficients $a sub k$ are to be used for spectral calculations,
then it is not immediately obvious how the coefficients should be
calculated.
In fact, it is still best to choose them to minimize the above sum.
This is at least plausible, for an impulsive excitation will have a
rather small mean power \(em most of the samples are zero.
It can be justified theoretically in terms of
.ul
spectral whitening,
for it can be shown that minimizing the mean-squared error
produces an error signal whose spectrum is maximally flat.
Now the only two waveforms whose spectra are absolutely flat
are a single impulse and white noise.  Hence if
the speech is voiced, minimizing the mean-squared error
will lead to an error signal which is as nearly impulsive
as possible.  Provided the time-frame for minimizing is short enough,
the impulse will correspond to a single excitation pulse.
If the speech is unvoiced, minimization will lead to an error
signal which is as nearly white noise as possible.
.pp
How does one choose the linear predictive coefficients to minimize
the mean-squared error?  The total squared prediction error is
.LB
.EQ
M~=~~sum from n ~e(n) sup 2~~=~~sum from n
~[x(n)~-~ sum from k=1 to p ~a sub k x sub n-k ] sup 2 ,
.EN
.LE
leaving the range of summation unspecified for the moment.
To minimize $M$ by choice of the coefficients $a sub j$, differentiate
with respect to each of them and set the resulting derivatives
to zero.
.LB
.EQ
dM over {da sub j} ~~=~~-2 sum from n ~x(n-j)[x(n)~-~~
sum from k=1 to p ~a sub k x(n-k)]~~=~0~,
.EN
.LE
so
.LB
.EQ
sum from k=1 to p ~a sub k ~ sum from n ~x(n-j)x(n-k)~~=~~
sum from n ~x(n)x(n-j)~~~~j~=~1,~2,~...,~p.
.EN
.LE
.pp
This is a set of $p$ linear equations for the $p$ unknowns $a sub 1$, ...,
$a sub p$.
Solving it is equivalent to inverting a $p times p$ matrix.
This job must be repeated at the frame rate, and so if
real-time operation is desired quite a lot of calculation is needed.
.rh "The autocorrelation method."
So far, the range of the $n$-summation has been left open.  The
coefficients of the matrix equation have the form
.LB
.EQ
sum from n ~x(n-j)x(n-k).
.EN
.LE
If a doubly-infinite summation were made, with $x(n)$ being defined
as zero whenever $n<0$, we could make use of the fact that
.sp
.ce
.EQ
sum from {n=- infinity} to infinity ~x(n-j)x(n-k)~=~~
sum from {n=- infinity} to infinity ~x(n-j+1)x(n-k+1)~=~...~=~~
sum from {n=- infinity} to infinity ~x(n)x(n+j-k)
.EN
.sp
to simplify the matrix equation.  This just states that the
autocorrelation of an infinite sequence depends only on the lag at which
it is computed, and not on absolute time.
.pp
Defining $R(m)$ as the
autocorrelation at lag $m$, that is,
.LB
.EQ
R(m)~=~ sum from n ~x(n)x(n+m),
.EN
.LE
the matrix equation becomes
.LB
.ne7
.nf
.EQ
R(0)a sub 1 ~+~R(1)a sub 2 ~+~R(2)a sub 3 ~+~...~~=~R(1)
.EN
.EQ
R(1)a sub 1 ~+~R(0)a sub 2 ~+~R(1)a sub 3 ~+~...~~=~R(2)
.EN
.EQ
R(2)a sub 1 ~+~R(1)a sub 2 ~+~R(0)a sub 3 ~+~...~~=~R(3)
.EN
.EQ
etc
.EN
.fi
.LE
An elegant method due to Durbin and Levinson exists for solving this
special system of equations.  It requires much less computational
effort than is generally needed for symmetric matrix equations.
.pp
Of course, an infinite range of summation can not be used in
practice.  For one thing, the power spectrum is changing, and
only the data from a short time-frame should be used for
a realistic estimate of the optimum linear predictive coefficients.
Hence a windowing procedure,
.LB
.EQ
x(n) sup * ~=~w sub n x(n),
.EN
.LE
is used to reduce the signal to zero outside a finite range of
interest.  Windows were discussed in Chapter 4 from the
point of view of Fourier analysis of speech signals, and the same
sort of considerations apply to choosing a window for linear
prediction.
.pp
This is known as the
.ul
autocorrelation method
of computing prediction parameters.  Typically a window of
100 to 250 samples is used for analysis of one frame of speech.
.rh "Algorithm for the autocorrelation method."
The algorithm for obtaining linear prediction coefficients
by the autocorrelation method is quite simple.  It is
straightforward to compute the matrix coefficients
$R(m)$ from the speech samples and window coefficients.
The Durbin-Levinson method of solving matrix equations operates
directly on this $R$-vector to produce the coefficient vector $a sub k$.
The complete procedure is given as Procedure 6.1, and is shown
diagrammatically in Figure 6.5.
.FC "Figure 6.5"
.RF
.fi
.na
.nh
.ul
const
N=256; p=15;
.ul
type
svec =
.ul
array
[0..N\-1]
.ul
of
real;
cvec =
.ul
array
[1..p]
.ul
of
real;
.sp
.ul
procedure
autocorrelation(signal: vec; window: svec;
.ul
var
coeff: cvec);
.sp
{computes linear prediction coefficients by autocorrelation method
in coeff[1..p]}
.sp
.ul
var
R, temp:
.ul
array
[0..p]
.ul
of
real;
n: [0..N\-1]; i,j: [0..p]; E: real;
.sp
.ul
begin
{window the signal}
.in+6n
.ul
for
n:=0
.ul
to
N\-1
.ul
do
signal[n] := signal[n]*window[n];
.sp
{compute autocorrelation vector}
.br
.ul
for
i:=0
.ul
to
p
.ul
do begin
.in+2n
R[i] := 0;
.br
.ul
for
n:=0
.ul
to
N\-1\-i
.ul
do
R[i] := R[i] + signal[n]*signal[n+i]
.in-2n
.ul
end;
.sp
{solve the matrix equation by the Durbin-Levinson method}
.br
E := R[0];
.br
coeff[1] := R[1]/E;
.br
.ul
for
i:=2
.ul
to
p
.ul
do begin
.in+2n
E := (1\-coeff[i\-1]*coeff[i\-1])*E;
.br
coeff[i] := R[i];
.br
.ul
for
j:=1
.ul
to
i\-1
.ul
do
coeff[i] := coeff[i] \- R[i\-j]*coeff[j];
.br
coeff[i] := coeff[i]/E;
.br
.ul
for
j:=1
.ul
to
i\-1
.ul
do
temp[j] := coeff[j] \- coeff[i]*coeff[i\-j];
.br
.ul
for
j:=1
.ul
to
i\-1
.ul
do
coeff[j] := temp[j]
.in-2n
.ul
end
.in-6n
.ul
end.
.nf
.FG "Procedure 6.1  Pascal algorithm for the autocorrelation method"
.pp
This algorithm is not quite as efficient as it might be, for some
multiplications are repeated during the calculation of the
autocorrelation vector.  Blankinship (1974) shows how
the number of multiplications can be reduced by about half.
.[
Blankinship 1974
.]
.pp
If the algorithm is performed in fixed-point arithmetic
(as it often is in practice because of speed considerations),
some scaling must be done.  The maximum and minimum values of
the windowed signal can be determined within the window
calculation loop, and one extra pass over the vector will
suffice to scale it to maximum significance.
(Incidentally, if all sample values are the same the procedure
cannot produce a solution because $E$ becomes zero, and this
can easily be checked when scaling.)
.pp
The absolute value of the $R$-vector has no significance, and since
$R(0)$ is always the greatest element, this can be set to the largest
fixed-point number and the other $R$'s scaled down appropriately
after they have been calculated.
These scaling operations are shown as dashed boxes in Figure 6.5.
$E$ decreases monotonically
as the computation proceeds, so it is safe to initialize it to $R(0)$
without extra scaling.  The remainder of the scaling is straightforward,
with the linear prediction coefficients $a sub k$ appearing as fractions.
.rh "The covariance method."
One of the advantages of linear predictive methods that was
promised earlier was that it allows us to escape from
the problem of windowing.  To do this, we must abandon the
requirement that the coefficients of the matrix equation have
the symmetry property of autocorrelations.  Instead, suppose
that the range of $n$-summation uses a fixed number of
elements, say N, starting at $n=h$, to estimate the prediction
coefficients between sample number $h$ and sample number $h+N$.
.pp
This leads to the matrix equation
.LB
.EQ
sum from k=1 to p ~a sub k sum from n=h to h+N-1 ~x(n-j)x(n-k) ~~=~~
sum from n=h to h+N-1 ~x(n)x(n-j)~~~~j~=~1,~2,~...,~p.
.EN
.LE
Alternatively, we could write
.LB
.EQ
sum from k=1 to p ~a sub k ~ Q sub jk sup h~~=~~Q sub 0j sup h
~~~~j~=~1,~2,~...,~p;
.EN
.LE
where
.LB
.EQ
Q sub jk sup h~~=~~sum from n=h to h+N-1 ~x(n-j)x(n-k).
.EN
.LE
Note that some values of $x(n)$ outside the range  $h ~ <= ~ n ~ < ~ h+N$  are
required:  these are shown diagrammatically in Figure 6.6.
.FC "Figure 6.6"
.pp
Now  $Q sub jk sup h ~=~ Q sub kj sup h$,  so the equation has
a diagonally symmetric matrix; and in fact the matrix $Q sup h$ can
be shown to be positive semidefinite \(em and is almost always positive
definite in practice.  Advantage can be taken of these facts
to provide a computationally efficient method for solving the
equation.  According to a result called Cholesky's theorem, a
positive definite symmetric matrix $Q$ can be factored into the form
$Q ~ = ~ LL sup T$, where $L$ is a lower triangular matrix.
This leads to an efficient
solution algorithm.
.pp
This method of computing prediction coefficients has become known
as the
.ul
covariance method.
It does not use windowing of the speech signal, and can give accurate
estimates of the prediction coefficients with a smaller analysis
frame than the autocorrelation method.  Typically, 50 to 100 speech samples
might be used to estimate the coefficients, and they are re-calculated
every 100 to 250 samples.
.rh "Algorithm for the covariance method."
An algorithm for the covariance method is given in Procedure 6.2,
.RF
.fi
.na
.nh
.ul
const
N=100; p=15;
.ul
type
svec =
.ul
array
[\-p..N\-1]
.ul
of
real;
cvec =
.ul
array
[1..p]
.ul
of
real;
.sp
.ul
procedure
covariance(signal: svec;
.ul
var
coeff: cvec);
.sp
{computes linear prediction coefficients by covariance method
in coeff[1..p]}
.sp
.ul
var
Q:
.ul
array
[0..p,0..p]
.ul
of
real;
n: [0..N\-1]; i,j,r: [0..p]; X: real;
.sp
.ul
begin
{calculate upper-triangular covariance matrix in Q}
.in+6n
.ul
for
i:=0
.ul
to
p
.ul
do
.in+2n
.ul
for
j:=i
.ul
to
p
.ul
do begin
.in+2n
Q[i,j]:=0;
.br
.ul
for
n:=0
.ul
to
N\-1
.ul
do
.in+2n
Q[i,j] := Q[i,j] + signal[n\-i]*signal[n\-j]
.in-2n
.in-2n
.ul
end;
.in-2n
.sp
{calculate the square root of Q}
.br
.ul
for
r:=2
.ul
to
p
.ul
do
.in+2n
.ul
begin
.in+2n
.ul
for
i:=2
.ul
to
r\-1
.ul
do
.in+2n
.ul
for
j:=1
.ul
to
i\-1
.ul
do
.in+2n
Q[i,r] := Q[i,r] \- Q[j,i]*Q[j,r];
.in-2n
.ul
for
j:=1
.ul
to
r\-1
.ul
do
.in+2n
.ul
begin
.in+2n
X := Q[j,r];
.br
Q[j,r] := Q[j,r]/Q[j,i];
.br
Q[r,r] := Q[r,r] \- Q[j,r]*X
.in-2n
.ul
end
.in-2n
.in-2n
.in-2n
.ul
end;
.in-2n
.sp
{calculate coeff[1..p]}
.br
.ul
for
r:=2
.ul
to
p
.ul
do
.in+2n
.ul
for
i:=1
.ul
to
r\-1
.ul
do
Q[0,r] := Q[0,r] \- Q[i,r]*Q[0,i];
.in-2n
.ul
for
r:=1
.ul
to
p
.ul
do
Q[0,r] := Q[0,r]/Q[r,r];
.br
.ul
for
r:=p\-1
.ul
downto
1
.ul
do
.in+2n
.ul
for
i:=r+1
.ul
to
p
.ul
do
Q[0,r] := Q[0,r] \- Q[r,i]*Q[0,i];
.in-2n
.ul
for
r:=1
.ul
to
p
.ul
do
coeff[r] := Q[0,r]
.in-6n
.ul
end.
.nf
.FG "Procedure 6.2  Pascal algorithm for the covariance method"
and is shown diagrammatically in Figure 6.7.
.FC "Figure 6.7"
The algorithm shown is not terribly efficient from a computation
and storage point of view, although it is workable.  For one thing,
it uses the obvious method for computing the covariance matrix
by calculating
.EQ
Q sub 01 sup h ,
.EN
.EQ
Q sub 02 sup h , ~ ...,
.EN
.EQ
Q sub 0p sup h ,
.EN
.EQ
Q sub 11 sup h , ...,
.EN
in turn, which repeats most of the multiplications $p$ times \(em not
an efficient procedure.  A simple alternative is to precompute the necessary
multiplications and store them in a  $(N+h) times (p+1)$ diagonally symmetric
table, but even apart from the extra storage required for this, the number
of additions which must be performed subsequently to give the $Q$'s is far
larger than necessary.  It is possible, however, to write a procedure which is
both time- and space-efficient (Witten, 1980).
.[
Witten 1980 Algorithms for linear prediction
.]
.pp
The scaling problem is rather more tricky for the covariance
method than for the autocorrelation method.  The $x$-vector
should be scaled initially in the same way as before, but now there
are $p+1$ diagonal elements of the covariance matrix, any of which could
be the greatest element.  Of course,
.LB
.EQ
Q sub jk ~~ <= ~~ Max ( Q sub 11 , Q sub 22 , ..., Q sub pp ),
.EN
.LE
but despite the considerable communality in the summands of the diagonal
elements, there are no
.ul
a priori
bounds on the ratios between them.
.pp
The only way to scale the $Q$ matrix properly is to calculate each of its $p$
diagonal elements and use the greatest as a scaling factor.
Alternatively, the fact that
.LB
.EQ
Q sub jk ~~ <= ~~ N times Max( x sub n sup 2 )
.EN
.LE
can be used to give a bound for scaling purposes; however, this
is usually a rather conservative bound, and as $N$ is often around 100, several
bits of significance will be lost.
.pp
Scaling difficulties do not cease when $Q$ has been determined.  It is possible
to show that the elements of the lower-triangular matrix $L$ which represents
the square root of $Q$ are actually
.ul
unbounded.
In fact there is a slightly different variant of the Cholesky decomposition
algorithm which guarantees bounded coefficients but suffers from the
disadvantage that it requires square roots to be taken (Martin
.ul
et al,
1965).
.[
Martin Peters Wilkinson 1965
.]
However, experience with the method indicates that it is rare for the elements
of $L$ to exceed 16 times the maximum element of $Q$, and the possibility of
occasional failure to adjust the coefficients may be tolerable in a practical
linear prediction system.
.rh "Comparison of autocorrelation and covariance analysis."
There are various factors which should be taken into account when
deciding whether to use the autocorrelation or covariance method for linear
predictive analysis.  Furthermore, there is a rather different technique,
called the "lattice method", which will be discussed shortly.
The autocorrelation method involves windowing, which means that in
practice a rather longer stretch of speech should be used
for analysis.  We have illustrated this by setting $N$=256 in the
autocorrelation algorithm and 100 in the covariance one.
Offsetting the extra calculation that this entails is the
fact that the Durbin-Levinson method of inverting a matrix is much more
efficient than Cholesky decomposition.  In practice, this means
that similar amounts of computation are needed for each method \(em a
detailed comparison is made in Witten (1980).
.[
Witten 1980 Algorithms for linear prediction
.]
.pp
A factor which weighs against the covariance method is the
difficulty of scaling intermediate quantities within the algorithm.
The autocorrelation method can be implemented quite satisfactorily
in fixed-point arithmetic, and this makes it more suitable for
hardware implementation.  Furthermore, serious instabilities sometimes
arise with the covariance method, whereas it can be shown that
the autocorrelation one is always stable.  Nevertheless, the approximations
inherent in the windowing operation, and the smearing effect of taking a
larger number of sample points, mean that covariance-method coefficients
tend to represent the speech more accurately, if they can be obtained.
.pp
One way of using the covariance method which has proved to be rather
satisfactory in practice is to synchronize the analysis frame with
the beginning of a pitch period, when the excitation is strongest.
Pitch synchronous techniques were discussed in Chapter 4 in the context
of discrete Fourier transformation of speech.  The snag, of course, is that
pitch peaks do not occur uniformly in time, and furthermore it is difficult
to estimate their locations precisely.
.sh "6.2  Linear predictive synthesis"
.pp
If the linear predictive coefficients and the error signal are available,
it is easy to regenerate the original speech by
.LB
.EQ
x(n)~=~~e(n)~+~~ sum from k=1 to p ~a sub k x(n-k) .
.EN
.LE
If the error signal is parametrized into the sound source type
(voiced or unvoiced), amplitude, and pitch (if voiced), it can be
regenerated by an impulse repeated at the appropriate pitch
frequency (if voiced), or white noise (if unvoiced).
.pp
However, it may be that the filter represented by the coefficients $a sub k$ is
unstable, causing the output speech signal to oscillate wildly.
In fact, it is only possible for the covariance method to produce an
unstable filter, and not the autocorrelation method \(em although even
with the latter, truncation of the $a sub k$'s for transmission may turn
a stable filter into an unstable one.  Furthermore, the coefficients
$a sub k$ are not suitable candidates for quantization, because small
changes in them can have a dramatic effect on the characteristics of
the synthesis filter.
.pp
Both of these problems can be solved by using a different set of numbers,
called
.ul
reflection coefficients,
for quantization and transmission.  Thus, for example, in Figures 6.1
and 6.3 these reflection coefficients could be derived at the
transmitter, quantized, and used by the receiver to reproduce
the speech waveform.  They can be related to reflection and transmission
parameters at the junctions of an acoustic tube model of the vocal tract;
hence the name.  Procedure 6.3 shows an algorithm for calculating the
reflection coefficients from the filter coefficients $a sub k$.
.RF
.fi
.na
.nh
.ul
const
p=15;
.ul
type
cvec =
.ul
array
[1..p]
.ul
of
real;
.sp
.ul
procedure
reflection(coeff: cvec;
.ul
var
refl: cvec);
.sp
{computes reflection coefficients in refl[1..p] corresponding
to linear prediction coefficients in coeff[1..p]}
.sp
.ul
var
temp: cvec;  i, m: 1..p;
.sp
.ul
begin
.in+6n
.ul
for
m:=p
.ul
downto
1
.ul
do begin
.in+2n
refl[m] := coeff[m];
.br
.ul
for
i:=1
.ul
to
m\-1
.ul
do
temp[i] := coeff[i];
.br
.ul
for
i:=1
.ul
to
m\-1
.ul
do
.ti+2n
coeff[i] :=
.ti+4n
(coeff[i] + refl[m]*temp[m\-i]) / (1 \- refl[m]*refl[m]);
.in-2n
.ul
end
.in-6n
.ul
end.
.nf
.MT 2
Procedure 6.3  Pascal algorithm for producing reflection coefficients
from filter coefficients
.TE
.pp
Although we will not go into the theoretical details here,
reflection coefficients are bounded by $+-$1 for stable filters,
and hence form a useful test for stability.  Having a limited
range makes them easy to quantize for transmission, and in fact
they behave better under quantization than do the filter coefficients.
One could resynthesize speech from reflection coefficients by first
converting them to filter coefficients and using the synthesis
method described above.  However, it is natural to seek a single-stage
procedure which can regenerate speech directly from reflection
coefficients.
.pp
Such a procedure does exist, and is called a
.ul
lattice filter.
Figure 6.8 shows one form of lattice for speech synthesis.
.FC "Figure 6.8"
The error signal (whether transmitted or synthesized)
enters at the upper left-hand corner, passes along the top forward
signal path, being modified on the way, to give the output signal
at the right-hand side.
Then it passes back through a chain of delays along the bottom,
backward, path, and is used to modify subsequent forward signals.
Finally it is discarded at the lower left-hand corner.
.pp
There are $p$ stages in the lattice structure of Figure 6.8, where $p$ is the
order of the linear predictive filter.
Each stage involves two multiplications by the appropriate
reflection coefficients, one by the backward signal \(em the
result of which is added into the forward path \(em and the other by
the forward signal \(em the result of which is subtracted from the
backward path.  Thus the number of multiplications is twice
the order of the filter, and hence twice as many as for the
realization using coefficients $a sub k$.  If the labour necessary
to turn the reflection coefficients into $a sub k$'s is included,
the computational load becomes the same.  Moreover, since the
reflection coefficients need fewer quantization bits than the $a sub k$'s
(for a given speech quality), the word lengths are smaller in the
lattice realization.
.pp
The advantages of the lattice method of synthesis over direct evaluation
of the prediction using filter coefficients $a sub k$, then, are:
.LB
.NP
the reflection coefficients are used directly
.NP
the stability of the filter is obvious from the reflection coefficient
values
.NP
the system is more tolerant to quantization errors in fixed-point
implementations.
.LE
Although it may seem unlikely that an unstable filter would be produced
by linear predictive analysis, instability is in fact a real problem
in non-lattice implementations.  For example,
coefficients are often interpolated at the receiver, to allow longer
frame times and smooth over sudden transitions, and it is quite likely that
an unstable configuration is obtained when interpolating filter coefficients
between two stable configurations.
This cannot happen with reflection coefficients, however, because a
necessary and sufficient condition for stability is that all
coefficients lie in the interval $(-1,+1)$.
.sh "6.3  Lattice filtering"
.pp
Lattice filters are an important new method of linear predictive
.ul
analysis
as well as synthesis, and so
it is worth considering the theory behind them a little further.
.rh "Theory of the lattice synthesis filter."
Figure 6.9 shows a single stage of the synthesis lattice given earlier.
.FC "Figure 6.9"
There are two signals at each side of the lattice, and the $z$-transforms
of these have been labelled $X sup +$ and $X sup -$ at the left-hand side
and $Y sup +$ and $Y sup -$ at the right-hand side.
The direction of signal flow is forwards along the upper ("positive") path
and backwards along the lower ("negative") one.
.pp
The signal flows show that the following two relationships hold:
.LB
.EQ
Y sup + ~=~~ X sup + ~+~ k z sup -1 Y sup - ~~~~~~
.EN
for the forward (upper) path
.br
.EQ
X sup - ~ =~ -kY sup + ~+~ z sup -1 Y sup - ~~~~~~~
.EN
\h'-\w'\-'u'for the backward (lower) path.
.LE
Re-arranging the first equation yields
.LB
.EQ
X sup + ~ =~~ Y sup + ~-~ k z sup -1 Y sup - ,
.EN
.LE
and so we can describe the function of the lattice by a single matrix
equation:
.LB
.ne4
.EQ
left [ matrix {ccol {X sup + above X sup -}} right ] ~~=~~
left [ matrix {ccol {1 above -k} ccol {-kz sup -1 above z sup -1}} right ]
~ left [ matrix {ccol {Y sup + above Y sup -}} right ] ~ .
.EN
.LE
It would be nice to be able to
call this an input-output equation, but it is not;
for the input signals to the lattice stage are $X sup +$ and $Y sup -$,
and the outputs are $X sup -$ and $Y sup +$.
We have written it in this form because it allows a multi-stage lattice to
be described by cascading these matrix equations.
.pp
A single-stage lattice filter has $Y sup +$ and $Y sup -$ connected together,
forming its output (call this $X sub output$), while the input is $X sup +$
($X sub input$).
Hence the input is related to the output by
.LB
.EQ
left [ matrix {ccol {X sub input above \(sq }} right ] ~~ =
~~ left [ matrix {ccol {1 above -k} ccol {-k z sup -1
above z sup -1}} right ]
~ left [ matrix {ccol {X sub output above X sub output}} right ] ~ ,
.EN
.LE
so
.LB
.EQ
X sub input ~ = ~~ (1~-~ k z sup -1 )~X sub output ,
.EN
.LE
or
.LB
.EQ
{X sub output} over {X sub input} ~~=~~ 1 over {1~-~ k sub 1 z sup -1} ~ .
.EN
.LE
(The symbol \(sq is used here and elsewhere
to indicate an unimportant element of a vector
or matrix.)  This certainly has the form of a linear predictive
synthesis filter, which is
.LB
.EQ
X(z) over E(z) ~~=~~ 1 over {1~-~~ sum from k=1 to p ~a sub k
z sup -k}~~=~~ 1 over {1~-~a sub 1 z sup -1 } ~~~~~~
.EN
when $p=1$.
.LE
.pp
The behaviour of a second-order lattice filter, shown in Figure 6.10,
can be described by
.LB
.ne4
.EQ
left [ matrix {ccol {X sub 3 sup + above X sub 3 sup -}} right ] ~~ =
~~ left [ matrix {ccol {1 above -k sub 2 } ccol {-k sub 2 z sup -1
above z sup -1}} right ]
~ left [ matrix {ccol {X sub 2 sup + above X sub 2 sup -}} right ]
.EN
.sp
.ne4
.EQ
left [ matrix {ccol {X sub 2 sup + above X sub 2 sup -}} right ] ~~ =
~~ left [ matrix {ccol {1 above -k sub 1 } ccol {-k sub 1 z sup -1
above z sup -1}} right ]
~ left [ matrix {ccol {X sub 1 sup + above X sub 1 sup -}} right ]
.EN
.LE
with
.LB
.ne3
.EQ
X sub 3 sup + ~=~X sub input
.EN
.br
.EQ
X sub 1 sup + ~=~ X sub 1 sup - ~=~ X sub output .
.EN
.LE
.FC "Figure 6.10"
$X sub 2 sup +$ and $X sub 2 sup -$ can be eliminated by substituting the
second equation into the first, which yields
.LB
.EQ
left [ matrix {ccol {X sub input above \(sq }} right ] ~~ mark =
~~ left [ matrix {ccol {1 above -k sub 2 } ccol {-k sub 2 z sup -1
above z sup -1}} right ]
~ left [ matrix {ccol {1 above -k sub 1 } ccol {-k sub 1 z sup -1
above z sup -1}} right ]
~ left [ matrix {ccol {X sub output above X sub output}} right ]
.EN
.sp
.sp
.EQ
lineup = ~~ left [ matrix {ccol {1+k sub 1 k sub 2 z sup -1 above \(sq }
ccol { -k sub 1 z sup -1 -k sub 2 z sup -2 above \(sq }} right ]
~ left [ matrix {ccol {X sub output above X sub output}} right ] ~ .
.EN
.LE
This leads to an input-output relationship
.LB
.EQ
{X sub output} over {X sub input} ~~ = ~~
1 over {1~+~k sub 1 (k sub 2 -1)z sup -1 ~-~k sub 2 z sup -2} ~ ,
.EN
.LE
which has the required form, namely
.LB
.EQ
1 over {1~-~~ sum from k=1 to p ~a sub k z sup -k } ~~~~~~ (p=2)
.EN
.LE
when
.LB
.EQ
a sub 1 ~=~-k sub 1 (k sub 2 -1)
.EN
.br
.EQ
a sub 2 ~=~k sub 2.
.EN
.LE
.pp
A third-order filter is described by
.LB
.EQ
left [ matrix {ccol {X sub input above \(sq }} right ] ~~ =
~~ left [ matrix {ccol {1 above -k sub 3 } ccol {-k sub 3 z sup -1
above z sup -1}} right ]
~ left [ matrix {ccol {1 above -k sub 2 } ccol {-k sub 2 z sup -1
above z sup -1}} right ]
~ left [ matrix {ccol {1 above -k sub 1 } ccol {-k sub 1 z sup -1
above z sup -1}} right ]
~ left [ matrix {ccol {X sub output above X sub output}} right ] ~ ,
.EN
.LE
and brave souls can verify that this gives an input-output
relationship
.LB
.EQ
{X sub output} over {X sub input} ~~ = ~~ 
1 over {1~+~[k sub 2 k sub 3 + k sub 1 (1-k sub 2 )] z sup -1 ~+~
[k sub 1 k sub 3 (1-k sub 2 ) -k sub 2 ] z sup -2 ~-~ k sub 3 z sup -3 } ~ .
.EN
.LE
It is fairly obvious that a $p$'th order lattice filter will give the
required all-pole $p$'th order synthesis form,
.LB
.EQ
1 over { 1~-~~ sum from k=1 to p ~a sub k z sup -k } ~ .
.EN
.LE
.pp
We have not shown that the algorithm given in Procedure 6.3 for producing
reflection coefficients from filter coefficients gives those values
for $k sub i$ which are necessary to make the lattice filter equivalent
to the ordinary synthesis filter.  However, this is the case, and it is
easy to verify by hand for the first, second, and third-order cases.
.rh "Different lattice configurations."
The lattice filters of Figures 6.8, 6.9, and 6.10 have two multipliers
per section.
This is called a "two-multiplier" configuration.
However, there are other configurations which achieve
the same effect, but require different numbers of multiplies.
Figure 6.11 shows one-multiplier and four-multiplier configurations,
along with the familiar two-multiplier one.
.FC "Figure 6.11"
It is easy to verify that the three configurations can be modelled in
matrix terms by
.LB
.ne4
$
left [ matrix {ccol {X sup + above X sup -}} right ] ~~ = ~~
left [ matrix {ccol {1 above -k} ccol {-kz sup -1 above z sup -1}} right ]
~ left [ matrix {ccol {Y sup + above Y sup -}} right ]
$		two-multiplier configuration
.sp
.sp
.ne4
$
left [ matrix {ccol {X sup + above X sup -}} right ] ~~ = ~~
left [ {1-k over 1+k} right ] sup 1/2 ~
left [ matrix {ccol {1 above -k} ccol {-kz sup -1 above z sup -1}} right ]
~ left [ matrix {ccol {Y sup + above Y sup -}} right ]
$	one-multiplier configuration
.sp
.sp
.ne4
$
left [ matrix {ccol {X sup + above X sup -}} right ] ~~ = ~~
1 over {(1-k sup 2) sup 1/2} ~
left [ matrix {ccol {1 above -k} ccol {-kz sup -1 above z sup -1}} right ]
~ left [ matrix {ccol {Y sup + above Y sup -}} right ]
$	four-multiplier configuration.
.LE
Each of the three has the same frequency-domain response, although
a different constant factor is involved in each case.
The effect of this can be annulled by performing a single multiply
operation on the output of a complete lattice chain.
The multiplier has the form
.LB
.EQ
left [ {1 - k sub p} over {1 + k sub p} ~.~
{1 - k sub p-1} over {1 + k sub p-1} ~.~...~.~
{1 - k sub 1} over {1 + k sub 1} right ] sup 1/2
.EN
.sp
.LE
for single-multiplier lattices, and
.LB
.EQ
left [ 1 over {1 - k sub p sup 2} ~.~
1 over {1 - k sub p-1 sup 2} ~.~...~.~
1 over {1 - k sub 1 sup 2} right ] sup 1/2
.EN
.LE
for four-multiplier lattices, where the reflection coefficients
in the lattice are $k sub p$, $k sub p-1$, ..., $k sub 1$.
.pp
There are important differences between these three configurations.
If multiplication is time-consuming, the one-multiplier model has obvious
computational advantages over the other two methods.
However, the four-multiplier structure behaves substantially better
in finite word-length implementations.  It is easy to show that, with this
configuration,
.LB
.EQ
(X sup - ) sup 2 ~+~ (Y sup + ) sup 2 ~~ = ~~
(X sup + ) sup 2 ~+~ (z sup -1 Y sup - ) sup 2 ,
.EN
.LE
\(em a relationship which suggests that the "energy" in the
the input signals, namely  $X sup +$ and $Y sup -$,  is preserved in the output
signals,  $X sup -$ and $Y sup +$.
Notice that care must be taken with the $z$-transforms, since squaring is a
non-linear operation.  $(z sup -1 Y sup - ) sup 2$  means the square of
the previous value of  $Y sup -$,  which is not the same
as  $z sup -2 (Y sup - ) sup 2$.
.pp
It has been shown (Gray and Markel, 1975) that the four-multiplier
configuration has some stability properties which are not shared by other
digital filter structures.
.[
Gray Markel 1975 Normalized digital filter structure
.]
When a linear predictive filter is used for synthesis, the parameters
of the filter \(em the $k$-parameters in the case of lattice filters,
and the $a$-parameters in the case of direct ones \(em change with time.
It is usually rather difficult to guarantee stability in the case of
time-varying filter parameters, but some guarantees can be made for a
chain of four-multiplier lattices.  Furthermore, if the input is a
discrete delta function, the cumulative energies at each stage of the
lattice are the same, and so maximum dynamic range will be achieved
for the whole filter if each section is implemented with the same
word size.
.rh "Lattice analysis."
It is quite easy to construct a filter which is inverse to
a single-stage lattice.
The structure of Figure 6.12(a) does the job.
(Ignore for a moment
the dashed lines connecting Figure 6.12(a) and (b).)  Its matrix transfer
function is
.FC "Figure 6.12"
.LB
.ne4
$
left [ matrix {ccol {Y sup + above Y sup -}} right ] ~~=~~
left [ matrix {ccol {1 above -k} ccol {-kz sup -1 above z sup -1}} right ]
~ left [ matrix {ccol {X sup + above X sup -}} right ]
$	analysis lattice (Figure 6.12(a)).
.LE
Notice that this is exactly the same as the transfer function of the
synthesis lattice of Figure 6.9, which is reproduced
in Figure 6.12(b), except that the $X$'s and $Y$'s are reversed:
.LB
.ne4
$
left [ matrix {ccol {X sup + above X sup -}} right ] ~~=~~
left [ matrix {ccol {1 above -k} ccol {-kz sup -1 above z sup -1}} right ]
~ left [ matrix {ccol {Y sup + above Y sup -}} right ]
$	synthesis lattice (Figure 6.12(b)),
.LE
or, in other words,
.LB
.ne4
$
left [ matrix {ccol {Y sup + above Y sup -}} right ] ~~ = ~~
left [ matrix {ccol {1 above -k} ccol {-kz sup -1 above z sup -1}}
right ] sup -1
~ left [ matrix {ccol {X sup + above X sup -}} right ]
$	synthesis lattice (Figure 6.12(b)).
.LE
Hence if the filters of Figures 6.12(a) and (b) were connected together
as shown by the dashed lines, they
would cancel each other out, and the overall transfer would be unity:
.LB
.ne4
.EQ
left [ matrix {ccol {1 above -k} ccol {-kz sup -1 above z sup -1}}
right ] ~
left [ matrix {ccol {1 above -k} ccol {-kz sup -1 above z sup -1}}
right ] sup -1 ~~ = ~~
left [ matrix {ccol {1 above 0} ccol {0 above 1}} right ] ~ .
.EN
.LE
Actually, such a connection is not possible in physical terms,
for although the upper paths can be joined together the lower ones can not.
The right-hand lower point of Figure 6.12(a) is an
.ul
output
terminal, and so is the left-hand lower one of Figure 6.12(b)!  However,
there is no need to envisage a physical connection of the lower paths.
It is sufficient for cancellation just to assume that the signals at both
of the points turn out to be the same.
.pp
And they do.
The general case of a $p$-stage analysis lattice
connected to a $p$-stage synthesis
lattice is shown in Figure 6.13.
.FC "Figure 6.13"
Notice that the forward and backward paths are connected together at both
of the extreme ends of the system.
It is not difficult to show that under these
conditions the signal at the lower righthand
terminal of the analysis chain will equal that at the lower lefthand
terminal of the synthesis chain, even though they are not connected,
provided the upper terminals are connected together as shown by the dashed
line.
Of course, the reflection coefficients  $k sub 1$, $k sub 2$, ...,
$k sub p$  in the analysis lattice must equal those in the synthesis
lattice, and as Figure 6.13 shows the order is reversed in the synthesis
lattice.
Successive analysis and synthesis sections pair off, working from
the middle outwards.  At each stage the sections cancel each other out,
giving a unit transfer function as demonstrated above.
.rh "Estimating reflection coefficients."
As stated earlier in this chapter, the key problem in linear prediction is to
determine the values of the predictive coefficients \(em in this case, the
reflection coefficients.
If this is done correctly, we have shown using Procedure 6.3 that
the the synthesis part of Figure 6.13 performs the same calculation that
a conventional direct-form linear predictive synthesizer would, and hence
the signal that excites it \(em that is, the signal represented by the
dashed line \(em must be the prediction residual, or error signal, discussed
earlier.  The system is effectively the same as the high-order adaptive
differential pulse code modulation one of Figure 6.1.
.pp
One of the most interesting features of the lattice structure for
analysis filters is that calculation of suitable values for the
reflection coefficients can be done locally at each stage of the lattice.
For example, consider the $i$'th section of the analysis lattice in
Figure 6.13.  It is possible to determine a suitable value of $k sub i$
simply by performing a calculation on the inputs to the $i$'th
section (ie $X sup +$ and $X sup -$ in Figure 6.12).
No longer need the complicated global optimization technique of matrix
inversion be used, as in the autocorrelation and covariance methods discussed
earlier.
.pp
A suitable value for $k$ in the single lattice section of Figure 6.12 is
.LB
.EQ
k~ = ~~ {E[ x sup + (n) x sup - (n-1)]} over
{( E[ x sup + (n) sup 2 ] E[ x sup - (n-1) sup 2 ] ) sup 1/2} ~~ ;
.EN
.LE
that is, the statistical correlation between $x sup + (n)$ and
$x sup - (n-1)$.
Here, $x sup + (n)$ and $x sup - (n)$ represent the input signals to the
upper and lower paths (recall that $X sup +$ and $X sup -$
are their $z$-transforms).
$x sup - (n-1)$ is just $x sup - (n)$ delayed by one time unit, that is,
the output of the $z sup -1$ box in the Figure.
.pp
The criterion of optimality for the autocorrelation and covariance methods
was that the prediction error, that is, the signal which emerges from
the right-hand end of the upper path of a lattice analysis filter,
should be minimized in a mean-square sense.
The reflection coefficients obtained from the above formula do not necessarily
satisfy any such global minimization criterion.
Nevertheless, they do keep the error signal small, and have been used with
success in speech analysis systems.
.pp
It is easy to minimize the output from either the upper or the lower path
of the lattice filter at each stage.  For example, the $z$-transform of the
upper output is given by
.LB
.EQ
Y sup + ~~=~~ X sup + ~-~ k z sup -1 X sup - ,
.EN
.LE
or
.LB
.EQ
y sup + (n) ~~=~~ x sup + (n) ~-~ k x sup - (n-1) .
.EN
.LE
Hence
.LB
.EQ
E[y sup + (n) sup 2 ] ~~ = ~~ E[x sup + (n) sup 2 ] ~-~
2kE[x sup + (n) x sup - (n-1) ] ~+~ k sup 2 E [x sup - (n-1) sup 2 ] ,
.EN
.LE
where $E$ stands for expected value, and this reaches a minimum when the
derivative with respect to $k$ becomes zero:
.LB
.EQ
-2E[x sup + (n) x sup - (n-1) ] ~+~ 2kE[x sup - (n-1) sup 2 ] ~~=~0 ,
.EN
.LE
that is, when
.LB
.EQ
k~ = ~~ {E[x sup + (n) x sup - (n-1) ]} over {E[x sup - (n-1) sup 2 ]
} ~ .
.EN
.LE
A similar calculation shows that the output of the lower path is minimized
when
.LB
.EQ
k~ = ~~ {E[x sup + (n) x sup - (n-1) ]} over {E[x sup + (n-1) sup 2 ]
} ~ .
.EN
.LE
Unfortunately, either of these expressions can exceed 1, leading to an
unstable filter.
The value of $k$ cited earlier is the geometric mean of these two
expressions, and since it is a correlation coefficient, must be less than 1.
.pp
Another possibility is to minimize the expected value of the sum of the
squares of the upper and lower outputs:
.LB
.EQ
y sup + (n) sup 2 ~+~ y sup - (n) sup 2 ~~ = ~~
(1+k sup 2 )x sup + (n) sup 2 ~-~ 2kx sup + (n) x sup - (n-1) ~+~
(1+k sup 2 )x sup - (n) sup 2 .
.EN
.LE
Taking expected values and setting the derivative with respect to k to zero
leads to
.LB
.EQ
k~ = ~~ {E[x sup + (n) x sup - (n-1) ]} over
{ half ~ E[x sup + (n) sup 2 ~+~ x sup - (n-1) sup 2 ]} ~.
.EN
.LE
This also is guaranteed to be less than 1, and has given good results
in speech analysis systems.
.pp
Figure 6.14 shows the implementation of a single section of an analysis
lattice.
.FC "Figure 6.14"
The signals $x sup + (n)$ and $x sup - (n-1)$ are fed to a
correlator, which produces a suitable value for $k$.
This value is used to calculate the output of the lattice section,
and hence the input to the next lattice section.
The reflection coefficient needs to be low-pass filtered, because it will
only be transmitted to the synthesizer occasionally (say every 20\ msec) and so a
short-term average is required.
.pp
One implementation of the correlator is shown in Figure 6.15 (Kang, 1974).
.[
Kang 1974
.]
.FC "Figure 6.15"
This calculates the value of $k$ given by the last equation above, and does it
by summing and differencing the two
signals $x sup + (n)$ and $x sup - (n-1)$, squaring the results to give
.LB
.EQ
x sup + (n) sup 2 + 2x sup + (n mark ) x sup - (n-1) +x sup - (n-1) sup 2
~~~~~~~~ x sup + (n) sup 2 - 2x sup + (n) x sup - (n-1) +x sup - (n-1) sup 2
~ ,
.EN
.LE
and summing and differencing these, to yield
.LB
.EQ
lineup 2x sup + (n) sup 2 + 2x sup - (n-1) sup 2 ~~~~~~~~
4x sup + (n) x sup - (n-1) ~ .
.EN
.LE
.sp
Before these are divided to give the final coefficient $k$, they are
individually low-pass filtered.
While some rather complex schemes have been proposed,
based upon Kalman filter theory (eg Matsui
.ul
et al,
1972),
.[
Matsui Nakajima Suzuki Omura 1972
.]
a simple exponential weighted past average has been found to be
satisfactory.  This has $z$-transform
.LB
.EQ
1 over {64 - 63 z sup -1} ~ ,
.EN
.LE
that is, in the time domain,
.LB
.EQ
y(n)~ = ~~ 63 over 64 ~ y(n-1) ~+~ 1 over 64 ~ y(n) ~ .
.EN
.LE
This filter exponentially averages past sample values
with a time-constant of 64 sampling intervals
\(em that is, 8\ msec at an 8\ kHz sampling rate.
.sh "6.4  Pitch estimation"
.pp
It is sometimes useful to think of linear prediction as a kind of
curve-fitting technique.
Figure 6.16 illustrates how four samples of a speech signal can predict
the next one.
.FC "Figure 6.16"
In essence, a curve is drawn through four points
to predict the position of the fifth, and only the prediction error
is actually transmitted.  Now if the order of linear prediction
is high enough (at least 10), and if the coefficients are chosen
correctly, the prediction will closely model the resonances of the
vocal tract.  Thus the error will actually be zero, except at pitch
pulses.
.pp
Figure 6.17 shows a segment of voiced speech together with the prediction
error (often called the prediction residual).
.FC "Figure 6.17"
It is apparent that the
error is indeed small, except at pitch pulses.
This suggests that a good way to determine the pitch period is to examine
the error signal, perhaps by looking at its autocorrelation function.
As with all pitch detection methods, one must be
careful:  spurious peaks can occur, especially in nasal sounds when
the all-pole model provided by linear prediction fails.  Continuity
constraints, which use previous values of pitch period when determining
which peak to accept as a new pitch impulse, can eliminate many of these
spurious peaks.  Unvoiced speech should produce an error signal with no
prominent peaks, and this needs to be detected.
Voiced fricatives are a difficult case:  peaks should be present
but the general noise level of the error signal will be greater than
it is in
purely voiced speech.
Such considerations have been taken into account in a practical pitch
estimation system based upon this technique (Markel, 1972).
.[
Markel 1972 SIFT
.]
.pp
This method of pitch detection highlights another advantage of the lattice
analysis technique.  When using autocorrelation or covariance analysis to
determine the filter (or reflection) coefficients, the error signal is not
normally produced.  It can, of course, be found by taking the speech samples
which constitute the current frame and running them through an analysis
filter whose parameters are those determined by the analysis, but this
is a computationally demanding exercise, for the filter must run at the
speech sampling rate (say 8\ kHz) instead of at the frame rate (say 50\ Hz).
Usually, pitch is estimated by other methods, like those discussed in
Chapter 4, when using autocorrelation or covariance linear prediction.
However, we have seen above that with the lattice method, the error
signal is produced as a byproduct:  it appears at the right-hand end
of the  upper path of the lattice chain.  Thus it is already available
for use in determining pitch periods.
.sh "6.5  Parameter coding for linear predictive storage or transmission"
.pp
In this section, the coding requirements of linear predictive parameters
will be examined.  The parameters that need to be stored or transmitted
are:
.LB
.NP
pitch
.NP
voiced-unvoiced flag
.NP
overall amplitude level
.NP
filter coefficients or reflection coefficients.
.LE
The first three are parameters of the excitation source.
They can be derived directly from the error signal as indicated above, if
it is generated (as it is in lattice implementations); or by other
methods if no error signal is calculated.
The filter or reflection coefficients are, of course, the main product
of linear predictive analysis.
.pp
It is generally agreed that around 60 levels, logarithmically spaced,
are needed to represent pitch for telephone quality speech.
The voiced-unvoiced indication requires one bit, but since pitch is
irrelevant in unvoiced speech it can be coded as one of the pitch
levels.  For example, with 6-bit coding of pitch, the value 0 can be
reserved to indicate unvoiced speech, with values 1\-63 indicating the
pitch of voiced speech.
The overall gain has not been discussed above:  it is simply the average
amplitude of the error signal.  Five bits on a logarithmic scale
are sufficient to represent it.
.pp
Filter coefficients are not very amenable to quantization.  At least
8\-10\ bits are required for each one.  However, reflection coefficients
are better behaved, and 5\-6\ bits each seems adequate.  The number of
coefficients that must be stored or transmitted is the same as the
order of the linear prediction:  10 is commonly used for low-quality
speech, with as many as 15 for higher qualities.
.pp
These figures give around 100\ bits/frame for a 10'th order system using
filter coefficients, and around 65\ bits/frame for a 10'th order system
using reflection coefficients.  Frame lengths vary between 10\ msec
and 25\ msec, depending on the quality desired.  Thus for 20\ msec frames,
the data rates work out at around 5000\ bit/s using filter coefficients,
and 3250\ bit/s using reflection coefficients.
.pp
Substantially lower data rates can be achieved by more careful
coding of parameters.  In 1976, the US Government defined a standard
coding scheme for 10-pole linear prediction with a data rate of
2400\ bit/s \(em conveniently chosen as one of the
commonly-used rates for serial data transmission.
This standard, called LPC-10, tackles the difficult problem of
protection against transmission errors (Fussell
.ul
et al,
1978).
.[
Fussell Boudra Abzug Cowing 1978
.]
.pp
Whenever data rates are reduced, redundancy inherent in the signal is
necessarily lost and so the effect of transmission errors becomes
greatly magnified.
For example, a single corrupted sample in PCM transmission of speech
will probably not be noticed, and even a short burst of errors will be
perceived as a click which can readily be distinguished from the speech.
However, any error in LPC transmission will last for one entire
frame \(em say 20\ msec \(em and worse still, it will be integrated into the
speech signal and not easily discriminated from it by the listener's brain.
A single corruption may, for example, change a voiced frame into an
unvoiced one, or vice versa.  Even if it affects only 
a reflection coefficient it will change the resonance characteristics
of that frame, and change them in a way that does not simply sound like
superimposed noise.
.pp
Table 6.1 shows the LPC-10 coding scheme.
.RF
.in+0.1i
.ta 2.0i +1.8i +0.6i
.nr x1 (\w'voiced sounds'/2)
.nr x2 (\w'unvoiced sounds'/2)
.ul
	\h'-\n(x1u'voiced sounds	\h'-\n(x2u'unvoiced sounds
.sp
pitch/voicing	7	7	60 pitch levels, Hamming
			\h'\w'00 'u'and Gray coded
energy	5	5	logarithmically coded
$k sub 1$	5	5	coded by table lookup
$k sub 2$	5	5	coded by table lookup
$k sub 3$	5	5
$k sub 4$	5	5
$k sub 5$	4	\-
$k sub 6$	4	\-
$k sub 7$	4	\-
$k sub 8$	4	\-
$k sub 9$	3	\-
$k sub 10$	2	\-
synchronization	1	1	alternating 1,0 pattern
error detection/	\-	\h'-\w'0'u'21
correction
	\h'-\w'__'u+\w'0'u'__	\h'-\w'__'u+\w'0'u'__
.sp
	\h'-\w'0'u'54	\h'-\w'0'u'54
.sp
.ta 0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i
	frame rate: 44.4\ Hz (22.5\ msec frames)
.in 0
.FG "Table 6.1  Bit requirements for each parameter in LPC-10 coding scheme"
Different coding is used for voiced and unvoiced frames.
Only four reflection coefficients are transmitted for unvoiced frames,
because it has been determined that no perceptible increase in speech quality
occurs when more are used.
The bits saved are more fruitfully employed to provide error detection
and correction for the other parameters.
Seven bits are used for pitch and the voiced-unvoiced flag, and they are
redundant in that only 60 possible pitch values are
allowed.
Most transmission errors in this field will be detected by the receiver;
which can then use an estimate of pitch based on previous values and
discard the erroneous one.  Pitch values are also Gray coded so that
even if errors are not detected, there is a good chance that an adjacent
pitch value is read instead.
Different numbers of bits are allocated to the various reflection
coefficients:  experience shows that the lower-numbered ones contribute
most highly to intelligibility and so these are quantized most finely.
In addition, a table lookup operation is performed on the code
generated for the first two, providing a non-linear quantization which is
chosen to minimize the error on a statistical basis.
.pp
With 54\ bits/frame and 22.5\ msec frames, LPC-10 requires a 2400\ bit/s
data rate.  Even lower rates have been used successfully for lower-quality
speech.  The Speak 'n Spell toy, described in Chapter 11, has an
average data rate of 1200\ bit/s.  Rates as low as 600\ bit/s have
been achieved (Kang and Coulter, 1976) by pattern recognition techniques operating
on the reflection coefficients:  however, the speech quality is not good.
.[
Kang Coulter 1976
.]
.sh "6.6  References"
.LB "nnnn"
.[
$LIST$
.]
.LE "nnnn"
.sh "6.7  Further reading"
.pp
Most recent books on digital signal processing contain some information
on linear prediction (see Oppenheim and Schafer, 1975; Rabiner and Gold, 1975;
and Rabiner and Schafer, 1978; all referenced at the end of Chapter 4).
.LB "nn"
.\"Atal-1971-1
.]-
.ds [A Atal, B.S.
.as [A " and Hanauer, S.L.
.ds [D 1971
.ds [T Speech analysis and synthesis by linear prediction of the acoustic wave
.ds [J JASA
.ds [V 50
.ds [P 637-655
.nr [P 1
.ds [O August
.nr [T 0
.nr [A 1
.nr [O 0
.][ 1 journal-article
.in+2n
This paper is of historical importance because it introduced the idea
of linear prediction to the speech processing community.
.in-2n
.\"Makhoul-1975-2
.]-
.ds [A Makhoul, J.I.
.ds [D 1975
.ds [K *
.ds [T Linear prediction: a tutorial review
.ds [J Proc IEEE
.ds [V 63
.ds [N 4
.ds [P 561-580
.nr [P 1
.ds [O April
.nr [T 0
.nr [A 1
.nr [O 0
.][ 1 journal-article
.in+2n
An interesting, informative, and readable survey of linear prediction.
.in-2n
.\"Markel-1976-3
.]-
.ds [A Markel, J.D.
.as [A " and Gray, A.H.
.ds [D 1976
.ds [T Linear prediction of speech
.ds [I Springer Verlag
.ds [C Berlin
.nr [T 0
.nr [A 1
.nr [O 0
.][ 2 book
.in+2n
This is the only book which is entirely devoted to linear prediction of speech.
It is an essential reference work for those interested in the subject.
.in-2n
.\"Wiener-1947-4
.]-
.ds [A Wiener, N.
.ds [D 1947
.ds [T Extrapolation, interpolation and smoothing of stationary time series
.ds [I MIT Press
.ds [C Cambridge, Massachusetts
.nr [T 0
.nr [A 1
.nr [O 0
.][ 2 book
.in+2n
Linear prediction is often thought of as a relatively new technique,
but it is only its application to speech processing that is novel.
Wiener develops all of the basic mathematics used in linear prediction
of speech, except the lattice filter structure.
.in-2n
.LE "nn"
.EQ
delim $$
.EN
.CH "7  JOINING SEGMENTS OF SPEECH"
.ds RT "Joining segments of speech
.ds CX "Principles of computer speech
.pp
The obvious way to provide speech output from computers
is to select the basic acoustic units to be used; record them;
and generate utterances by concatenating together appropriate segments
from this pre-stored inventory.
The crucial question then becomes, what are the basic units?
Should they be whole sentences, words, syllables, or phonemes?
.pp
There are several trade-offs to be considered here.
The larger the units, the more utterances have to be stored.
It is not so much the length of individual utterances that is of concern,
but rather their variety, which tends to increase exponentially instead
of linearly with the size of the basic unit.  Numbers provide an
easy example:  there are $10 sup 7$ 7-digit telephone numbers, and it is
certainly infeasible to record each one individually.
Note that as storage technology improves the limitation is becoming
more and more one of recording the utterances in the first place rather
than finding somewhere to store them.
At a PCM data rate of 50\ Kbit/s, a 100\ Mbyte disk can hold over 4\ hours
of continuous speech.
With linear predictive coding at 1\ Kbit/s it holds 0.8 of a
megasecond \(em well over a week.  And this is a 24-hour 7-day week,
which corresponds to a working month; and continuous speech \(em without
pauses \(em which probably requires another factor of five for
production by a person.
Setting up a recording session to fill the disk would be a formidable
task indeed!
Furthermore, the use of videodisks \(em which will be common domestic items
by the end of the decade \(em could increase these figures by a factor of 50.
.pp
The word seems to be a sensibly-sized basic unit.
Many applications use a rather limited vocabulary \(em 190 words
for the airline reservation system described in Chapter 1.
Even at PCM data rates, this will consume less than 0.5\ Mbyte of
storage.
Unfortunately, coarticulation and prosodic factors now come into play.
.pp
Real speech is connected \(em there are few gaps between words.
Coarticulation, where sounds are affected by those on either side,
naturally operates across word boundaries.
And the time constants of coarticulation are associated with the
mechanics of the vocal tract and hence measure tens or hundreds
of msec.  Thus the effects straddle several pitch periods (100\ Hz pitch
has 10\ msec period) and cannot be simulated by simple interpolation of the
speech waveform.
.pp
Prosodic features \(em notably pitch and rhythm \(em span much longer
stretches of speech than single words.  As far as most speech output
applications are concerned, they operate at the utterance level of
a single, sentence-sized, information unit.  They cannot be
accomodated if speech waveforms of individual words of
the utterance are stored,
for it is rarely feasible to alter the fundamental
frequency or duration of a time waveform without changing all the formant
resonances as well.
However, both word-to-word coarticulation and the essential features
of rhythm and intonation can be incorporated if the stored words are
coded in source-filter form.
.pp
For more general applications of speech output, the limitations of
word storage soon become apparent.  Although people's daily
vocabularies are not large, most words have a variety
of inflected forms which need to be treated separately if a strict
policy is adopted of word storage.  For instance, in this book
there are 84,000 words, and 6,500 (8%) different ones (counting
inflected forms).
In Chapter 1 alone, there are 6,800 words and 1,700 (25%) different ones.
.pp
It seems crazy to treat a simple inflection like "$-s$" or its voiced
counterpart, "$-z$" (as in "inflection\c
.ul
s\c
"),
as a totally different word from the base form.
But once you consider storing roots and endings separately,
it becomes apparent
that there is a vast number of different endings, and it is difficult to know
where to draw the line.  It is natural to think instead of simply
using the syllable as the basic unit.
.pp
A generous estimate of the number of different syllables in English is 10,000.
At three a second, only about an
hour's storage is required for them all.  But waveform storage
will certainly not do.
Although coarticulation effects between words are needed to make
speech sound fluent, coarticulation between syllables is necessary
for it even to be
.ul
comprehensible.
Adopting a source-filter form of representation is essential, as is
some scheme of interpolation between syllables which simulates
coarticulation.
Unfortunately, a great deal of acoustic action occurs at syllable
boundaries \(em stops are exploded, the sound source changes
between voicing and frication, and so on.  It may be more appropriate
to consider inverse syllables, comprising a vowel-consonant-vowel sequence
instead of consonant-vowel-consonant.
(These have jokingly been dubbed "lisibles"!)
.pp
There is again some considerable practical difficulty in creating
an inventory of syllables, or lisibles.
Now it is not so much the recording that is impractical, but
the editing needed to ensure that the cuts between syllables are made
at exactly the right point.  As units get smaller, the exact
placement of the boundaries becomes ever more critical; and several thousand
sensitive editing jobs is no easy task.
.pp
Since quite general effects of coarticulation must be accomodated
with syllable synthesis, there will not necessarily be significant
deterioration if smaller, demisyllable, units are employed.
This reduces the segment inventory to an estimated 1000\-2000 entries,
and the tedious job of editing each one individually becomes at
least feasible, if not enviable.
Alternatively, the segment inventory could be created by artificial
means involving cut-and-try experiments with resonance parameters.
.pp
The ultimate in economy of inventory size, of course, is to use
phonemes as the basic unit.  This makes the most critical
part of the task interpolation between units, rather than their
construction or recording.  With only about 40 phonemes
in English, each one can be examined in many different contexts to
ascertain the best data to store.
There is no need to record them directly from a human voice \(em it
would be difficult anyway for most cannot be produced in isolation.
In fact, a phoneme is an abstract unit, not a particular sound
(recall the discussion of phonology in Chapter 2), and so it is
most appropriate that data be abstracted from several different
realizations rather than an exact record made of any one.
.pp
If information is stored about phonological units of
speech \(em phonemes \(em the difficult task of phonological-to-phonetic
conversion must necessarily be performed automatically.
Allophones are created by altering the transitions between units,
and to a lesser extent by modifying the central parts of the units
themselves.
The rules for making transitions will have a big effect on the
quality of the resulting speech.
Instead of trying to perform this task automatically by a computer
program, the allophones themselves could be stored.  This will
ease the job of generating transitions between segments, but
will certainly not eliminate it.
The total number of allophones will depend on the narrowness of the
transcription system:  60\-80 is typical, and it is unlikely to exceed
one or two hundred.  In any case there will not be a storage problem.
However, now the burden of producing an allophonic transcription
has been transferred to the person who codes the utterance prior
to synthesizing it.  If he is skilful and patient, he should
be able to coax the system into producing fairly understandable
speech, but the effort required for this on a per-utterance basis
should not be underestimated.
.RF
.nr x0 \w'sentences  '
.nr x1 \w'  '
.nr x2 \w'depends on  '
.nr x3 \w'generalized or  '
.nr x4 \w'natural speech  '
.nr x5 \w'author of segment'
.nr x6 \n(x0u+\n(x1u+\n(x2u+\n(x3u+\n(x4u+\n(x5u
.nr x7 (\n(.l-\n(x6)/2
.in \n(x7u
.ta \n(x0u +\n(x1u +\n(x2u +\n(x3u +\n(x4u
	|	size of	storage	source of	principal
	|	utterance	method	utterance	burden is
	|	inventory		inventory	placed on
	|\h'-1.0i'\l'\n(x6u\(ul'
	|
sentences	|	depends on	waveform or	natural speech	recording artist,
	|	application	source-filter		storage medium
	|		parameters
	|
words	|	depends on	source-filter	natural speech	recording artist
	|	application	parameters		and editor,
	|				storage medium
	|
syllables/	|	\0\0\010000	source-filter	natural speech	recording editor
  lisibles	|		parameters
	|
demi-	|	\0\0\0\01000	source-filter	natural speech	recording editor
  syllables	|		parameters	or artificially	or inventory
	|			generated	compiler
	|
phonemes	|	\0\0\0\0\0\040	generalized	artificially	author of segment
	|		parameters	generated	concatenation
	|				program
	|
allophones	|	\0\050\-100	generalized or	artificially	coder of
	|		source-filter	generated or	synthesized
	|		parameters	natural speech	utterances
	|\h'-1.0i'\l'\n(x6u\(ul'
.in 0
.ta 0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i
.FG "Table 7.1  Some issues relevant to choice of basic unit"
.pp
Table 7.1 summarizes in broad brush-strokes the issues which relate to the
choice of basic unit for concatenation.
The sections which follow provide more detail about the different
methods of joining segments of speech together.
Only segmental aspects are considered, for the important problems of
prosody will be treated in the next chapter.
All of the methods rely to some extent on the acoustic properties of speech,
and as smaller basic units are considered the role of speech acoustics
becomes more important.
It is impossible in a book like this to give a detailed account of acoustic
phonetics, for it would take several volumes!
What I aim to do in the following pages is to highlight some salient features
which are relevant to segment concatenation, without attempting to be
complete.
.sh "7.1  Word concatenation"
.pp
For general speech output, word concatenation is an inherently limited
technique because of the large number of phonetically different words.
Despite this fact, it is at present the most widely-used synthesis
method, and is likely to remain so for several years.
We have seen that the primary problems are word-to-word
coarticulation and prosody; and both can be overcome, at least to a useful
approximation, by coding the words in source-filter form.
.rh "Time-domain techniques."
Nevertheless, a surprising number of applications simply store
the time waveform, coded, usually, by one of the techniques described in
Chapter 3.
From an implementation point of view there are many advantages to this.
Speech quality can easily be controlled by selecting a suitable sampling
rate and coding scheme.
A natural-sounding voice is guaranteed; male or female as desired.
The equipment required is minimal \(em a digital-to-analogue
converter and post-sampling filter will do for synthesis if
PCM coding is used, and
DPCM, ADPCM, and delta modulation decoders are not much more complicated.
.pp
From a speech point of view, the resulting utterances can never be made
convincingly fluent.
We discussed the early experiments of Stowe and Hampton (1961)
at the beginning of Chapter 3.
.[
Stowe Hampton 1961
.]
A major drawback to word concatenation in the
analogue domain is the introduction of clicks and other interference
between words:  it is difficult to prevent the time waveform transitions
from adding extraneous sounds.
This poses no problem with digital storage, however, for the waveforms
can be edited accurately prior to storage so that they start
and finish at an exactly
zero level.
Rather, the lack of fluency stems from the absence of proper control
of coarticulation and prosody.
.pp
But this is not necessarily a serious drawback if the application is
a sufficiently limited one.  Complete, invariant utterances can be
stored as one unit.  Often they must contain data-dependent
slot-fillers, as in
.LB
This flight makes \(em stops
.LE
and
.LB
Flight number \(em leaves \(em at \(em , arrives in \(em at \(em
.LE
(taken from the airline reservation system of Chapter 1
(Levinson and Shipley, 1980)).
.[
Levinson Shipley 1980
.]
Then, each slot-filling word is recorded in an intonation consistent
both with its position in the template utterance and with the
intonation of that utterance.
This could be done by embedding the word in the utterance
for recording, and excising it by digital editing before storage.
It would be dangerous to try to take into account coarticulation effects,
for the coarticulation could not be made consistent with both the
several slot-fillers and the single template.
This could be overcome if several versions of the template were stored,
but then the scheme becomes subject to combinatorial explosion
if there is more than one slot in a single utterance.
But it is not really necessary, for the lack of fluency will probably
be interpreted by a benevolent listener as an attempt to convey the
information as clearly as possible.
.pp
Difficulties will occur if the same slot-filler is used in different
contexts.  For instance, the first gap in each of the sentences above
contains a number; yet the intonation of that number is different.
Many systems simply ignore this problem.
Then one does notice anomalies, if one is attentive:  the words come,
as it were, from different mouths, without fluency.
However, the problem is not necessarily acute.  If it is, two or more
versions of each slot-filler can be recorded, one for each context.
.pp
As an example, consider the synthesis of 7-digit telephone numbers,
like 289\-5371.  If one version only of each digit is stored,
it should be recorded in a level tone of voice.  A pause should be
inserted after the third digit of the synthetic number, to accord
with common elocution.  The result will certainly be unnatural, although
it should be clear and intelligible.
Any pitch errors in the recordings will make certain numbers
audibly anomalous.
At the other extreme, 70 single digits could be stored, one version of
each digit for each position in the number.  The recording will be
tedious and error-prone, and the synthetic utterances will still not
be fluent \(em for coarticulation is ignored \(em but instead
unnaturally clearly enunciated.  A compromise is to record only
three versions of each digit, one for any of the
five positions
.nr x1 \w'\(ul'
.nr x2 (8*\n(x1)
.nr x3 0.2m
\zx\h'\n(x1u'\zx\h'\n(x1u'\h'\n(x1u'\z\-\h'\n(x1u'\zx\h'\n(x1u'\zx\h'\n(x1u'\c
\zx\h'\n(x1u'\h'\n(x1u'\v'\n(x3u'\l'-\n(x2u\(ul'\v'-\n(x3u' ,
another one for the third position
\h'\n(x1u'\h'\n(x1u'\zx\h'\n(x1u'\z\-\h'\n(x1u'\h'\n(x1u'\c
\h'\n(x1u'\h'\n(x1u'\h'\n(x1u'\v'\n(x3u'\l'-\n(x2u\(ul'\v'-\n(x3u' ,
and the last for the final position
\h'\n(x1u'\h'\n(x1u'\h'\n(x1u'\z\-\h'\n(x1u'\h'\n(x1u'\c
\h'\n(x1u'\h'\n(x1u'\zx\h'\n(x1u'\v'\n(x3u'\l'-\n(x2u\(ul'\v'-\n(x3u' .
The first version will be in a level voice, the second an
incomplete, rising tone; and the third a final, dropping pitch.
.rh "Joining formant-coded words."
The limitations of the time-domain method are lack of
fluency caused by unnatural transitions between words, and the
combinatorial explosion created by recording slot-fillers several times
in different contexts.
Both of these problems can be alleviated by storing formant tracks,
concatenating them with suitable interpolation, and applying a complete
pitch contour suitable for the whole utterance.
But one can still not generate conversational speech, for natural speech
rhythms cause non-linear warpings of the time axis which cannot reasonably
be imitated by this method.
.pp
Solving problems often creates others.
As we saw in Chapter 4, it is not easy to obtain reliable formant tracks
automatically.  Yet hand-editing of formant parameters adds a whole new
dimension to the problem of vocabulary construction, for it is
an exceedingly tiresome and time-consuming task.
Even after such tweaking, resynthesized utterances will be degraded
considerably from the original, for the source-filter model is by no means
a perfect one.
A hardware or real-time software formant synthesizer must be added
to the system, presenting design problems and creating extra cost.
Should a serial or parallel synthesizer be used? \(em the latter offers
potentially better speech (especially in nasal sounds), but requires
additional parameters, namely formant amplitudes, to be estimated.
Finally, as we will see in the next chapter, it is not an easy matter to
generate a suitable pitch contour and apply it to the utterance.
.pp
Strangely enough, the interpolation itself does not present any great
difficulty, for there is not enough information in the formant-coded
words to make possible sophisticated coarticulation.
The need for interpolation is most pressing when one word ends with
a voiced sound and the next begins with one.
If either the end of the first or the beginning of the second word
(or both) is unvoiced, unnatural formant transitions do not matter
for they will not be heard.
Actually, this is only strictly true for fricative transitions:  if
the juncture is aspirated then formants will be perceived in the
aspiration.  However,
.ul
h
is the only fully aspirated sound in English,
and it is relatively uncommon.
It is not absolutely necessary to interpolate the fricative filter resonance,
because smooth transitions from one fricative sound to another are rare
in natural speech.
.pp
Hence unless both sides of the junction are voiced, no interpolation
is needed:  simple abuttal of the stored parameter tracks will do.
Note that this is
.ul
not
the same as joining time waveforms, for the synthesizer
will automatically ensure a relatively smooth transition from one
segment to another because of energy storage in the filters.
A new set of resonance parameters for the formant-coded words will be stored
every 10 or 20 msec (see Chapter 5), and so the transition will automatically
be smoothed over this time period.
.pp
For voiced-to-voiced transitions, some interpolation is needed.
An overlap period of duration, say, 50\ msec, is established, and
the resonance parameters in the final 50\ msec of the first word are
averaged with those in the first 50\ msec of the second.
The average is weighted, with the first word's formants dominating
at the beginning and their effect progressively dying out
in favour of the second word.
.pp
More sophisticated than a simple average is to weight the components
according to how rapidly they are changing.
If the spectral change in one word is much greater than that in the
other, we might expect that this will dominate the transition.
A simple measure of spectral derivative at any given time can be found
by adding the magnitude of the discrepancies in each formant frequency
between one sample and the next.
The spectral change in the transition region can be obtained by summing
the spectral derivatives at each sample in the region.
Such a measure can perhaps be made more accurate by taking into
account the relative importance of the formants, but will probably
never be more than a rough and ready yardstick.
At any rate, it can be used to load the average in favour of the
dominant side of the junction.
.pp
Much more important for naturalness of the speech are the effects
of rhythm and intonation, discussed in the next chapter.
.pp
Such a scheme has been implemented and tested on \(em guess what! \(em 7-digit
telephone numbers (Rabiner
.ul
et al,
1971).
.[
Rabiner Schafer Flanagan 1971
.]
Significant improvement (at the 5% level of statistical
significance) in people's
ability to recall numbers was found for this method over direct
abuttal of either natural or synthetic versions of the digits.
Although the method seemed, on balance, to produce utterances that were
recalled less accurately than completely natural spoken
telephone numbers, the difference was not significant (at the 5% level).
The system was also used to generate wiring instructions by computer
directly from the connection list, as described in Chapter 1.
As noted there, synthetic speech was actually preferred to natural speech
in the noisy environment of the production line.
.rh "Joining linear predictive coded words."
Because obtaining accurate formant tracks for natural utterances
by Fourier transform methods is difficult, it is worth considering
the use of linear prediction as the source-filter model.
Actually, formant resonances can be extracted from linear predictive
coefficients quite easily, but there is no need to do this because
the reflection coefficients themselves are quite suitable
for interpolation.
.pp
A slightly different interpolation scheme from that described in the
previous section has been reported (Olive, 1975).
.[
Olive 1975
.]
The reflection coefficients were spliced during an overlap region of
only 20\ msec.
More interestingly, attempts were made to suppress the plosive bursts
of stop sounds in cases where they were followed by another stop at
the beginning of the next word.
This is a common coarticulation, occurring, for instance, in the phrase
"stop burst".  In running speech, the plosion on the
.ul
p
of "stop" is
normally suppressed because it is followed by another stop.
This is a particularly striking case because the place of articulation
of the two stops
.ul
p
and
.ul
b
is the same:  complete suppression is not as likely
to happen in "stop gap", for example (although it may occur).
Here is an instance of how extra information could improve the
quality of the synthetic transitions considerably.
However, automatically identifying the place of articulation of stops is
a difficult job, of a complexity far above what is appropriate for
simply joining words stored in source-filter form.
.pp
Another innovation was introduced into the transition between two
vowel sounds, when the second word began with an accented syllable.
A glottal stop was placed at the juncture.
Although the glottal stop was not described in Chapter 2, it is a sound
used in many dialects of English.  It frequently occurs
in the utterance "uh-uh", meaning "no".  Here it
.ul
is
used to separate two vowel sounds, but in fact this is not particularly
common in most dialects.
One could say "the apple", "the orange", "the onion" with a neutral vowel
in "the" (to rhyme with "\c
.ul
a\c
bove") and a glottal stop as separator,
but it is much more usual to rhyme "the" with "he" and introduce a
.ul
y
between the words.
Similarly, even speakers who do not normally pronounce an
.ul
r
at the
end of words will introduce one in "bigger apple", rather than
using a glottal stop.
Note that it would be wrong to put an
.ul
r
in "the apple", even
for speakers who usually terminate "the" and "bigger" with the same sound.
Such effects occur at a high level of processing, and are practically
impossible to simulate with word-interpolation rules.
Hence the expedient of introducing a glottal stop is a good one, although
it is certainly unnatural.
.sh "7.2  Concatenating whole or partial syllables"
.pp
The use of segments larger than a single phoneme or allophone but smaller
than a word as the basic unit for speech synthesis has an interesting
history.
It has long been realized that transitions between phonemes are
extremely sensitive and critical components of speech, and thus are
essential for successful synthesis.
Consider the unvoiced stop sounds
.ul
p, t,
and
.ul
k.
Their central portion is actually silence!  (Try saying a word like
"butter" with a very long
.ul
t.\c
)  Hence
in this case it is
.ul
only
the transitional information which can distinguish these sounds from
each other.
.pp
Sound segments which comprise the transition from the centre of one phoneme
to the centre of the next are called
.ul
dyads
or
.ul
diphones.
The possibility of using them as the basic units for concatenation
was first mooted in the mid 1950's.
The idea is attractive because there is relatively little spectral
movement in the central, so-called "steady-state", portion of many
phonemes \(em in the extreme case of unvoiced stops there is not only
no spectral movement, but no spectrum at all in the steady state!
At that time the resonance synthesizer was in its infancy, and
so recorded segments of live speech were used.  The early experiments
met with little success because of the technical difficulties
of joining analogue waveforms and inevitable discrepancies between
the steady-state parts of a phoneme recorded in different contexts \(em not
to mention the problems of coarticulation and prosody which effectively
preclude the use of waveform concatenation at such a low level.
.pp
In the mid 1960's, with the growing use of resonance synthesizers,
it became possible to generate diphones by copying resonance parameters
manually from a spectrogram, and improving the result by trial and error.
It was not feasible to extract formant frequencies automatically from real
speech, though, because the fast Fourier transform was not yet widely
known and the computational burden of slow Fourier transformation was
prohibitive.
For example, a project at IBM stored manually-derived parameter tracks
for diphones, identified by pairs of phoneme names (Dixon and Maxey, 1968).
.[
Dixon Maxey 1968
.]
To generate a synthetic utterance it was coded in
phonetic form and used to access
the diphone table to give a set of parameter tracks for the complete
utterance.  Note that this is the first system we have encountered
whose input is a phonetic transcription which relates to an inventory
of truly synthetic character:  all previous schemes used recordings of
live speech, albeit processed in some form.
Since the inventory was synthetic, there was no difficulty in ensuring
that discontinuities did not arise between segments beginning and ending with
the same phoneme.  Thus interpolation was irrelevant, and the synthesis
procedure concentrated on prosodic questions.  The resulting speech
was reported to be quite impressive.
.pp
Strictly speaking, diphones are not demisyllables but phoneme pairs.
In the simplest case they happen to be similar, for two primary diphones
characterize a consonant-vowel-consonant syllable.
There is an advantage to using demisyllables rather than diphones as the basic
unit, for many syllables begin or end with complicated consonant clusters
which are not easy to produce convincingly by diphone
concatenation.
But they are not easy to produce by hand-editing resonance parameters
either!
Now that speech analysis methods have been developed and refined,
resonance parameters or linear predictive coefficients
can be extracted automatically
from natural utterances, and there has been a resurgence of interest in
syllabic and demisyllabic synthesis methods.  The wheel has turned
full circle, from segments of natural speech to hand-tailored parameters
and back again!
.pp
The advantage of storing demisyllables over syllables (or lisibles) from
the point of view of storage capacity has already been pointed out
(perhaps 1,000\-2,000 demisyllables as opposed to 4,000\-10,000 syllables).
But it is probably not too significant with the continuing decline
of storage costs.
The requirements are of the order of 25\ Kbyte versus 0.5\ Mbyte
for 1200\ bit/s linear predictive coding, and the latter could
almost be accomodated today \(em 1981 \(em on a state-of-the-art
read-only memory chip.
A bigger advantage comes from rhythmic considerations.
As we will see in the next chapter, the rhythms of fluent speech cause
dramatic variations in syllable duration, but these seem to affect
the vowel and closing consonant cluster much more than the initial consonant
cluster.  Thus if a demisyllable is deemed to begin shortly (say 60\ msec)
after onset of the vowel, when the formant structure has settled down,
the bulk of the vowel and the closing consonant cluster will form a
single demisyllable.  The opening cluster of the next syllable will lie
in the next demisyllable.  Then differential lengthening can be applied
to that part of the syllable which tends to be stretched in live speech.
.pp
One system for demisyllable concatenation has produced excellent results
for monosyllabic English words (Lovins and Fujimura, 1976).
.[
Lovins Fujimura 1976
.]
Complex word-final consonant clusters are excluded from the inventory by
using syllable affixes
.ul
s, z, t,
and
.ul
d;
these are attached to the
syllabic core as a separate exercise (Macchi and Nigro, 1977).
.[
Macchi Nigro 1977
.]
Prosodic rather than segmental considerations are likely to prove the major
limiting factor when this scheme is extended to running speech.
.pp
Monosyllabic words spoken in isolation are coded as linear predictive
reflection coefficients, and segmented by digital editing into the initial
consonant cluster and the vocalic nucleus plus final cluster.
The cut is made 60\ msec into the vowel, as suggested above.
This minimizes the difficulty of interpolation when concatenating
segments, for there is ample voicing on either side of the juncture.
The reflection coefficients should not differ radically because the
vowel is the same in each demisyllable.
A 40\ msec overlap is used, with the usual linear interpolation.
An alternative smoothing rule applies when the second segment has
a nasal or glide after the vowel.  In this case anticipatory coarticulation
occurs, affecting even the early part of the vowel.  For example, a vowel
is frequently nasalized when followed by a nasal sound \(em even in English
where nasalization is not a distinctive feature in vowels (see Chapter 2).
Under these circumstances the overlap area is moved forward in time so
that the colouration applies throughout almost the whole vowel.
.sh "7.3  Phoneme synthesis"
.pp
Acoustic phonetics is the study of how the acoustic
signal relates to the phonetic sequence which was spoken or heard.
People \(em especially engineers \(em often ask, how could phonetics not
be acoustic?  In fact it can be articulatory, auditory, or linguistic
(phonological), for example, and we have touched on the first and last
in Chapter 2.
The invention of the sound spectrograph in the late 1940's was an
event of colossal significance for acoustic phonetics, for it somehow
seemed to make the intricacies of speech visible.
(This was thought to be a greater advance than actually turned
out:  historically-minded readers should refer to Potter
.ul
et al,
1947,
for an enthusiastic contemporary appraisal of the invention.)  A
.[
Potter Kopp Green 1947
.]
result of several years of research at Haskins Laboratories in New York
during the 1950's was a set of "minimal rules for synthesizing speech",
which showed how stylized formant patterns could generate cues for
identifying vowels and, particularly, consonants
(Liberman, 1957; Liberman
.ul
et al,
1959).
.[
Liberman 1957 Some results of research on speech perception
.]
.[
Liberman Ingemann Lisker Delattre Cooper 1959
.]
.pp
These were to form the basis of many speech synthesis-by-rule computer
programs in the ensuing decades.  Such programs take as input a
phonetic transcription of the utterance and generate a spoken version
of it.  The transcription may be broad or narrow, depending on the
system.  Experience has shown that the Haskins rules really are
minimal, and the success of a synthesis-by-rule program depends on
a vast collection of minutia, each seemingly insignificant in isolation
but whose effects combine to influence the speech quality dramatically.
The best current systems produce clearly understandable
speech which is nevertheless something of a strain to listen to for
long periods.
However, many are not good; and some are execrable.
In recent times commercial influences have unfortunately restricted
the free exchange of results and programs between academic researchers,
thus slowing down progress.
Research attention has turned to prosodic factors,
which are certainly less well understood than segmental ones, and
to synthesis from plain English text rather than from phonetic transcriptions.
.pp
The remainder of this chapter describes the techniques of segmental
synthesis.  First it is necessary to introduce some
elements of acoustic phonetics.
It may be worth re-reading Chapter 2 at this point, to refresh
your memory about the classification of speech sounds.
.sh "7.4  Acoustic characterization of phonemes"
.pp
Shortly after the invention of the sound spectrograph an inverse
instrument was developed, called the "pattern playback" synthesizer.
This took as input a spectrogram, either in its original form or
painted by hand.
An optical arrangment was used to modulate the amplitude of some
fifty harmonically-related oscillators by the lightness or darkness
of each point on the frequency axis of the spectrogram.
As it was drawn past the playing head, sound was produced which
had approximately the frequency components shown on the spectrogram,
although the fundamental frequency was constant.
.pp
This device allowed the complicated
acoustic effects seen on a spectrogram (see for example Figures 2.3 and 2.4)
to be replayed in either original or simplified form.
Hence the features which are important for perception of the different sounds
could be isolated.  The procedure was to copy from an actual spectrogram
the features which were most prominent visually, and then to make further
changes by trial and error until the result was judged to have
reasonable intelligibility when replayed.
.pp
For the purpose of acoustic characterization of particular phonemes,
it is useful to consider the central, steady-state part separately from
transitions into and out of the segment.
The steady-state part is that sound which is heard when the phoneme
is prolonged.  The term "phoneme" is being used in a rather loose sense
here:  it is more appropriate to think of a "sound segment" rather than
the abstract unit which forms the basis of phonological classification,
and this is the terminology I will adopt.
.pp
The essential auditory characteristics of some sound segments are inherent in
their steady states.
If a vowel, for example, is spoken and prolonged, it can readily be
identified by listening to any part of the utterance.
This is not true for diphthongs:  if you say "I" very slowly and freeze
your vocal tract posture at any time, the resulting steady-state sound
will not be sufficient to identify the diphthong.  Rather, it will be
a vowel somewhere between
.ul
aa
(in "had") or
.ul
ar
(in "hard") and
.ul
ee
(in "heed").
Neither is it true for glides, for prolonging
.ul
w
(in "want") or
.ul
y
(in "you") results in vowels resembling respectively
.ul
u
("hood") or
.ul
ee
("heed").
Fricatives, voiced or unvoiced, can be identified from the steady state;
but stops can not, for their's is silent (or \(em in the case
of voiced stops \(em something close to it).
.pp
Segments which are identifiable from their steady state are easy to synthesize.
The difficulty lies with the others, for it must be the transitions which
carry the information.  Thus "transitions" are an essential part of speech,
and perhaps the term is unfortunate for it calls to mind an unimportant
bridge between one segment and the next.
It is tempting to use the words "continuant" and "non-continuant" to distinguish
the two categories; unfortunately they are used by phoneticians in a different
sense.
We will call them "steady-state" and "transient" segments.  The latter term
is not particularly appropriate, for even sounds in this class
.ul
can
be prolonged:  the point is that the identifying information is in the
transitions rather than the steady state.
.RF
.nr x1 (\w'excitation'/2)
.nr x2 (\w'formant resonance'/2)
.nr x3 (\w'fricative'/2)
.nr x4 (\w'frequencies (Hz)'/2)
.nr x5 (\w'resonance (Hz)'/2)
.nr x0 4n+1.7i+0.8i+0.6i+0.6i+1.0i+\w'00'+\n(x5
.nr x6 (\n(.l-\n(x0)/2
.in \n(x6u
.ta 4n +1.7i +0.8i +0.6i +0.6i +1.0i
		\h'-\n(x1u'excitation		\0\0\h'-\n(x2u'formant resonance	\0\0\h'-\n(x3u'fricative
				\0\0\h'-\n(x4u'frequencies (Hz)	\0\0\c
\h'-\n(x5u'resonance (Hz)
\l'\n(x0u\(ul'
.sp
.nr x1 (\w'voicing'/2)
\fIuh\fR	(the)	\h'-\n(x1u'voicing	\0500	1500	2500
\fIa\fR	(bud)	\h'-\n(x1u'voicing	\0700	1250	2550
\fIe\fR	(head)	\h'-\n(x1u'voicing	\0550	1950	2650
\fIi\fR	(hid)	\h'-\n(x1u'voicing	\0350	2100	2700
\fIo\fR	(hod)	\h'-\n(x1u'voicing	\0600	\0900	2600
\fIu\fR	(hood)	\h'-\n(x1u'voicing	\0400	\0950	2450
\fIaa\fR	(had)	\h'-\n(x1u'voicing	\0750	1750	2600
\fIee\fR	(heed)	\h'-\n(x1u'voicing	\0300	2250	3100
\fIer\fR	(heard)	\h'-\n(x1u'voicing	\0600	1400	2450
\fIar\fR	(hard)	\h'-\n(x1u'voicing	\0700	1100	2550
\fIaw\fR	(hoard)	\h'-\n(x1u'voicing	\0450	\0750	2650
\fIuu\fR	(food)	\h'-\n(x1u'voicing	\0300	\0950	2300
.nr x1 (\w'aspiration'/2)
\fIh\fR	(he)	\h'-\n(x1u'aspiration
.nr x1 (\w'frication'/2)
.nr x2 (\w'frication and voicing'/2)
\fIs\fR	(sin)	\h'-\n(x1u'frication				6000
\fIz\fR	(zed)	\h'-\n(x2u'frication and voicing				6000
\fIsh\fR	(shin)	\h'-\n(x1u'frication				2300
\fIzh\fR	(vision)	\h'-\n(x2u'frication and voicing				2300
\fIf\fR	(fin)	\h'-\n(x1u'frication				4000
\fIv\fR	(vat)	\h'-\n(x2u'frication and voicing				4000
\fIth\fR	(thin)	\h'-\n(x1u'frication				5000
\fIdh\fR	(that)	\h'-\n(x2u'frication and voicing				5000
\l'\n(x0u\(ul'
.ta 0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i
.in 0
.FG "Table 7.2  Resonance synthesizer parameters for steady-state sounds"
.rh "Steady-state segments."
Table 7.2 shows appropriate values for the resonance parameters and
excitation sources of a resonance synthesizer, for steady-state
segments only.
There are several points to note about it.
Firstly, all the frequencies involved obviously depend upon the
speaker \(em the size of his vocal tract, his accent and speaking habits.
The values given are nominal ones for a male speaker with a dialect of
British English called "received pronunciation" (RP) \(em for it is what
used to be "received" on the wireless in the old days
before the British Broadcasting Corporation
adopted a policy of more informal, more regional, speech.
Female speakers have formant frequencies approximately 15% higher
than male ones.
Secondly, the third formant is relatively unimportant for vowel
identification; it is
the first and second that give the vowels their character.
Thirdly, formant values for
.ul
h
are not given, for they would be meaningless.
Although it is certainly a steady-state sound,
.ul
h
changes radically
in context.  If you say "had", "heed", "hud", and so on, and freeze
your vocal tract posture on the initial
.ul
h,
you will find it
already configured for the following vowel \(em an excellent
example of anticipatory coarticulation.
Fourthly, amplitude values do play some part in identification,
particularly for fricatives.
.ul
th
is the weakest sound, closely followed by
.ul
f,
with
.ul
s
and
.ul
sh
the
strongest.  It is necessary to get a reasonable mix of excitation in
the voiced fricatives; the voicing amplitude is considerably less than
in vowels.  Finally, there are other sounds that might be considered
steady state ones.  You can probably identify
.ul
m, n,
and
.ul
ng
just by
their steady states.  However, the difference is not particularly
strong; it is the transitional parts which discriminate most effectively
between these sounds.  The steady state of
.ul
r
is quite distinctive, too,
for most speakers, because the top of the tongue is curled back in a
so-called "retroflex" action and this causes a radical change in the
third formant resonance.
.rh "Transient segments."
Transient sounds include diphthongs, glides,
nasals, voiced and unvoiced stops, and affricates.
The first two are relatively easy to characterize, for they are
basically continuous, gradual transitions from one vocal tract posture
to another \(em sort of dynamic vowels.  Diphthongs and glides are
similar to each other.  In fact "you" could be transcribed as
a triphthong,
.ul
i e uu,
except that in the initial posture the tongue
is even higher, and the vocal tract correspondingly more constricted,
than in
.ul
i
("hid") \(em though not as constricted as in
.ul
sh.
Both categories can be represented in terms of target formant
values, on the understanding that these are not to be
interpreted as steady state configurations but strictly as
extreme values at the beginning or end of the formant motion (for
transitions out of and into the segment, respectively).
.pp
Nasals have a steady-state portion comprising a strong nasal formant
at a fairly low frequency, on account of the large size of the
combined nasal and oral cavity which is resonating.
Higher formants are relatively weak, because of attenuation effects.
Transitions into and out of nasals are strongly nasalized,
as indeed are adjacent vocalic segments, with
the oral and nasal tract operating in parallel.  As discussed in
Chapter 5, this cannot be simulated on a series synthesizer.
However, extremely fast motions of the formants occur on account of
the binary switching action of the velum, and it turns out that
fast formant transitions are sufficient to simulate nasals because
the speech perception mechanism is accustomed to hearing them only
in that context!  Contrast this with the extremely slow transitions
in diphthongs and glides.
.pp
Stops form the most interesting category, and research using the pattern
playback synthesizer was instrumental in providing adequate acoustic
characterizations for them.  Consider unvoiced stops.
They each have three phases:  transition in, silent central portion,
and transition out.  There is a lot of action on the transition out
(and many phoneticians would divide this part alone into several "phases").
First, as the release occurs, there is a small burst of fricative noise.
Say "t\ t\ t\ ..." as in "tut-tut", without producing any voicing.
Actually, when used as an admonishment this is accompanied by
an ingressive, inhaling air-stream instead of the normal egressive,
exhaling one used in English speech (although some languages
do have ingressive sounds).
In any case, a short fricative somewhat resembling a tiny
.ul
s
can be heard as the tongue leaves the roof of the mouth.
Frication is produced when the gap is very narrow, and ceases
rapidly as it becomes wider.
Next, when an unvoiced stop is released, a significant amount of aspiration
follows the release.
Say "pot", "tot", "cot" with force and you will hear the
.ul
h\c
-like
aspiration quite clearly.
It doesn't always occur, though; for example you will hear little
aspiration when a fricative like
.ul
s
precedes the stop in the
same syllable, as in "spot", "scot".  The aspiration is a distinguishing
feature between "white spot" and the rather unlikely "White's pot".
It tends to increase as the emphasis on the syllable increases,
and this in an example of a prosodic feature influencing segmental
characteristics.  Finally, at the end of the segment,
the aspiration \(em if any \(em will turn to voicing.
.pp
What has been described applies to
.ul
all
unvoiced stops.
What distinguishes one from another?
The tiny fricative burst will be different because the noise is produced
at different places in the vocal tract \(em at the lips for
.ul
p,
tongue and front of palate for
.ul
t,
and tongue and back of palate for
.ul
k.
The most important difference, however, is the formant motion illuminated
by the last vestiges of voicing at closure and by both aspiration and the
onset of voicing at opening.
Each stop has target formant values which, although
they cannot be heard during the stopped portion (for there is no
sound there), do affect the transitions in and out.
An added complexity is that the target positions themselves vary to some
extent depending on the adjacent segments.
If the stop is heavily aspirated, the vocal posture will have almost
attained that for the following vowel before voicing begins, but
the formant transitions will be perceived because they affect
the sound quality of aspiration.
.pp
The voiced stops
.ul
b, d,
and
.ul
g
are quite similar to their unvoiced analogues
.ul
p, t,
and
.ul
k.
What distinguishes them from each other are the formant transitions to
target positions, heard during closure and opening.
They are distinguished from their unvoiced counterparts by the fact
that more voicing is present:  it lingers on longer at closure
and begins earlier on opening.  Thus little or no aspiration appears
during the opening phase.  If an unvoiced stop is uttered in a context
where aspiration is suppressed, as in "spot", it is almost identical to the
corresponding voiced stop, "sbot".  Luckily no words in English require
us to make a distinction in such contexts.
Voicing sometimes pervades the entire stopped portion of a voiced stop,
especially when it is surrounded by other voiced segments.
When saying a word like "baby" slowly you can choose whether or not to
prolong voicing throughout the second
.ul
b.
If you do, creating what is
called a "voice bar" in spectrograms,
the sound escapes through the cheeks, for
the lips are closed \(em try doing it for a very long time and your cheeks
will fill up with air!
This severely attenuates high-frequency components, and can
be simulated with a weak first formant at a low resonant frequency.
.RF
.nr x0 \w'unvoiced stops:    'u
.nr x1 4n
.nr x2 \n(x0+\n(x1+\w'aspiration burst (context- and emphasis-dependent)'u
.nr x3 (\n(.l-\n(x2)/2
.in \n(x3u
.ta \n(x0u +\n(x1u
unvoiced stops:	closure (early cessation of voicing)
	silent steady state
	opening, comprising
		short fricative burst
		aspiration burst (context- and emphasis-dependent)
		onset of voicing
.sp
voiced stops:	closure (late cessation of voicing)
	steady state (possibility of voice bar)
	opening, comprising
		pre-voicing
		short fricative burst
.ta 0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i
.in 0
.FG "Table 7.3  Acoustic phases of stop consonants"
.pp
Table 7.3 summarizes some of the acoustic phases of voiced and unvoiced
stops.  There are many variations that have not been mentioned.
Nasal plosion ("good news") occurs (at the word boundary, in this case)
when the nasal formant pervades the
opening phase.  Stop bursts are suppressed when the next sound is a stop
too (the burst on the
.ul
p
of "apt", for example).
It is difficult to distinguish a voiced stop from an unvoiced one
at the end of a word ("cab" and "cap"); if the speaker is trying to
make himself particularly clear he will put a short neutral vowel
after the voiced stop to emphasize its early onset of voicing.
(If he is Italian he will probably do this anyway, for it is the norm
in his own language.)
.pp
Finally, we turn to affricates, of which there are only two
in English:
.ul
ch
("chin") and
.ul
j
("djinn").
They are very similar to the stops
.ul
t
and
.ul
d
followed by the fricatives
.ul
sh
and
.ul
zh
respectively, and their acoustic characterization is similar to that
of the phoneme pair.
.ul
ch
has a closing phase, a stopped phase, and a long fricative burst.
There is no aspiration,
for the vocal cords are not involved.
.ul
j
is the same except that voicing extends further into the stopped
portion, and the terminating fricative is also voiced.
It may be pronounced with a voice bar if the preceding segment is voiced
("adjunct").
.sh "7.5  Speech synthesis by rule"
.pp
Generation of speech by rules acting upon a phonetic transcription
was first investigated in the early 1960's (Kelly and Gerstman, 1961).
.[
Kelly Gerstman 1961
.]
Most systems employ a hardware resonance synthesizer, analogue or digital,
series or parallel,
to reduce the load on the computer which operates the rules.
The speech-by-rule program, rather than the
synthesizer, inevitably contributes by far the greater part of the
degradation in the resulting speech.
Although parallel synthesizers offer greater potential control over
the spectrum, it is not clear to what extent a synthesis program can take
advantage of this.  Parameter tracks for a series synthesizer can
easily be converted into linear predictive coefficients, and systems
which use a linear predictive synthesizer will probably become popular
in the near future.
.pp
The phrase "synthesis by rule", which is in common use, does not
make it clear just what sort of features the rules are supposed to
accomodate, and what information must be included explicitly in the
input transcription.
Early systems made no attempt to simulate prosodics.
Pitch and rhythm could be controlled, but only by inserting
pitch specifiers and duration markers in the input.
Some kind of prosodic control was often incorporated later,
but usually as a completely separate phase from segmental synthesis.
This does not allow interaction effects (such as the extra
aspiration for voiceless stops in accented syllables) to be taken
into account easily.
Even systems which perform prosodic operations invariably need to have
prosodic specifications embedded explicitly in the input.
.pp
Generating parameter tracks for a synthesizer from a phonetic transcription
is a process of data
.ul
expansion.
Six bits are ample to specify a phoneme, and a speaking rate of 12 phonemes/sec
leads to an input data rate of 72 bit/s.
The data rate required to control the synthesizer will depend upon the number
of parameters and the rate at which they are sampled,
but a typical figure is 6 Kbit/s (Chapter 5).
Hence there is something like a hundredfold data expansion.
.pp
Figure 7.1 shows the parameter tracks for a series synthesizer's rendering
of the utterance
.ul
s i k s.
.FC "Figure 7.1"
There are eight parameters.
You can see the onset of frication at the beginning and end (parameter 5),
and the amplitude of voicing (parameter 1) come on for the
.ul
i
and off again before the
.ul
k.
The pitch (parameter 0) is falling slowly throughout the utterance.
These tracks are stylized:  they come from a computer synthesis-by-rule
program and not from a human utterance.
With a parameter update rate of 10 msec, the graphs can be represented
by 90 sets of eight parameter values, a total of 720 values or 4320 bits
if a 6-bit representation is used for each value.
Contrast this with the input of only four phoneme segments, or say 24 bits.
.rh "A segment-by-segment system."
A seminal paper appearing in 1964 was the first comprehensive
description of a computer-based synthesis-by-rule system
(Holmes
.ul
et al,
1964).
.[
Holmes Mattingly Shearme 1964
.]
The same system is still in use and has been reimplemented in a more
portable form (Wright, 1976).
.[
Wright 1976
.]
The inventory of sound segments
includes the phonemes listed in Table 2.1, as well as diphthongs and
a second allophone of
.ul
l.
(Many British speakers use quite a different vocal posture for
pre- and post-vocalic
.ul
l\c
\&'s, called clear and dark
.ul
l\c
\&'s
respectively.)  Some phonemes are expanded into sub-phonemic
"phases" by the program.  Stops have three phases, corresponding to
the closure, silent steady state, and opening.
Diphthongs have two phases.  We will call individual phases and
single-phase phonemes "segments", for they are subject to exactly
the same transition rules.
.pp
Parameter tracks are constructed out of linear pieces.
Consider a pair of adjacent segments in an utterance to be synthesized.
Each one has a steady-state portion and an internal transition.
The internal transition of one phoneme is dubbed "external"
as far as the other is concerned.
This is important because instead of each segment being responsible
for its own internal transition, one of the pair is identified
as "dominant" and it controls the duration of both transitions \(em its
internal one and its external (the other's internal) one.
For example, in Figure 7.2 the segment
.ul
sh
dominates
.ul
ee
and so it
governs the duration of both transitions shown.
.FC "Figure 7.2"
Note that each
segment contributes as many as three linear pieces to the parameter track.
.pp
The notion of domination is similar to that discussed earlier for
word concatenation.
The difference is that for word concatenation the dominant segment was
determined by computing the spectral derivative over the transition
region, whereas for synthesis-by-rule
segments are ranked according to a static precedence,
and the higher-ranking segment dominates.
Segments of stop consonants have the highest rank (and also
the greatest spectral derivative), while fricatives, nasals, glides,
and vowels follow in that order.
.pp
The concatenation procedure is controlled by a table which associates
25 quantities with each segment.  They are
.LB
.NI
rank
.NI
2\ \ overall durations (for stressed and unstressed occurrences)
.NI
4\ \ transition durations (for internal and external transitions of
formant frequencies and amplitudes)
.NI
8\ \ target parameter values (amplitudes and frequencies of three
formant resonances, plus fricative information)
.NI
5\ \ quantities which specify how to calculate boundary values for
formant frequencies (two for each formant except the third,
which has only one)
.NI
5\ \ quantities which specify how to calculate boundary values for
amplitudes.
.LE
This table is rather large.  There are 80 segments in all (remember
that many phonemes are represented by more than one segment),
and so it has 2000 entries.  The system was an offline one which ran on
what was then \(em 1964 \(em a large computer.
.pp
The advantage of such a large table of "rules" is the
flexibility it affords.
Notice that transition durations are specified independently for
formant frequency and amplitude parameters \(em this permits
fine control which is particularly useful for stops.
For each parameter the boundary value between segments is calculated
using a fixed contribution from the dominant one
and a proportion of the steady state value of the other.
.pp
It is possible that the two transition durations which are
calculated for a segment actually exceed the overall duration specified
for it.  In this case, the steady-state target values will be approached
but not actually attained, simulating a situation where coarticulation
effects prevent a target value from being reached.
.rh "An event-based system."
The synthesis system described above, in common with many others, takes
an uncompromisingly segment-by-segment view of speech.
The next phoneme is read, perhaps split into a few segments, and
these are synthesized one by one with due attention being paid
to transitions between them.
Some later work has taken a more syllabic view.
Mattingly (1976) urges a return to syllables for both practical and
theoretical reasons.
.[
Mattingly 1976 Syllable synthesis
.]
Transitional effects are particularly strong
within a syllable and comparatively weak (but by no means negligible)
from one syllable to the next.  From a theoretical viewpoint,
there are much stronger phonetic restrictions on phoneme sequences
than there are on syllable sequences:  pretty well any syllable can
follow another (although whether the pair makes sense is
a different matter), but the linguistically
acceptable phoneme sequences are only a fraction
of those formed by combining phonemes in all
possible ways.
Hill (1978) argues against what be calls the "segmental assumption"
that progress through the utterance should be made one segment at a time,
and recommends a description of speech based upon perceptually relevant
"events".
.[
Hill 1978 A program structure for event-based speech synthesis by rules
.]
This framework is interesting because it provides an opportunity for prosodic
considerations to be treated as an integral part of the synthesis
process.
.pp
The phonetic segments and other information that specify an utterance
can be regarded as a list of events which describes it
at a relatively high level.
Synthesis-by-rule is the act of taking this list and elaborating on it
to produce lower-level events which are realized by the vocal tract,
or acoustically simulated by a resonance synthesizer, to give a speech
waveform.
In articulatory terms, an event might be "begin tongue motion towards
upper teeth with a given effort", while in resonance terms it could be
"begin second formant transition towards 1500\ Hz at a given rate".
(These two examples are
.ul
not
intended to describe the same event:  a tongue motion causes much more
than the transition of a single formant.)  Coarticulation
issues such as stop burst suppression and nasal plosion should
be easier to imitate within an event-based scheme than a segment-to-segment
one.
.pp
The ISP system (Witten and Abbess, 1979) is event-based.
.[
Witten Abbess 1979
.]
The key to its operation is the
.ul
synthesis list.
To prepare an utterance for synthesis, the lexical items which specify
it are joined into a linked list.  Figure 7.3 shows the start of
the list created for
.LB
1
.ul
dh i z  i z  /*d zh aa k s  /h aa u s
.LE
(this is Jack's house); the "1\ ...\ /*\ ...\ /\ ..." are
prosodic markers which will be discussed in the next chapter.
.FC "Figure 7.3"
Next, the rhythm and pitch assignment routines
augment the list with syllable boundaries, phoneme
cluster identifiers, and duration and pitch specifications.
Then it is passed to the segmental synthesis routine
which chains events into the appropriate places and, as it
proceeds, removes the no longer useful elements (phoneme names,
pitch specifiers, etc) which originally constituted the synthesis list.
Finally, an interrupt-driven speech synthesizer handler removes
events from the list as they become due and uses them to control
the hardware synthesizer.
.pp
By adopting the synthesis list as a uniform data structure for
holding utterances at every stage of processing, the problems of storage
allocation and garbage collection are minimized.
Each list element has a forward pointer and five data words, the first
indicating what type of element it is.
Lexical items which may appear in the input are
.LB
.NI
end of utterance (".", "!", ",", ";")
.NI
intonation indicator ("1", ...)
.NI
rhythm indicator ("/", "/*")
.NI
word boundary ("  ")
.NI
syllable boundary ("'")
.NI
phoneme segment
(\c
.ul
ar, b, ng, ...\c
)
.NI
explicit duration or pitch information.
.LE
Several of these have to do with prosodic features \(em a prime
advantage of the structure is that it does not create an artificial
division between segmentals and prosody.
Syllable boundaries and duration and pitch information are optional.
They will normally be computed by ISP, but the user can override them in the
input in a natural way.
The actual characters which identify lexical items are not fixed
but are taken from the rule table.
.pp
As synthesis
proceeds, new elements are chained in to the synthesis list.
For segmental purposes, three types of event are defined \(em
target events, increment events, and aspiration events.
With each event is associated a time at which the event becomes due.
For a target event, a parameter number, target parameter value,
and time-increment are specified.
When it becomes due, motion of the parameter towards the
target is begun.  If no other event for that parameter intervenes,
the target value will be reached after the given time-increment.
However, another target event for the parameter may change its motion
before the target has been attained.
Increment events contain a parameter number, a parameter increment,
and a time-increment.  The fixed increment is added to the parameter value
throughout the time specified.  This provides an easy way to make a
fricative burst during the opening phase of a stop consonant.
Aspiration events switch the mode of excitation from voicing to aspiration
for a given period of time.  Thus the aspirated part of unvoiced stops
can be accomodated in a natural manner, by changing the mode of excitation
for the duration of the aspiration.
.RF
.nr x1 (\w'excitation'/2)
.nr x2 (\w'formant resonance'/2)
.nr x3 (\w'fricative'/2)
.nr x4 (\w'type'/2)
.nr x5 (\w'frequencies (Hz)'/2)
.nr x6 (\w'resonance (Hz)'/2)
.nr x0 1.0i+0.7i+0.6i+0.6i+1.0i+1.2i+(\w'long vowel'/2)
.nr x7 (\n(.l-\n(x0)/2
.in \n(x7u
.ta 1.0i +0.7i +0.6i +0.6i +1.0i +1.2i
	\h'-\n(x1u'excitation		\0\0\h'-\n(x2u'formant resonance	\0\0\h'-\n(x3u'fricative	\h'-\n(x4u'type
			\0\0\h'-\n(x5u'frequencies (Hz)	\0\0\h'-\n(x6u'resonance (Hz)
\l'\n(x0u\(ul'
.sp
.nr x1 (\w'voicing'/2)
.nr x2 (\w'vowel'/2)
\fIuh\fR	\h'-\n(x1u'voicing	\0490	1480	2500		\c
\h'-\n(x2u'vowel
\fIa\fR	\h'-\n(x1u'voicing	\0720	1240	2540		\h'-\n(x2u'vowel
\fIe\fR	\h'-\n(x1u'voicing	\0560	1970	2640		\h'-\n(x2u'vowel
\fIi\fR	\h'-\n(x1u'voicing	\0360	2100	2700		\h'-\n(x2u'vowel
\fIo\fR	\h'-\n(x1u'voicing	\0600	\0890	2600		\h'-\n(x2u'vowel
\fIu\fR	\h'-\n(x1u'voicing	\0380	\0950	2440		\h'-\n(x2u'vowel
\fIaa\fR	\h'-\n(x1u'voicing	\0750	1750	2600		\h'-\n(x2u'vowel
.nr x2 (\w'long vowel'/2)
\fIee\fR	\h'-\n(x1u'voicing	\0290	2270	3090		\h'-\n(x2u'long vowel
\fIer\fR	\h'-\n(x1u'voicing	\0580	1380	2440		\h'-\n(x2u'long vowel
\fIar\fR	\h'-\n(x1u'voicing	\0680	1080	2540		\h'-\n(x2u'long vowel
\fIaw\fR	\h'-\n(x1u'voicing	\0450	\0740	2640		\h'-\n(x2u'long vowel
\fIuu\fR	\h'-\n(x1u'voicing	\0310	\0940	2320		\h'-\n(x2u'long vowel
.nr x1 (\w'aspiration'/2)
.nr x2 (\w'h'/2)
\fIh\fR	\h'-\n(x1u'aspiration					\h'-\n(x2u'h
.nr x1 (\w'voicing'/2)
.nr x2 (\w'glide'/2)
\fIr\fR	\h'-\n(x1u'voicing	\0240	1190	1550			 \h'-\n(x2u'glide
\fIw\fR	\h'-\n(x1u'voicing	\0240	\0650			\h'-\n(x2u'glide
\fIl\fR	\h'-\n(x1u'voicing	\0380	1190			\h'-\n(x2u'glide
\fIy\fR	\h'-\n(x1u'voicing	\0240	2270			\h'-\n(x2u'glide
.nr x2 (\w'nasal'/2)
\fIm\fR	\h'-\n(x1u'voicing	\0190	\0690	2000		\h'-\n(x2u'nasal
.nr x1 (\w'none'/2)
.nr x2 (\w'stop'/2)
\fIb\fR	\h'-\n(x1u'none	\0100	\0690	2000		\h'-\n(x2u'stop
\fIp\fR	\h'-\n(x1u'none	\0100	\0690	2000		\h'-\n(x2u'stop
.nr x1 (\w'voicing'/2)
.nr x2 (\w'nasal'/2)
\fIn\fR	\h'-\n(x1u'voicing	\0190	1780	3300		\h'-\n(x2u'nasal
.nr x1 (\w'none'/2)
.nr x2 (\w'stop'/2)
\fId\fR	\h'-\n(x1u'none	\0100	1780	3300		\h'-\n(x2u'stop
\fIt\fR	\h'-\n(x1u'none	\0100	1780	3300		\h'-\n(x2u'stop
.nr x1 (\w'voicing'/2)
.nr x2 (\w'nasal'/2)
\fIng\fR	\h'-\n(x1u'voicing	\0190	2300	2500		\h'-\n(x2u'nasal
.nr x1 (\w'none'/2)
.nr x2 (\w'stop'/2)
\fIg\fR	\h'-\n(x1u'none	\0100	2300	2500		\h'-\n(x2u'stop
\fIk\fR	\h'-\n(x1u'none	\0100	2300	2500		\h'-\n(x2u'stop
.nr x1 (\w'frication'/2)
.nr x2 (\w'voice + fric'/2)
.nr x3 (\w'fricative'/2)
\fIs\fR	\h'-\n(x1u'frication				6000	\h'-\n(x3u'fricative
\fIz\fR	\h'-\n(x2u'voice + fric	\0190	1780	3300	6000	\h'-\n(x3u'fricative
\fIsh\fR	\h'-\n(x1u'frication				2300	\h'-\n(x3u'fricative
\fIzh\fR	\h'-\n(x2u'voice + fric	\0190	2120	2700	2300	\h'-\n(x3u'fricative
\fIf\fR	\h'-\n(x1u'frication				4000	\h'-\n(x3u'fricative
\fIv\fR	\h'-\n(x2u'voice + fric	\0190	\0690	3300	4000	\h'-\n(x3u'fricative
\fIth\fR	\h'-\n(x1u'frication				5000	\h'-\n(x3u'fricative
\fIdh\fR	\h'-\n(x2u'voice + fric	\0190	1780	3300	5000	\h'-\n(x3u'fricative
\l'\n(x0u\(ul'
.ta 0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i
.in 0
.FG "Table 7.4  Rule table for an event-based synthesis-by-rule program"
.pp
Now the rule table, which is shown in Table 7.4,
holds simple target positions for each phoneme segment, as well as
the segment type.  The latter is used to trigger events by computer
procedures which have access to the context of the segment.
In principle, this allows considerably more sophistication to be
introduced than does a simple segment-by-segment approach.
.RF
.nr x1 0.5i+0.5i+\w'preceding consonant in this syllable (suppress burst if fricative)'u
.nr x1 (\n(.l-\n(x1)/2
.in \n(x1u
.ta 0.5i +0.5i
fricative bursts on stops
aspiration bursts on unvoiced stops, affected by
	preceding consonant in this syllable (suppress burst if fricative)
	following consonant (suppress burst if another stop; introduce
		nasal plosion if a nasal)
	prosodics (increase burst if syllable is stressed)
voice bar on voiced stops (in intervocalic position)
post-voicing on terminating voiced stops, if syllable is stressed
anticipatory coarticulation for \fIh\fR
vowel colouring when a nasal or glide follows
.ta 0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i
.in 0
.FG "Table 7.5  Some coarticulation effects"
.pp
For example, Table 7.5 summarizes some of the subtleties of the
speech production process which have been mentioned earlier in this
chapter.  Most of them are context-dependent, with the prosodic
context (whether two segments are in the same syllable; whether a
syllable is stressed) playing a significant role.  A scheme where
data-dependent "demons" fire on particular patterns in a linked list
seems to be a sensible approach towards incorporating such rules.
.rh "Discussion."
There are two opposing trends in speech synthesis by rule.
On the one hand larger and larger segment inventories can be used,
containing more and more allophones explicitly.
This is the approach of the Votrax sound-segment synthesizer,
discussed in Chapter 11.
It puts an increasing burden on the person who codes the utterances
for synthesis, although, as we shall see, computer programs can assist with
this task.
On the other hand the segment inventory can be kept small, perhaps
comprising just the logical phonemes as in the ISP system.
This places the onus on the computer program to accomodate allophonic variations,
and to do so it must take account of the segmental and prosodic
context of each phoneme.
An event-based approach seems to give the best chance of incorporating
contextual modification whilst avoiding undesired interactions.
.pp
The second trend brings synthesis closer to the articulatory process
of speech production.  In fact an event-based system would be
an ideal way of implementing an articulatory model for speech synthesis
by rule.  It would be much more satisfying to have the rule table
contain articulatory target positions instead of resonance ones,
with events like "begin tongue motion towards upper teeth with a given
effort".  The problem is that hard data on articulatory postures and
constraints is much more difficult to gather than resonance information.
.pp
An interesting question that relates to articulation is whether formant
motion can be simulated adequately by a small number of linear pieces.
The segment-by-segment system described above had as many as nine
pieces for a single phoneme, for some phonemes had three phases
and each one contributes up to three pieces (transition in,
steady state, and transition out).
Another system used curves of decaying exponential
form which ensured that all transitions started rapidly towards
the target position but slowed down as it was approached (Rabiner, 1968, 1969).
.[
Rabiner 1968 Speech synthesis by rule Bell System Technical J
.]
.[
Rabiner 1969 A model for synthesizing speech by rule
.]
The time-constant of decay was stored with each segment in the rule
table.  The rhythm of the synthetic speech was controlled at this level,
for the next segment was begun when all the formants had attained
values sufficiently close to the current targets.
This is a poor model of the human speech production process, where rhythm
is dictated at a relatively high level and the next phoneme is not
simply started when the current one happens to end.
Nevertheless, the algorithm produced smooth, continuous formant motions
not unlike those found in spectrograms.
.pp
There is, however, by no means universal agreement on decaying exponential formant
motions.  Lawrence (1974) divided segments into "checked" and "free"
categories, corresponding roughly to consonants and vowels; and postulated
.ul
increasing
exponential transitions into checked segments, and decaying transitions into
free ones.
.[
Lawrence 1974
.]
This is a reasonable supposition if you consider the mechanics of
articulation.  The speed of movement of the tongue (for example) is likely
to increase until it is physically stopped by reaching the roof of the
mouth.
When moving away from a checked posture into a free one the transition will
be rapid at first but slow down to approach the target asymptotically,
governed by proprioceptive feedback.
.pp
The only thing that seems to be agreed is that the formant tracks should
certainly
.ul
not
be piecewise linear.  However, in the face of
conflicting opinions as to whether exponentials should be decaying
or increasing, piecewise linear motions seem to be a reasonable
compromise!  It is likely that the precise shape of formant
tracks is unimportant so long as the gross features are imitated
correctly.
Nevertheless, this is a question which an articulatory model
could help to answer.
.sh "7.6  References"
.LB "nnnn"
.[
$LIST$
.]
.LE "nnnn"
.sh "7.7  Further reading"
.pp
There are unfortunately few books to recommend on the subject of
joining segments of speech.
The references form a representative and moderately comprehensive bibliography.
Here is some relevant background reading in linguistics.
.LB "nn"
.\"Fry-1976-1
.]-
.ds [A Fry, D.B.(Editor)
.ds [D 1976
.ds [T Acoustic phonetics
.ds [I Cambridge Univ Press
.ds [C Cambridge, England
.nr [T 0
.nr [A 0
.nr [O 0
.][ 2 book
.in+2n
This book of readings contains many classic papers on acoustic phonetics
published from 1922\-1965.
It covers much of the history of the subject, and is intended
primarily for students of linguistics.
.in-2n
.\"Lehiste-1967-2
.]-
.ds [A Lehiste, I.(Editor)
.ds [D 1967
.ds [T Readings in acoustic phonetics
.ds [I MIT Press
.ds [C Cambridge, Massachusetts
.nr [T 0
.nr [A 0
.nr [O 0
.][ 2 book
.in+2n
Another basic collection of references which covers much the same ground
as Fry (1976), above.
.in-2n
.\"Sivertsen-1961-3
.]-
.ds [A Sivertsen, E.
.ds [D 1961
.ds [K *
.ds [T Segment inventories for speech synthesis
.ds [J Language and Speech
.ds [V 4
.ds [P 27-89
.nr [P 1
.nr [T 0
.nr [A 1
.nr [O 0
.][ 1 journal-article
.in+2n
This is a careful early study of the quantitative implications of using
phonemes, demisyllables, syllables, and words as the basic building
blocks for speech synthesis.
.in-2n
.LE "nn"
.EQ
delim $$
.EN
.CH "8  PROSODIC FEATURES IN SPEECH SYNTHESIS"
.ds RT "Prosodic features
.ds CX "Principles of computer speech
.pp
Prosodic features are those which characterize an utterance as a whole,
rather than having a local influence on individual sound segments.
For speech output from computers, an "utterance" usually comprises a
single unit of information which stretches over several words \(em a clause
or sentence.  In natural speech an utterance can be very much longer, but
it will be broken into prosodic units which are again roughly the size of a
clause or sentence.  These prosodic units are certainly closely related
to each other.  For example, the pitch contour used when introducing a new
topic is usually different from those employed to develop it subsequently.
However, for the purposes of synthesis the successive prosodic units can
be treated independently, and information about pitch contours to be used
will have to be specified in the input for each one.
The independence between them is not complete, however, and
lower-level contextual effects, such as interpolation of pitch between
the end of one prosodic unit and the start of the next, must still be
imitated.
.pp
Prosodic features were introduced briefly in Chapter 2.
Variations in voice dynamics occur in three dimensions:  pitch of the voice,
time, and amplitude.
These dimensions are inextricably twined together in living speech.
Variations in voice quality are much less important for the factual
kind of speech usually sought in voice response applications,
although they can play a considerable in conveying emotions
(for a discussion of the acoustic manifestations of emotion in speech,
see Williams and Stevens, 1972).
.[
Williams Stevens 1972
.]
.pp
The distinction between prosodic and segmental effects is a traditional one,
but it becomes rather fuzzy when examined in detail.
It is analogous to the distinction between hardware and
software in computer science:  although useful from some points of view
the borderline becomes blurred as one gets closer to actual systems \(em with
microcode, interrupts, memory management, and the like.
At a trivial level, prosodics
cannot exist without segmentals, for there must be some vehicle to carry the
prosodic contrasts.
Timing \(em a prosodic feature \(em is actually realized by the durations of
individual segments.  Pauses are tantamount to silent segments.
.pp
While pitch may seem to be relatively independent of segmentals \(em and
this view is reinforced by the success of the source-filter model
which separates the frequency of the
excitation source from the filter characteristics \(em there
are some subtle phonetic effects of pitch.
It has been observed that it drops on the transition into certain
consonants, and rises again on the transition out (Haggard
.ul
et al,
1970).
.[
Haggard Ambler Callow 1970
.]
This can be explained in terms of variations in pressure from the
lungs on the vocal cords (Ladefoged, 1967).
.[
Ladefoged 1967
.]
Briefly, the increase in mouth pressure which occurs during some consonants
causes a reduction in the pressure difference across the vocal cords
and in the rate of flow of air between them.
This results in a decrease in their frequency of vibration.
When the constriction is released, there is a temporary increase in the air
flow which increases the pitch again.
The phenomenon is called "microintonation".
It is particularly noticeable in voiced stops, but also occurs in voiced
fricatives and unvoiced stops.
Simulation of the effect in synthesis-by-rule has often been found to give
noticeable improvements in the speech quality.
.pp
Loudness also has a segmental role.  For example, we noted in the last chapter
that amplitude values play a small part in identification of fricatives.
In fact loudness is a very
.ul
weak
prosodic feature.  It contributes little to the perception of stress.
Even for shouting the distinction from normal speech is as much in the voice
quality as in amplitude
.ul
per se.
It is not necessary to consider varying loudness on a prosodic basis
in most speech synthesis systems.
.pp
The above examples show how prosodic features have segmental influences
as well.
The converse is also true:  some segmental features have a prosodic effect.
The last chapter described how stress is associated with increased aspiration
of syllable-initial unvoiced stops.  Furthermore, stressed syllables
are articulated with greater effort than unstressed ones, and hence the formant
transitions are more likely to attain their target values
under circumstances which would otherwise cause them to fall short.
In unstressed syllables, extreme vowels (like
.ul
ee, aa, uu\c
)
tend to more centralized sounds
(like
.ul
i, uh, u
respectively).
Although all British English vowels
.ul
can
appear in unstressed syllables, they often become "reduced" into a
centralized form.
Consider the following examples.
.LB
.NI
diplomat	\ 
.ul
d i p l uh m aa t
.NI
diplomacy	\ 
.ul
d i p l uh u m uh s i
.NI
diplomatic	\ 
.ul
d i p l uh m aa t i k.
.LE
The vowel of the second syllable is reduced to
.ul
uh
in "diplomat" and "diplomatic", whereas the root form "diploma", and also
"diplomacy", has a diphthong
(\c
.ul
uh u\c
)
there.  The third syllable has an
.ul
aa
in "diplomat" and "diplomatic" which is reduced to
.ul
uh
in "diplomacy".
In these cases the reduction is shown explicitly in the phonetic transcription;
but in more marginal examples where it is less extreme it will not be.
.pp
I have tried to emphasize in previous chapters that prosodic features are
important in speech synthesis.
There is something very basic about them.
Rhythm is an essential part of all bodily activity \(em of breathing,
walking, working and playing \(em and so it pervades speech too.
Mothers and babies communicate effectively using intonation alone.
Some experiments have indicated that the language environment of
an infant affects his babbling at an early age, before he has effective
segmental control.
There is no doubt that "tone of voice" plays a large part in human
communication.
.pp
However, early attempts at synthesis did not pay too
much attention to prosodics, perhaps because it was thought sufficient to get the
meaning across by providing clear segmentals.
As artificial speech grows more widespread, however, it is becoming
apparent that its acceptability to users, and hence its ultimate
success, depends to a large extent on incorporating natural-sounding
prosodics.  Flat, arhythmic speech may be comprehensible in short stretches,
but it strains the concentration in significant discourse and people
are not usually prepared to listen to it.
Unfortunately, current commercial speech output systems do not really tackle
prosodic questions, which indicates our present rather inadequate
state of knowledge.
.pp
The importance of prosodics for automatic speech
.ul
recognition
is beginning to be appreciated too.  Some research projects
have attended to the automatic identification of points of stress,
in the hope that the clear articulation of stressed syllables can be used
to provide anchor points in an unknown utterance (for example, see Lea
.ul
et al,
1975).
.[
Lea Medress Skinner 1975
.]
.pp
But prosodics and segmentals are closely intertwined.
I have chosen to
treat them in separate chapters in order to split the material up into
manageable chunks rather than to enforce a deep division between them.
It is also true that synthesis of prosodic features is an uncharted and
controversial area, which gives this chapter rather a different
flavour from the last.
It is hard to be as definite about alternative strategies
and methods as you can for segment concatenation.
In order to make the treatment as concrete and down-to-earth as possible,
I will describe in some detail two example projects in prosodic synthesis.
The first treats the problem of transferring pitch from one utterance to
another, while the second considers how artificial timing and pitch can be
assigned to synthetic speech.
These examples illustrate quite different problems, and are reasonably
representative of current research activity.
(Other systems are described by Mattingly, 1966; Rabiner
.ul
et al,
1969.)  Before
.[
Mattingly 1966
.]
.[
Rabiner Levitt Rosenberg 1969
.]
looking at the two examples, we will discuss
a feature which is certainly prosodic but does not appear in the
list given earlier \(em stress.
.sh "8.1  Stress"
.pp
Stress is an everyday notion, and when
listening to natural speech people can usually agree on which syllables
are stressed.  But it is difficult to characterize in acoustic terms.
From the speaker's point of view, a stressed syllable is produced by
pushing more air out of the lungs.  For a listener, the points of stress
are "obvious".
You may think that stressed syllables are louder than the others:  however,
instrumental studies show that this is not necessarily (nor even usually)
so (eg Lehiste and Peterson, 1959).
.[
Lehiste Peterson 1959
.]
Stressed syllables frequently have a longer vowel than unstressed
ones, but this is by no means universally true \(em if you say "little"
or "bigger" you will find that the vowel in the first, stressed, syllable
is short and shows little sign of lengthening as you increase the emphasis.
Moreover, experiments using bisyllabic nonsense words have indicated
that some people consistently judge the
.ul
shorter
syllable to be stressed in the absence of other clues (Morton and Jassem,
1965).
.[
Morton Jassem 1965
.]
Pitch often helps to indicate stress.
It is not that stressed syllables are always higher- or lower-pitched
than neighbouring ones, or even that they are uttered with a rising or
falling pitch.  It is the
.ul
rate of change
of pitch that tends to be greater
for stressed syllables:  a sharp rise or fall,
or a reversal of direction, helps to give emphasis.
.pp
Stress is acoustically manifested in timing and pitch,
and to a much lesser extent in loudness.
However it is a rather subtle feature and does
.ul
not
correspond simply to duration increases or pitch rises.
It seems that listeners unconsciously put together all the clues
that are present in an utterance in order to deduce which syllables are
stressed.
It may be that speech is perceived by a listener with reference to how
he would have produced it himself, and that this is how he detects which syllables
were given greater vocal effort.
.pp
The situation is confused by the fact that certain syllables in words are
often said in ordinary language to be "stressed" on account of their
position in the word.  For example, the words
"diplomat", "diplomacy", and "diplomatic" have stress on the first,
second, and third syllables respectively.
But here we are talking about the word itself rather than
any particular utterance of it.  The "stress" is really
.ul
latent
in the indicated syllables and only made manifest upon uttering them,
and then to a greater or lesser degree depending on exactly how
they are uttered.
.pp
Some linguists draw a careful distinction between salient syllables,
accented syllables, and stressed syllables,
although the words are sometimes used differently by different authorities.
I will not adopt a precise terminology here,
but it is as well to be aware of the subtle distinctions involved.
The term "salience" is applied to actual utterances, and salient
syllables are those that are perceived as being more prominent than their
neighbours.
"Accent" is the potential for salience, as marked, for example,
in a dictionary or lexicon.
Thus the discussion of the "diplo-" words above is about accent.
Stress is an articulatory phenomenon associated with increased
muscular activity.
Usually, syllables which are perceived as salient were produced with stress,
but in shouting, for example, all syllables can be stressed \(em even
non-salient ones.
Furthermore, accented syllables may not be salient.
For instance, the first syllable of the word "very" is accented,
that is, potentially salient, but in a sentence as uttered it may or may not be
salient.  One can say
.LB
"\c
.ul
he's
very good"
.LE
with salience on "he" and possibly "good", or
.LB
"he's
.ul
very
good"
.LE
with salience on the first syllable of "very", and possibly "good".
.pp
Non-standard stress patterns are frequently used to bring out contrasts.
Words like "a" and "the" are normally unstressed, but can be stressed
in contexts where ambiguity has arisen.
Thus factors which operate at a much higher level than the phonetic structure
of the utterance must be taken into account when deciding where stress
should be assigned.  These include syntactic and semantic considerations,
as well as the attitude of the speaker and the likely attitude of
the listener to the material being spoken.
For example, I might say
.LB
"Anna
.ul
and
Nikki should go",
.LE
with emphasis on the "and" purely because I was aware that my listener
might quibble about the expense of sending them both.
Clearly some notation is needed to communicate to the synthesis process
how the utterance is supposed to be rendered.
.sh "8.2  Transferring pitch from one utterance to another"
.pp
For speech stored in source-filter form and concatenated on a
slot-filling basis, it would be useful to
have stored typical pitch contours which can be applied to the
synthetic utterances.
From a practical point of view it is important to be able to generate
natural-sounding pitch for high-quality artificial speech.
Although several algorithms for creating completely synthetic contours
have been proposed \(em and we will examine one later in this chapter \(em
they are unsuitable for high-quality speech.
They are generally designed for use with synthesis-by-rule from phonetics,
and the rather poor quality of articulation does not encourage the
development of excellent pitch assignment procedures.  With speech
synthesized by rule there is generally an emphasis on keeping the
data storage requirements to a minimum, and so it is not appropriate
to store complete contours.
Moreover, if speech is entered in textual
form as phoneme strings, it is natural to attach pitch information as markers
in the text rather than by entering a complete and detailed contour.
.pp
The picture is rather different for concatenated segments of natural speech.
In the airline reservation system, with utterances formed from templates like
.LB
Flight number \(em leaves \(em at \(em , arrives in \(em at \(em ,
.LE
it is attractive to store the pitch contour of one complete instance of the
utterance and apply it to all synthetic versions.
.pp
There is an enormous literature on the anatomy of intonation, and much of it
rests upon the notion of a pitch contour as a descriptive aid to analysis.
Underlying this is the assumption, usually unstated, that a contour can be
discussed independently of the particular stream of words that manifests it;
that a single contour can somehow be bound to any sentence (or phrase, or
clause) to produce an acceptable utterance.  But the contour, and its binding,
are generally described only at the grossest level, the details being left
unspecified.
.pp
There are phonetic influences on pitch \(em the characteristic lowering
during certain consonants was mentioned above \(em and these are
not normally considered as part of intonation.
Such effects will certainly spoil attempts to store contours extracted
from living speech and apply them to different utterances, but the impairment
may not be too great, for pitch is only one of many segmental clues to
consonant identification.
.pp
In the system mentioned earlier which generated 7-digit telephone numbers
by concatenating formant-coded words, a single natural pitch contour
was applied to all utterances.
It was taken to match as well as possible the general shape of the
contours measured in naturally-spoken telephone numbers.  However, this is a very
restricted environment, for telephone numbers exhibit almost no variety in
the configuration of stressed and unstressed syllables \(em
the only digit which is not a monosyllable is "seven".
Significant problems arise when more general utterances are considered.
.pp
Suppose the pitch contour of one utterance (the "source")
is to be transferred to another (the "target").
Assume that the utterances are encoded in source-filter form,
either as parameter tracks for a formant synthesizer or as linear predictive
coefficients.
Then there are no technical obstacles to combining pitch and segmentals.
The source must be available as a complete utterance, while the target
may be formed by concatenating smaller units such as words.
.pp
For definiteness, we will consider utterances of the form
.LB
The price is \(em dollars and \(em cents,
.LE
where the slots are filled by numbers less than 100;
and of the form
.LB
The price is \(em cents.
.LE
The domain of prices encompasses a wide range of syllable
configurations.
There are between one and five syllables in each variable part,
if the numbers are restricted to be less than 100.
The sentences have a constant pragmatic, semantic, and syntactic structure.
As in the vast majority of real-life situations,
minimal phonetic distinctions between utterances do not occur.
.pp
Pitch transfer is complicated by the fact that values of the source pitch
are only known during the voiced parts of the utterance.
Although it would certainly be possible to extrapolate pitch
over unvoiced parts, this would introduce some artificiality into
the otherwise completely natural contours.
Let us assume, therefore, that the pitch contour
of the voiced nucleus of each syllable in the source is applied to the
corresponding syllable nucleus in the target.
.pp
The primary factors which might tend to inhibit successful transfer
are
.LB
.NP
different numbers of syllables in the utterances;
.NP
variations in the pattern of stressed and unstressed syllables;
.NP
different syllable durations;
.NP
pitch discontinuities;
.NP
phonetic differences between the utterances.
.LE
.rh "Syllabification."
It is essential to take into account the syllable structures
of the utterances, so that pitch is transferred between
corresponding syllables rather than over the utterance
as a whole.
Fortunately, syllable boundaries can be detected automatically
with a fair degree of accuracy, especially if the speech is carefully
enunciated.
It is worth considering briefly how this can be done, even though it takes
us off the main topic of synthesis and into speech analysis.
.pp
A procedure developed by Mermelstein (1975)
involves integrating the spectral energy
at each point in the utterance.
.[
Mermelstein 1975 Automatic segmentation of speech into syllabic units
.]
First the low (<500\ Hz) and high (>4000\ Hz) ends are filtered out
with 12\ dB/octave cutoffs.
The resulting energy signal is smoothed
by a 40\ Hz lowpass filter, giving a so-called "loudness"
function.
All this can be accomplished with simple recursive digital filters.
.pp
Then, the loudness function is compared with its convex hull.
The convex hull is the shape a piece of elastic would assume if
stretched over the top of the loudness function and anchored down at
both ends, as illustrated in Figure 8.1.
.FC "Figure 8.1"
The point of maximum difference between the hull and loudness function
is taken to be a tentative syllable
boundary.
The hull is recomputed, but anchored to the actual loudness function
at the tentative boundary,
and the points of maximum hull-loudness difference in each of the
two halves  are selected as further tentative
boundaries.
The procedure continues recursively until the maximum hull-loudness
difference, with the hull anchored at each tentative boundary,
falls below a certain minimum (say 4\ dB).
.pp
At this stage, the number of tentative boundaries will greatly exceed
the actual number of syllables (by a factor of around 5).
Many of the extraneous boundaries are eliminated by the following
constraints:
.LB
.NP
if two boundaries lie within a certain time of each other
(say 120\ msec), one of them is discarded;
.NP
if the maximum loudness within a tentative syllable falls too
far short of the overall maximum for the utterance
(more than 20\ dB), one boundary is discarded.
.LE
The question of which boundary to discard can be decided by
examining the voicing continuity of the utterance.
If possible, voicing across a syllable boundary should be avoided.
Otherwise, the boundary with the smallest hull-loudness
difference should be rejected.
.RF
.nr x0 \w'boundaries moved slightly to correspond better with voicing:'
.nr x1 (\n(.l-\n(x0)/2
.in \n(x1u
.ta 3.4i +0.5i
\l'\n(x0u\(ul'
.sp
total syllable count:	332
boundaries missed by algorithm:	\0\09	(3%)
extra boundaries inserted by algorithm:	\029	(9%)
boundaries moved slightly to correspond better with voicing:
	\0\03	(1%)
.sp
total errors:	\041	(12%)
\l'\n(x0u\(ul'
.ta 0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i
.in 0
.FG "Table 8.1  Success of the syllable segmentation procedure"
.pp
Table 8.1 illustrates the success of this syllabification
procedure, in a particular example.
Segmentation is performed with less than 10% of extraneous
boundaries being inserted,
and much less than 10% of actual boundaries being missed.
These figures are rather sensitive to the values of the
three thresholds.
The values were chosen to err on the side
of over-zealous syllabification, because all the boundaries need to be checked
by ear and eye and it is easier to delete
a boundary by hand than to insert one at an appropriate place.
It may well be that with careful optimization of thresholds,
better figures could be
achieved.
.rh "Stressed and unstressed syllables."
If the source and target utterances have the same number of
syllables, and the same pattern of stressed and unstressed syllables,
pitch can simply be transferred from a syllable in the source
to the corresponding one in the target.
But if the pattern differs \(em even though the
number of syllables may be the same, as in "eleven" and "seventeen" \(em
then a one-to-one mapping will conflict with the stress points,
and certainly sound unnatural.
Hence an attempt should be made to ensure that the pitch is mapped in a
plausible way.
.pp
The syllables of each utterance can be classified as "stressed"
and "unstressed".
This distinction could be made automatically by
inspection of the pitch contour, within the domain of utterances used,
and possibly even in general (Lea
.ul
et al,
1975).
.[
Lea Medress Skinner 1975
.]
However, in many cases it is expedient to perform the job by hand.
In our example, the sentences have fixed "carrier" parts and
variable "number" parts.
The stressed carrier syllables, namely
.LB
"... price ... dol\- ... cents",
.LE
can be marked as such, by hand,
to facilitate proper alignment between the source and target.
This marking would be difficult to do automatically
because it would be hard to distinguish the carrier from the numbers.
.pp
Even after classifying the syllables as "carrier stressed",
"stressed", and "unstressed", alignment still presents problems,
because the configuration of syllables in the variable parts
of the utterances may differ.
Syllables in the source which have no
correspondence in the target can be ignored.
The pitch track of
the source syllable can be replicated for each
additional syllable in corresponding
position in the target.
Of course, a stressed syllable should be selected for copying
if the unmatched target syllable is stressed,
and similarly for unstressed ones.
It is rather dangerous to copy exactly a part of a pitch
contour, for the ear is very sensitive to the juxtaposition of
identically intoned segments of speech \(em especially when the segment is stressed.
To avoid this, whenever a stressed syllable is replicated the
pitch values should be decreased by, say, 20%, on the second copy.
It sometimes happens that a single stressed syllable in the source
needs to cover a stressed-unstressed pair in the target:  in
this case the first part of the source pitch track can be used
for the stressed syllable, and the remainder for the
unstressed one.
.pp
The example of Figure 8.2 will help to make these rules clear.
.FC "Figure 8.2"
Note that the marking alone is done by hand.
The detailed mapping decisions can be left to the computer.
The rules were derived intuitively, and do not have any sound theoretical
basis.
They are intended to give reasonable results in the majority of cases.
.pp
Figure 8.3 shows the result of transferring the pitch from "the price is ten
cents" to "the price is seventy-seven cents".
.FC "Figure 8.3"
The syllable boundaries which are marked were determined automatically.
The use of the last 30% of the
"ten" contour to cover the first "-en" syllable, and its replication
to serve the "-ty" syllable, can be seen.
However, the 70%\(em30% proportion is applied to the source contour,
and the linear distortion (described next) upsets the proportion in the
target utterance.
The contour of the second "seven" can be seen to be a
replication of that of the first one, lowered by 20%.
Notice that the pitch extraction procedure has introduced an artifact into the final
part of one of the "cents" contours by doubling the pitch.
.rh "Stretching and squashing."
The pitch contour over a source syllable nucleus must be stretched
or squashed to match the duration
of the target nucleus.
It is difficult to see how anything other than linear stretching
and squashing could be done without considerably increasing the
complexity of the procedure.
The gross non-linearities will have been accounted for
by the syllable alignment process, and so simple linear time-distortion
should not cause too much degradation.
.rh "Pitch discontinuities."
Sudden jumps in pitch during voiced speech sound peculiar,
although they can in fact be produced naturally (by yodelling).
People frequently burst into laughter on hearing them in synthetic speech.
It is particularly important to avoid this diverting effect in
voice response applications,
for the listener's attention is instantly directed
away from what is said to the voice that speaks.
.pp
Discontinuities can arise in the pitch-transfer procedure either by a
voiced-unvoiced-voiced transition between syllables mapping on to
a voiced-voiced transition in the target,
or by voicing continuity being broken when the syllable
alignment procedure drops or replicates a syllable.
There are several ways in which at least some of the possibilities can
be avoided.
For example, one could hold unstressed syllables at a constant pitch
whose value coincides with either the end of the previous
syllable's contour or the beginning of the next syllable's contour,
depending on which transition is voiced.
Alternatively, the policy of reserving the trailing part
of a stressed syllable in the source to cover an unmatched following
unstressed syllable in the target could be generalized to allow use of the leading 30%
of the next stressed syllable's contour instead,
if that maintained voicing continuity.
A third solution is simply to merge the pitch contours
at a discontinuity by mixing the average pitch value at the break
with the pitch contour on either side of it in a proportion which
increases linearly from the edges of the domain of influence to the discontinuity.
Figure 8.4 shows the effect of this merging,
when the pitch contour of "the price is seven cents"
is transferred to "the price is eleven cents".
.FC "Figure 8.4"
Of course, the
interpolated part will not necessarily be linear.
.rh "Results of an experiment on pitch transfer."
Some experiments have been conducted to evaluate the performance
of this pitch transfer method on the kind of utterances discussed above
(Witten, 1979).
.[
Witten 1979 On transferring pitch from one utterance to another
.]
First, the source and target sentences
were chosen to be lexically identical, that is, the same words were spoken.
For this experiment alone,
expert judges were employed.
Each sentence was recorded twice (by the same person),
and pitch was transferred from copy A
to copy B and vice versa.  Also, the originals were resynthesized from their linear
predictive coefficients with their own pitch contours.
Although all four often sounded extremely similar, sometimes the pitch
contours of originals A and B were quite different,
and in these cases it was immediately obvious to the ear that two of
the four utterances shared the same intonation,
which was different to that shared by the other two.
.pp
Experienced researchers in speech analysis-synthesis served as
judges.
In order to make the test as stringent as possible it was explained
to them exactly what had been done,
except that the order of the utterances in each quadruple was kept secret.
They were asked to identify which two of the four sentences did not have their
original contours,
and were allowed to listen to each quadruple as often as they liked.
On occasion they were prepared to identify only one, or even none,
of the sentences as artificial.
.pp
The result was that an utterance with pitch transferred
from another, lexically identical, one is indistinguishable from
a resynthesized version of the original, even to a skilled ear.
(To be more precise, this hypothesis
could not be rejected even at the 1% level of statistical significance.)  This
gave confidence in the transfer procedure.
However, one particular judge was quite successful at identifying the bogus contours,
and he attributed his success to the fact that
on occasion the segmental durations did not accord with the
pitch contour.
This casts a shadow of suspicion on the linear stretching and
squashing mechanism.
.pp
The second experiment examined pitch transfers between utterances having only one variable part
each ("the price is ... cents") to test the transfer
method under relatively controlled conditions.
Ten sentences of the form
.LB
"The price is \(em cents"
.LE
were selected to cover
a wide range of syllable structures.
Each one was regenerated with pitch transferred from each of
the other nine,
and these nine versions were paired with the original resynthesized
with its natural pitch.
The $10 times 9=90$ resulting pairs were recorded on tape in random order.
.pp
Five males and five females, with widely differing occupations
(secretaries, teachers, academics, and students), served as judges.
Written instructions explained that the tape contained pairs of
sentences which were lexically identical but had a slight difference
in "tone of voice", and that the subjects were to judge which of
each pair sounded "most natural and intelligible".  The
response form gave the price associated with each pair \(em
a preliminary experiment had shown that there was never
any difficulty in identifying this \(em and a column for decision.
With each decision, the subjects recorded their confidence in the decision.
Subjects could rest at any time during the test, which lasted for about
30 minutes, but they were not permitted to hear any pair a second time.
.pp
Defining a "success" to be a choice of the utterance with
natural pitch as the best of a pair,
the overall success rate was about 60%.
If choices were random, one would of course expect only a 50% success rate,
and the figure obtained was significantly different from this.
Almost half the choices were correct and made with high confidence;
high-confidence but incorrect choices accounted for a quarter of the
judgements.
.pp
To investigate structural effects in the pitch transfer process,
low confidence decisions were ignored to eliminate noise, and the others
lumped together and tabulated by source and target utterance.
The number of stressed and unstressed syllables does not appear to play
an important part in determining whether a particular utterance is an
easy target.
For example, it proved to be particularly difficult to tell
.EQ
delim @@
.EN
natural from transferred contours with utterances $0.37 and $0.77.
.EQ
delim $$
.EN
In fact, the results showed no better than random discrimination for them,
even though the decisions in which listeners expressed little confidence
had been discarded.
Hence it seems that the syllable alignment procedure and the policy
of replication were successful.
.pp
.EQ
delim @@
.EN
The worst target scores were for utterances $0.11 and $0.79.
.EQ
delim $$
.EN
Both of these contained large unbroken voiced periods
in the "variable" part \(em almost twice as long as the next longest
voiced period.
The first has an unstressed syllable followed by
a stressed one with no break in voicing,
involving, in a natural contour,
a fast but continuous climb in pitch over the juncture,
and it is not surprising that it proved to be the most difficult target.
A more sophisticated "smoothing" algorithm than the
one used may be worth investigating.
.pp
In a third experiment, sentences with two variable parts were used to check
that the results of the second experiment extended to more complex
utterances.
The overall success rate was 75%, significantly different from chance.
However, a breakdown of the results by source and target utterance
showed that there was one contour (for the utterance
"the price is 19 dollars and 8 cents") which exhibited very successful
transfer, subjects identifying the transferred-pitch utterances at only
a chance level.
.pp
Finally, transfers of pitch from utterances with two variable parts
to those with one variable part were tested.
Pitch contours were transferred to sentences with the same "cents"
figure but no "dollars" part; for example,
"the price is five dollars and thirteen cents"
to
"the price is thirteen cents".  The
contour was simply copied between the corresponding
syllables, so that no adjustment needed to be made
for different syllable structures.
The overall score was 60 successes in 100 judgements \(em
the same percentage as in the second experiment.
.pp
To summarize the results of these four experiments,
.LB
.NP
even accomplished linguists cannot distinguish an utterance from one with
pitch transferred from a different recording of it;
.NP
when the utterance contained only one variable part embedded in a
carrier sentence,
lay listeners identified the original correctly in 60% of cases,
over a wide variety of syllable structures:  this
figure differs significantly from the chance value of 50%;
.NP
lay listeners identified the original confidently and correctly in
50% of cases; confidently but incorrectly in 25% of cases;
.NP
the greatest hindrance to successful transfer was the presence of
a long uninterrupted period of voicing in the target utterance;
.NP
the performance of the method deteriorates as the number
of variable parts in the utterances increases;
.NP
some utterances seemed to serve better than others as the pitch source for
transfer, although this was not correlated with complexity of syllable structure;
.NP
even when the utterance contained two variable parts,
there was one source utterance whose pitch contour was
transferred to all the others so successfully that listeners could not identify
the original.
.LE
.pp
The fact that only 60% of originals in the second experiment were
spotted by lay listeners in a stringent
paired-comparison test \(em many of them being identified without confidence \(em
does encourage the use of the procedure for generating stereotyped,
but different, utterances of high quality in voice-response systems.
The experiments indicate that although different syllable patterns
can be handled satisfactorily by this procedure,
long voiced periods should be avoided if possible when designing
the message set,
and that if individual utterances must contain multiple variable parts
the source utterance should be chosen with the aid of listening tests.
.sh "8.3  Assigning timing and pitch to synthetic speech"
.pp
The pitch transfer method can give good results within a fairly narrow
domain of application.
But like any speech output technique which treats complete utterances
as a single unit, with provision for a small number of slot-fillers to
accomodate data-dependent messages, it becomes unmanageable in more general
situations with a large variety of utterances.
As with segmental synthesis it becomes necessary to consider methods
which use a textual rather than an acoustically-based representation
of the prosodic features.
.pp
This raises a problem with prosodics that was not there for segmentals:  how
.ul
can
prosodic features be written in text form?
The standard phonetic transcription method does not give much help with
notation for prosodics.  It does provide a diacritical mark to indicate
stress, but this is by no means enough information for synthesis.
Furthermore, text-to-speech procedures (described in the next chapter)
promise to allow segmentals to be specified by an ordinary orthographic
representation of the utterance; but we have seen that considerable
intelligence is required to derive prosodic features from text.
(More than mere intelligence may be needed:  this is underlined by a paper
(Bolinger, 1972)
delightfully entitled
"Accent is predictable \(em if you're a mind reader"!)
.[
Bolinger 1972 Accent is predictable \(em if you're a mind reader
.]
.pp
If synthetic speech is to be used as a computer output medium rather
than as an experimental tool for linguistic research, it is important
that the method of specifying utterances is natural and easy to learn.
Prosodic features must be communicated to the computer in a manner
considerably simpler than individual duration and pitch specifications
for each phoneme, as was required in early synthesis-by-rule systems.
Fortunately, a notation has been developed for conveying some of the
prosodic features of utterances, as a by-product of the linguistically
important task of classifying the intonation contours used in
conversational English (Halliday, 1967).
.[
Halliday 1967
.]
This system has even been used to help foreigners speak English
(Halliday, 1970) \(em which emphasizes the fact that it was designed for use
by laymen, not just linguists!
.[
Halliday 1970 Course in spoken English: Intonation
.]
.pp
Here are examples of the way utterances can be conveyed to the ISP
speech synthesis system which was described in the previous chapter.
The notation is based upon Halliday's.
.LB
.NI
3
.ul
^  aw\ t\ uh/m\ aa\ t\ i\ k  /s\ i\ n\ th\ uh\ s\ i\ s  uh\ v  /*s\ p\ ee\ t\ sh,
.NI
1
.ul
^  f\ r\ uh\ m  uh  f\ uh/*n\ e\ t\ i\ k  /r\ e\ p\ r\ uh\ z\ e\ n/t\ e\ i\ sh\ uh\ n.
.LE
(Automatic synthesis of speech, from a phonetic representation.)  Three
levels of stress are distinguished:  tonic or "sentence" stress,
marked by "*" before the syllable; foot stress (marked by "/");
and unstressed syllables.
The notion of a "foot" controls the rhythm of the speech in a way that
will be described shortly.
A fourth level of stress is indicated on a segmental basis when a syllable
contains a reduced vowel.
.pp
Utterances are divided by punctuation into
.ul
tone groups,
which are the basic prosodic unit \(em there are two in the example.
The shape of the pitch contour is governed by a numeral at the start of
each tone group.
Crude control over pauses is achieved by punctuation marks:  full stop, for
example, signals a pause while comma does not.
(Longer pauses can be obtained by several full stops as in "...".)  The
"^" character stands for a so-called "silent stress" or breath point.
Word boundaries are marked by two spaces between phonemes.
As mentioned in the previous chapter, syllable boundaries and explicit
pitch and duration specifiers can also be included in the input.
If they are not, the ISP system will attempt to compute them.
.rh "Rhythm."
Our understanding of speech rhythm knows many laws but little order.
In the mid 1970's there was a spate of publications reporting new data
on segmental duration in various contexts, and there is a growing
awareness that segmental duration is influenced by a great many factors,
ranging from the structure of a discourse, through semantic and syntactic
attributes of the utterances, their phonemic and phonetic make-up,
right down to physiological constraints
(these multifarious influences are ably documented and reviewed by
Klatt, 1976).
.[
Klatt 1976 Linguistic uses of segment duration in English
.]
What seems to be lacking in this work is a conceptual framework on to
which new information about segmental duration can be nailed.
.pp
One starting-point for imitating the rhythm of English speech is the
hypothesis of regularly recurring stresses.
These stresses are primarily
.ul
rhythmic
ones, and should be distinguished from the tonic stress mentioned above which
is primarily an
.ul
intonational
one.
Rhythmic stresses are marked in the transcription by a "/".
The stretch between one and the next is called a "foot",
and the hypothesis above is often referred to as that of isochronous feet
("isochronous" means "of equal time").
There is considerable controversy about this hypothesis.
It is most popular among British linguists and, it must be admitted,
amongst those who work by introspection and intuition and do not actually
.ul
measure
things.
Although the question of isochrony of feet has long been debated, there
seems to be general agreement
\(em even amongst American linguists \(em
that there is at least a tendency towards
equal spacing of foot boundaries.
However, little is known about the strength of this tendency and the extent
of deviations from it (see Hill
.ul
et al,
1979, for an attempt
to quantify it) \(em and there is even evidence to suggest that it may in part
be a
.ul
perceptual
phenomenon (Lehiste, 1973).
.[
Hill Jassem Witten 1979
.]
.[
Lehiste 1973
.]
On this basic point, as on many others, the designer of a prosodic synthesis
strategy must needs make assumptions which cannot be properly justified.
.pp
From a pragmatic point of view there are two advantages to basing
a synthesis strategy on this hypothesis.
Firstly, it provides a way to represent the many influences of higher-level
processes (like syntax and semantics) on rhythm using a simple notation which
fits naturally into the phonetic utterance representation,
and which people find quite easy to understand and generate.
Secondly, it tends to produce a heavily accentuated, but not unnatural,
speech rhythm which can easily be moderated into a more acceptable rhythm
by departing from isochrony in a controlled manner.
.pp
The ISP procedure does not make feet exactly isochronous.
It starts with a standard foot time and attempts to fit the syllables of the
foot into this time.
If doing so would result in certain syllables having less than a preset minimum
duration, the isochrony constraint is relaxed and the foot is expanded.
There is no preset
.ul
maximum
syllable length.
However, when the durations of individual phoneme postures are adjusted
to realize the calculated syllable durations,
limits are imposed on the amount by which individual phonemes can be expanded
or contracted.
Thus a hierarchy of limits exists.
.pp
The rate of talking is determined by the standard foot time.
If this time is short, many feet will be forced to have durations longer than
the standard, and the speech will be "less isochronous".
This seems to accord with common human experience.
If the standard time is longer, however, the minimum syllable limit
will always be exceeded and the speech will be completely isochronous.
If it is too long, the above-mentioned limits to phoneme expansion will
come into play and again partially destroy the isochrony.
.pp
It has often been observed that the final foot of an utterance tends to be
longer than others; as does the tonic foot \(em that which bears the
major stress.
This is easy to accomodate, simply by making the target duration
longer for these feet.
.rh "From feet to syllables."
A foot is a succession of syllables, one or more.
And it is obvious that since there are more syllables in some feet than
in others, some syllables must occupy less time than others in order to preserve
the tendency towards isochrony of feet.
.pp
However, the duration of a foot is not divided evenly between its constituent
syllables.  The syllables have a definite rhythm of their own, which seems
to be governed by
.LB
.NP
the nature of the salient (that is, the first) syllable of the foot
.NP
the presence of word boundaries within the foot.
.LE
A salient syllable tends to be long either if it contains one of
a class of so-called "long" vowels, or if there is a cluster of two or more
consonants following the vowel.
The pattern of syllables and word boundaries governs the rhythm of the foot,
and Table 8.2 shows the possibilities for one-, two-, and three-syllable feet.
This theory of speech rhythm is due to Abercrombie (1964).
.[
Abercrombie 1964 Syllable quantity and enclitics in English
.]
.RF
.nr x2 \w'three-syllable feet  'u
.nr x3 \w'sal-short  'u
.nr x4 \w'weak [#]  'u
.nr x5 \w'weak      'u
.nr x6 \w'/\fIit s incon\fR/ceivable    'u
.nr x1 (\w'syllable rhythm'/2)
.nr x7 \n(x2+\n(x3+\n(x4+\n(x5+\n(x6+\n(x1+\n(x1
.nr x7 (\n(.l-\n(x7)/2
.in \n(x7u
.ta \n(x2u +\n(x3u +\n(x4u +\n(x5u +\n(x6u
.ul
	syllable pattern		example	\0\0\h'-\n(x1u'syllable rhythm
.sp
one-syllable feet	salient			/\fIgood\fR /show	1
	^	weak		/\fI^ good\fR/bye	2:1
.sp
two-syllable feet	sal-long	weak		/\fIcentre\fR /forward	1:1
	sal-short	weak		/\fIatom\fR /bomb	1:2
	salient  #	weak		/\fItea for\fR /two	2:1
.sp
three-syllable feet	salient  #	weak [#]	weak	/\fIone for the\fR /road	2:1:1
				/\fIit's incon\fR/ceivable
	sal-long	weak #	weak	/\fIafter the\fR /war	2:3:1
	sal-short	weak #	weak	/\fImiddle to\fR /top	1:3:2
	sal-long	weak	weak	/\fInobody\fR /knows	3:1:2
	sal-short	weak	weak	/\fIanything\fR /more	1:1:1
.sp
	# denotes a word boundary;
	[#] is an optional word boundary
.ta 0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i
.FG "Table 8.2  Syllable patterns and rhythms"
.pp
A foot may have the rhythmical characteristics of a two-syllable foot
while having only one syllable, if the first place in it is filled by a
silent stress (marked by "^").
This is shown in the second one-syllable example of
Table 8.2.
A similar effect may occur with two- and three-syllable feet,
although examples are not given in the table.
Feet of four and five syllables \(em with or without a silent stress \(em are
considerably rarer.
.pp
Syllabification \(em splitting an utterance into syllables \(em is a job
which had to be done for the pitch-transfer procedure described earlier,
and the nature of syllable rhythms calls for it here too.
Even though the utterance is now specified phonetically instead of
acoustically, the same basic principle applies.
Syllables normally coincide with peaks of sonority,
where "sonority" measures the inherent loudness of a sound relative to
other sounds of the same duration and pitch.
However, difficult cases exist where it seems to be unclear how many syllables
there are in a word.  (Ladefoged, 1975, discusses this problem with examples
such as "real", "realistic", and "reality".)  Furthermore,
.[
Ladefoged 1975
.]
care must be taken to avoid counting two syllables in a word like "sky"
because of its two peaks of sonority \(em for the stop
.ul
k
has lower
sonority than the fricative
.ul
s.
.pp
Three levels of notional sonority are enough for syllabification.
Dividing phoneme segments into
.ul
sonorants
(glides and nasals),
.ul
obstruents
(stops and fricatives), and vowels; a general syllable has the form
.LB
.EQ
<obstruent> sup * ~ <sonorant> sup * ~ <vowel> sup * ~ <sonorant> sup * ~
<obstruent> sup * ~ ,
.EN
.LE
where "*" means repetition, that is, occurrence zero or more times.
This sidesteps the "sky" problem by giving fricatives the same
sonority as stops.
It is easy to use the above structure to count the number
of syllables in a given utterance by counting the sonority
peaks.
.pp
However, what is required is an indication of syllable
.ul
boundaries
as well as a syllable count.
For slow conversational speech, these can be approximated as follows.
Word divisions obviously form syllable boundaries, as should
foot markers \(em but it may be wise not to assume that the latter do if the
utterance has been prepared by someone with little knowledge of linguistics.
Syllable boundaries should be made to coincide with sonority minima.
As an
.ul
ad hoc
pragmatic
rule, if only one segment has the minimum sonority the boundary is placed
before it.
If there are two segments, each with the minimum sonority, it is placed between
them, while for three or more it is placed after the first two.
.pp
These rules produce obviously acceptable divisions in many cases
(to'day, ash'tray, tax'free), with perhaps unexpected positioning of the
boundary in others (ins'pire, de'par'tment).
Actually, people do differ in placement of syllable boundaries
(Abercrombie, 1967).
.[
Abercrombie 1967
.]
.rh "From syllables to segments."
The theory of isochronous feet (with the caveats noted earlier)
and that of syllable rhythms provide a way of producing durations for
individual syllables.  But where are these durations supposed to be measured?
There is a beat point, or tapping point, near the beginning of each syllable.
This is the place where a listener will tap if asked to give one tap to each
syllable; it has been investigated experimentally by Allen (1972).
.[
Allen 1972 Location of rhythmic stress beats in English One
.]
It is not necessarily at the very beginning of the syllable.
For example, in "straight", the tapping point is certainly after the
.ul
s
and the stopped part of the
.ul
t.
.pp
Another factor which relates to the division of the syllable duration
amongst phonetic segments is the often-observed fact that the length of the
vocalic nucleus is a strong clue to the degree of voicing of the terminating
cluster (Lehiste, 1970).
.[
Lehiste 1970 Suprasegmentals
.]
If you say in pairs words like "cap", "cab"; "cat", "cad"; "tack", "tag"
you will find that the vowel in the first word of each pair is significantly
shorter than that in the second.
In fact, the major difference between such pairs is the vowel length,
not the final consonant.
.pp
Such effects can be taken into account by considering a syllable to comprise
an initial consonant cluster, followed by a vocalic nucleus and a final
consonant cluster.
Any of these elements can be missing \(em the most unusual case where the
nucleus is absent occurs, for example, in so-called syllabic
.ul
n\c
\&'s
(as in renderings of "button", "pudding" which might be written
"butt'n", "pudd'n").
However, it is convenient to modify the definition of the nucleus
so as to rule out the possibility of it being empty.
Using the characterization of the syllable given above, the clusters can
be defined as
.LB
.NI
initial cluster	=  <obstruent>\u*\d <sonorant>\u*\d
.NI
nucleus	=  <vowel>\u*\d <sonorant>\u*\d
.NI
final cluster	=  <obstruent>\u*\d.
.LE
Sonorants are included in the nucleus so that it is always present,
even in the case of a syllabic consonant.
.pp
Then, rules can be used to divide the syllable duration between the
initial cluster, nucleus, and final cluster.
These must distinguish between situations where the terminating cluster
is voiced or unvoiced so that the characteristic differences in vowel lengths
can be accomodated.
.pp
Finally, the cluster durations must be apportioned amongst their constituent
phonetic segments.  There is little published data on which to base this.
Two simple schemes which have been used in ISP are described in
Witten (1977) and Witten & Smith (1977).
.[
Witten 1977 A flexible scheme for assigning timing and pitch to synthetic speech
.]
.[
Witten Smith 1977 Synthesizing British English rhythm
.]
.rh "Pitch."
There are two basically different ways of looking at the pitch of an
utterance.
One is to imagine pitch
.ul
levels
attached to individual syllables.
This has been popular amongst American linguists, and some people
have even gone so far as to associate pitch levels with levels of
stress.
The second approach is to consider pitch
.ul
contours,
as we did earlier when examining how to transfer pitch from one utterance
to another.
This seems to be easier for the person who transcribes the utterances
to produce, for the information required is much less detailed than levels
attached to each syllable.  Some indication needs to be given of how
the contour is to be bound to the utterance, and in the notation introduced above
the most prominent, or "tonic", syllable is indicated in the transcription.
.pp
Halliday's (1970) classification identifies five different primary intonation
contours, each hinging on the tonic syllable.
.[
Halliday 1970 Course in spoken English: Intonation
.]
These are sketched in Figure 8.5, in the style of Halliday.
.FC "Figure 8.5"
Several secondary contours, which are variations on the primary ones,
are defined as well.
However, this classification scheme is intended for consumption by people,
who bring to the problem a wealth of prior knowledge of speech and years
of experience with it!  It captures only the gross features
of the infinite variety of pitch contours found in living speech.
In a sense, the classification is
.ul
phonological
rather than
.ul
phonetic,
for it attempts to distinguish the features which make a logical difference
to the listener instead of the acoustic details of the pitch contours.
.pp
It is necessary to take these contours and subject them to a sort of
phonological-to-phonetic embellishment before applying them in synthetic
speech.
For example, the stretches with constant pitch which precede the tonic
syllable in tone groups 1, 2, and 3 sound
most unnatural when synthesized \(em for pitch is hardly ever
exactly constant in living speech.
Some pretonic pitch variation is necessary,
and this can be made to emphasize the salient syllable
of each foot.  A "lilting" effect which reaches a peak at each foot
boundary, and drops rather faster at the beginning of the foot than it
rises at the end, sounds more natural.  The magnitude of this inflection
can be altered slightly to add interest, but a considerable increase in it
produces a semantic change by making the utterance sound more emphatic.
It is a major problem to pin down exactly the turning points of pitch in
the falling-rising and rising-falling contours (4 and 5 in Figure 8.5).
And even deciding on precise values for the pitch frequencies involved is not
always easy.
.pp
The aim of the pitch assignment method of ISP is to allow the person
(or program) which originates a spoken message to exercise a great deal
of control over its intonation, without having to concern himself with
foot or syllable structure.  The message to be spoken must be broken down
into tone groups,
which correspond roughly to Halliday's tone groups.
Each one comprises a
.ul
tonic
of one or more feet, which is optionally preceded by a
.ul
pretonic,
also with a number of feet.  It is advantageous to allow a tone group
boundary to occur in the middle of a foot (whereas Halliday's scheme
insists that it occurs at a foot boundary).
The first foot of the tonic, the
.ul
tonic foot,
is marked by an asterisk at the beginning.
It is on the first syllable of this foot \(em the
"tonic" or "nuclear"
syllable \(em that the major stress of the tone group occurs.
If there is no asterisk in a tone group,
ISP takes the final foot as the tonic
(since this is the most common case).
.pp
The pitch contour on a tone group is specified by an array of ten numbers.
Of course, the system cannot generate all conceivable contours for a tone
group, but the definitions of the ten specifiable quantities have been
chosen to give a useful range of contours.
If necessary, more precise control over the pitch of an utterance can
be achieved by making the tone groups smaller.
.pp
The overall pitch movement is controlled by specifying the pitch at three
places:  the beginning of the tone group, the beginning of the tonic syllable,
and the end of the tone group.
Provision is made for an abrupt pitch break at the start of the tonic
syllable in order to simulate tone groups 2 and 3, and, to a lesser
extent, tone groups 4 and 5.
The pitch is interpolated linearly over the first part of the
tone group (up to the tonic syllable) and over the last part (from there to
the end), except that it is possible to specify a non-linearity on the tonic
syllable, for emphasis, as shown in Figure 8.6.
.FC "Figure 8.6"
.pp
On this basic shape are superimposed two finer pitch patterns.
One of these is an initialization-continuation option which allows
the pitch to rise (or fall) independently on the initial and final feet
to specified values, without affecting the contour on the rest
of the tone group (Figure 8.7).
.FC "Figure 8.7"
The other is a foot pattern which is superimposed on each pretonic foot,
to give the stressed syllables of the pretonic added prominence and avoid
the monotony of constant pitch.
This is specified by a
.ul
non-linearity
parameter which distorts the contour on the foot at a pre-determined
point along it.
Figure 8.8 shows the effect.
.FC "Figure 8.8"
.pp
The ten quantities that define a pitch contour are summarized in
Table 8.3, and shown diagrammatically in Figure 8.9.
.FC "Figure 8.9"
.RF
.nr x0 \w'H:    'u
.nr x1 \n(x0+\w'fraction along foot of the non-linearity position, for the tonic foot'u
.nr x1 (\n(.l-\n(x1)/2
.in \n(x1u
.ta \n(x0u +4n
A:	continuation from previous tone group
		zero gives no continuation
		non-zero gives pitch at start of tone group
B:	notional pitch at start
C:	pitch range on whole of pretonic
D:	departure from linearity on each foot of pretonic
E:	pitch change at start of tonic
F:	pitch range on tonic
G:	departure from linearity on tonic
H:	continuation to next tone group
		zero gives no continuation
		non-zero gives pitch at end of tone group
I:	fraction along foot of the non-linearity position, for pretonic feet
J:	fraction along foot of the non-linearity position, for the tonic foot
.ta 0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i
.in 0
.FG "Table 8.3  The quantities that define a pitch contour"
.pp
The intention of this parametric method of specifying contours
is that the parameters should be easily derivable from semantic variables
like emphasis, novelty of idea, surprise, uncertainty, incompleteness.
Here we really are getting into controversial, unresearched areas.
Roughly speaking, parameters D and G control emphasis, G by itself
controls novelty and surprise, and H and the relative sizes of E and F
control uncertainty and incompleteness.
Certain parameters (notably I and J) are defined because although they
do not appear to correspond to semantic distinctions, we do not yet know
how to generate them automatically.
.RF
.nr x0 0.6i+0.5i+0.5i+0.5i+0.5i+0.5i+0.5i+0.5i+0.5i+0.5i+0.5i+\w'0000'
.nr x1 (\n(.l-\n(x0)/2
.in \n(x1u
.ta 0.6i +0.5i +0.5i +0.5i +0.5i +0.5i +0.5i +0.5i +0.5i +0.5i +0.5i
Halliday's
tone group	\0\0A	\0\0B	\0\0C	\0\0D	\0\0E	\0\0F	\0\0G	\0\0H	\0\0I	\0\0J
\l'\n(x0u\(ul'
.sp
	1	\0\0\00	\0175	\0\0\00	\0\-40	\0\0\00	\-100	\0\-40	\0\0\00	0.33	\00.5
	2	\0\0\00	\0280	\0\0\00	\0\-40	\-190	\0100	\0\0\00	\0\0\00	0.33	\00.5
	3	\0\0\00	\0175	\0\0\00	\0\-40	\0\-70	\0\045	\0\-10	\0\0\00	0.33	\00.5
	4	\0\0\00	\0280	\-100	\0\-40	\0\020	\0\045	\0\-45	\0\0\00	0.33	\00.5
	5	\0\0\00	\0175	\0\060	\0\-40	\0\-20	\0\-45	\0\045	\0\0\00	0.33	\00.5
\l'\n(x0u\(ul'
.ta 0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i
.in 0
.FG "Table 8.4  Pitch contour table for Halliday's primary tone groups"
.pp
One basic requirement of the pitch assignment scheme was the ability to
generate contours which approximate Halliday's five primary tone groups.
Values of the ten specifiable quantities are given in Table 8.4, for each
tone group.
All pitches are given in\ Hz.
A distinctly dipping pitch movement has been given to each pretonic foot
(parameter D),
to lend prominence to the salient syllables.
.sh "8.4  Evaluating prosodic synthesis"
.pp
It is extraordinarily difficult to evaluate schemes for prosodic synthesis,
and this is surely a large part of the reason why prosodics are among the
least advanced aspects of artificial speech.
Segmental synthesis can be tested by playing people minimal pairs of
words which differ in just one feature that is being investigated.
For example, one might experiment with "pit", "bit"; "tot", "dot";
"cot", "got" to test the rules which discriminate unvoiced from voiced stops.
There are standard word-lists for intelligibility tests which can be
used to compare systems, too.
No equivalent of such micro-level evaluation exists for prosodics,
for they by definition have a holistic effect on utterances.
They are most noticeable, and most important, in longish stretches of speech.
Even monotonous, arhythmic speech will be intelligible in
sufficiently short samples provided the segmentals are good enough;
but it is quite impossible to concentrate on such speech in quantity.
Some attempts at evaluation appear in Ainsworth (1974) and McHugh (1976),
but these are primarily directed at assessing the success of pronunciation
rules, which are discussed in the next chapter.
.[
Ainsworth 1974 Performance of a speech synthesis system
.]
.[
McHugh 1976 Listener preference and comprehension tests
.]
.pp
One evaluation technique is to compare synthetic with natural versions
of utterances, as was done in the pitch transfer experiment.
The method described earlier used a sensitive paired-comparison test,
where subjects heard both versions in quick succession and were asked
to judge which was "most natural and intelligible".
This is quite a stringent test, and one that may not be so useful
for inferior, completely synthetic, contours.
It is essential to degrade the "natural" utterance so that it is
comparable segmentally to the synthetic one:  this was done in the
experiment described by extracting its pitch and resynthesizing it
from linear predictive coefficients.
.pp
Several other experiments could be undertaken to evaluate artificial
prosody.
For example, one could compare
.LB
.NP
natural and artificial rhythms, using artificial segmental synthesis
in both cases;
.NP
natural and artificial pitch contours, using artificial segmental synthesis
in both cases;
.NP
natural and artificial pitch contours, using segmentals extracted from
natural utterances.
.LE
There are many other topics which have not yet been fully investigated.
It would be interesting, for example, to define rules for generating speech
at different tempos.
Elisions, where phonemes or even whole syllables are suppressed,
occur in fast speech; these have been analyzed by linguists
but not yet incorporated into synthetic models.
It should be possible to simulate emotion by altering parameters such as
pitch range and mean pitch level; but this seems exceptionally difficult
to evaluate.  One situation where it would perhaps be possible to
measure emotion is in the reading of sports results \(em in fact a study
has already been made of intonation in soccer results (Bonnet, 1980)!
.[
Bonnet 1980
.]
Even the synthesis of voices with different pitch ranges requires
investigation, for, as noted earlier, it is difficult to place
precise frequency specifications on phonological contours such as
those sketched in Figure 8.5.
Clearly the topic of prosodic synthesis is a rich and potentially
rewarding area of research.
.sh "8.5  References"
.LB "nnnn"
.[
$LIST$
.]
.LE "nnnn"
.sh "8.6  Further reading"
.pp
There are quite a lot of books in the field of linguistics which
describe prosodic features.
Here is a small but representative sample from both sides of the Atlantic.
.LB "nn"
.\"Abercrombie-1965-1
.]-
.ds [A Abercrombie, D.
.ds [D 1965
.ds [T Studies in phonetics and linguistics
.ds [I Oxford Univ Press
.ds [C London
.nr [T 0
.nr [A 1
.nr [O 0
.][ 2 book
.in+2n
Abercrombie is one of the leading English authorities on phonetics,
and this is a collection of essays which he has written over the years.
Some of them treat prosodics explicitly, and others show the influence
of verse structure on Abercrombie's thinking.
.in-2n
.\"Bolinger-1972-2
.]-
.ds [A Bolinger, D.(Editor)
.ds [D 1972
.ds [T Intonation
.ds [I Penguin
.ds [C Middlesex, England
.nr [T 0
.nr [A 0
.nr [O 0
.][ 2 book
.in+2n
A collection of papers that treat a wide variety of different aspects
of intonation in living speech.
.in-2n
.\"Crystal-1969-3
.]-
.ds [A Crystal, D.
.ds [D 1969
.ds [T Prosodic systems and intonation in English
.ds [I Cambridge Univ Press
.nr [T 0
.nr [A 1
.nr [O 0
.][ 2 book
.in+2n
This book attempts to develop a theoretical basis for the study of British
English intonation.
.in-2n
.\"Gimson-1966-3
.]-
.ds [A Gimson, A.C.
.ds [D 1966
.ds [T The linguistic relevance of stress in English
.ds [B Phonetics and linguistics
.ds [E W.E.Jones and J.Laver
.ds [P 94-102
.nr [P 1
.ds [I Longmans
.ds [C London
.nr [T 0
.nr [A 1
.nr [O 0
.][ 3 article-in-book
.in+2n
Here is a careful discussion of what is meant by "stress", with much more
detail than has been possible in this chapter.
.in-2n
.\"Lehiste-1970-4
.]-
.ds [A Lehiste, I.
.ds [D 1970
.ds [T Suprasegmentals
.ds [I MIT Press
.ds [C Cambridge, Massachusetts
.nr [T 0
.nr [A 1
.nr [O 0
.][ 2 book
.in+2n
This is a comprehensive study of suprasegmental phenomena in natural speech.
It is divided into three major sections:  quantity (timing), tonal features
(pitch), and stress.
.in-2n
.\"Pike-1945-5
.]-
.ds [A Pike, K.L.
.ds [D 1945
.ds [T The intonation of American English
.ds [I Univ of Michigan Press
.ds [C Ann Arbor, Michigan
.nr [T 0
.nr [A 1
.nr [O 0
.][ 2 book
.in+2n
A classic, although somewhat dated, study.
Notice that it deals specifically with American English.
.in-2n
.LE "nn"
.EQ
delim $$
.EN
.CH "9  GENERATING SPEECH FROM TEXT"
.ds RT "Generating speech from text
.ds CX "Principles of computer speech
.pp
In the preceding two chapters I have described how artificial speech
can be produced from a written phonetic representation with additional
markers indicating intonation contours, points of major stress, rhythm,
and pauses.
This representation is substantially the same as that used by linguists
when recording natural utterances.
What we will discuss now are techniques for generating this information,
or at least some of it, from text.
.pp
Figure 9.1 shows various levels of the speech synthesis process.
.FC "Figure 9.1"
Starting from the top with plain text, the first box splits it into
intonation units (tone groups), decides where the major emphases
(tonic stresses) should be placed,
and further subdivides the tone group into rhythmic units (feet).
For intonation analysis it is necessary to decide on an "interpretation"
of the text, which in turn, as was emphasized at the beginning of the
previous chapter, depends both on the semantics of what is being said and
on the attitude of the speaker to his material.
The resulting representation will be at the level of Halliday's notation
for utterances, with the words still in English rather than phonetics.
Table 9.1 illustrates the utterance representation at the various levels
of the Figure.
.RF
.nr x0 \w'pitch and duration    '+\w'at 8 kHz sampling rate a 4-second utterance'
.nr x1 (\n(.l-\n(x0)/2
.in \n(x1u
.ta \w'pitch and duration    'u +\w'pause  'u +\w'00 msec   'u
representation	example
\l'\n(x0u\(ul'
.sp
plain text	Automatic synthesis of speech,
	from a phonetic representation.
.sp
text adorned with	3\0^ auto/matic /synthesis of /*speech,
prosodic markers	1\0^ from a pho/*netic /represen/tation.
.sp
phonetic text with	3\0\fI^  aw t uh/m aa t i k  /s i n th uh s i s\fR
prosodic markers	\0\0\fIuh v  /*s p ee t sh\fR ,
	1\0\fI^  f r uh m  uh  f uh/*n e t i k\fR
	\0\0\fI/r e p r uh z e n/t e i sh uh n\fR .
.sp
phonemes with	pause	80 msec
pitch and duration	\fIaw\fR	70 msec	105 Hz
	\fIt\fR	40 msec	136 Hz
	\fIuh\fR	50 msec	148 Hz
	\fIm\fR	70 msec	175 Hz
	\fIaa\fR	90 msec	140 Hz
		...
		...
		...
.sp
parameters for	10 parameters, each updated at a frame
formant or linear	rate of 10 msec
predictive	(4 second utterance gives 400 frames,
synthesizer	or 4,000 data values)
.sp
acoustic wave	at 8 kHz sampling rate a 4-second utterance
	has 32,000 samples
\l'\n(x0u\(ul'
.ta 0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i
.in 0
.FG "Table 9.1  Utterance representations at various levels in speech synthesis"
.pp
The next job is to translate the plain text into a broad phonetic
transcription.
This requires knowledge of letter-to-sound pronunciation
rules for the language under consideration.
But much more is needed.  The structure of each word must be examined for
prefixes and suffixes, because they \(em especially the latter \(em have a
strong influence on pronunciation.
This is called "morphological" analysis.
Actually it is also required for rhythmical purposes, because prefixes
are frequently unstressed (note that the word "prefix" is itself an
exception to this!).
Thus the appealing segmentation of the overall problem shown in Figure 9.1
is not very accurate, for the individual processes cannot be rigidly
separated as it implies.  In fact, we saw earlier how this intermixing of
levels occurs with prosodic and segmental features.
Nevertheless, it is helpful to structure discussion of the problem by
separating levels as a first approximation.
Further influences on pronunciation come from the semantics and syntax
of the utterance \(em and both also play a part in intonation and rhythm analysis.
The result of this second process is a phonetic representation, still
adorned with prosodic markers.
.pp
Now we move down from higher-level intonation and rhythm considerations
to the details of the pitch contour and segment durations.
This process was the subject of the previous chapter.
The problems are twofold:  to map an appropriate acoustic pitch contour
on to the utterance, using tonic stress point and foot boundaries as
anchor points; and to assign durations to segments using the
foot\(emsyllable\(emcluster\(emsegment hierarchy.
If it is accepted that the overall rhythm can be captured adequately by foot
markers, this process does not interact with earlier ones.
However, many researchers do not, believing instead that rhythm is
syntactically determined at a very detailed level.
This will, of course, introduce strong interaction between the duration
assignment process and the levels above.
(Klatt, 1975, puts it into his title \(em
"Vowel lengthening is syntactically determined in a connected discourse".
.[
Klatt 1975 Vowel lengthening is syntactically determined
.]
Contrast this with the paper cited earlier (Bolinger, 1972) entitled
"Accent is predictable \(em if you're a mind reader".
.[
Bolinger 1972 Accent is predictable \(em if you're a mind reader
.]
No-one would disagree that "accent" is an influential factor in vowel length!)
.pp
Notice incidentally that the representation of the result of the pitch
and duration assignment process in Table 9.1 is inadequate, for each segment
is shown as having just one pitch.
In practice the pitch varies considerably throughout every segment,
and can easily rise and fall on a single one.  For example,
.LB
"he's
.ul
very
good"
.LE
may have a rise-fall on the vowel of "very".
The linked event-list data-structure of ISP is much more suitable
than a textual string for utterance representation at this level.
.pp
The fourth and fifth processes of Figure 9.1 have little interaction with
the first two, which are the subject of this chapter.  Segmental
concatenation, which was treated in Chapter 7, is affected by prosodic
features like stress; but a notation which indicates stressed syllables
(like Halliday's) is sufficient to capture this influence.
Contextual modification of segments, by which I mean
the coarticulation effects which govern allophones of phonemes,
is included explicitly in the fourth process to emphasize that the upper levels
need only provide a broad phonemic transcription rather than a detailed
phonetic one.
Signal synthesis can be performed by either a formant synthesizer or a
linear predictive one (discussed in Chapters 5 and 6).
This will affect the details of the segmental concatenation process but should have no
impact at all on the upper levels.
.pp
Figure 9.1 performs a useful function by summarizing where we have
been in earlier chapters \(em the lower three boxes \(em and introducing the
remaining problems that must be faced by a full text-to-speech system.
It also serves to illustrate an important point:  that a speech output system
can demand that its utterances be entered in any of a wide range of
representations.
Thus one can enter at a low level with a digitized waveform or linear
predictive parameters; or higher up with a phonetic representation
that includes detailed pitch and duration specification at the phoneme level;
or with a phonetic text or plain text adorned with prosodic markers;
or at the very top with plain text as it would appear in a book.
A heavy price in naturalness and intelligibility is paid by moving up
.ul
any
of these levels \(em and this is just as true at the top of the Figure as
at the bottom.
.sh "9.1  Deriving prosodic features"
.pp
If you really need to start with plain text,
some very difficult problems present themselves.
The text should be understood, first of all, and then decisions need to be
made about how it is to be interpreted.
For an excellent speaker \(em like an actor \(em these decisions will be artistic,
at least in part.
They should certainly depend upon the opinion and attitude of the speaker,
and his perception of the structure and progress of the dialogue.
Very little is known about this upper level of speech synthesis from text.
In practice it is almost completely ignored \(em and the speech is at most
barely intelligible, and certainly uncomfortable to listen to.
Hence anybody contemplating building or using a speech output system which
starts from something close to plain text should consider carefully whether some extra
semantic information can be coded into the initial utterances to help with
prosodic interpretation.
Only rarely is this impossible \(em and reading machines for the blind are
a prime example of a situation where arbitrary, unannotated, texts
must be read.
.rh "Intonation analysis."
One distinction which a program can usefully try
to make is between basically rising
and basically falling pitch contours.  It is often said that pitch rises on
a question and falls on a statement, but if you listen to speech you will
find this to be a gross oversimplification.  It normally
falls on statements, certainly; but it falls as often as it rises on questions.
It is more accurate to say that pitch rises on "yes-no" questions
and falls on other utterances, although this rule is still only a rough guide.
A simple test which operates lexically on the input text is to determine
whether a sentence is a question by looking at the 
punctuation mark at its end, and then to examine the first word.
If it is a "wh"-word like "what", "which", "when", "why" (and also "how")
a falling contour is likely to fit.
If not, the question is probably a yes-no one, and the contour
should rise.
Such a crude rule will certainly not be very accurate
(it fails, for example, when the "wh"-word is embedded in a phrase as in
"at what time are you going?"), but at least it provides a starting-point.
.pp
An air of finality is given to an utterance when it bears a definite
fall in pitch, dropping to a rather low value at the end.
This should accompany the last intonation unit in an utterance
(unless it is a yes-no question).
However, a rise-fall contour such as Halliday's tone group 5 (Figure 8.5)
can easily be used in utterance-final position by one person
in a conversation \(em
although it would be unlikely to terminate the dialogue altogether.
A new topic is frequently introduced by a fall-rise contour \(em such as
Halliday's tone group 4 \(em and this often begins a paragraph.
.pp
Determining the type of pitch contour is only one part of
intonation assignment.  There are really three separate problems:
.LB
.NP
dividing the utterance into tone groups
.NP
choosing the tonic syllable, or major stress point, of each one
.NP
assigning a pitch contour to each tone group.
.LE
Let us continue to use the Halliday notation for intonation, which was introduced
in simplified form in the previous chapter.
Moreover, assume that the foot boundaries can be placed correctly \(em
this problem will be discussed in the next subsection.
Then a scheme which considers only the lexical form of the utterance
and does not attempt to "understand" it (whatever that means) is as follows:
.LB
.NP
place a tone group boundary at every punctuation mark
.NP
place the tonic at the first syllable of the last foot in a tone group
.NP
use contour 4 for the first tone group in a paragraph and contour 1
elsewhere, except for a yes-no question which receives contour 2.
.LE
.RF
.nr x0 \w'From Scarborough to Whitby\0\0\0\0'+\w'4  ^  from /Scarborough to /*Whitby is a'
.nr x1 (\n(.l-\n(x0)/2
.in \n(x1u
.ta \w'From Scarborough to Whitby\0\0\0\0\0\0'u
plain text	text adorned with prosodic markers
\l'\n(x0u\(ul'
.sp
From Scarborough to Whitby is a	4 ^ from /Scarborough to /*Whitby is a
very pleasant journey, with	1\- very /pleasant /*journey with
very beautiful countryside.	1\- very /beautiful /*countryside ...
In fact the Yorkshire coast is	1+ ^ in /fact the /Yorkshire /coast is
\0\0\0\0lovely,	\0\0\0\0/*lovely
all along, ex-	1+ all a/*long ex
cept the parts that are covered	_4 cept the /parts that are /covered
\0\0\0\0in caravans of course; and	\0\0\0\0in /*caravans of /course and
if you go in spring,	4 if you /go in /*spring
when the gorse is out,	4 ^ when the /*gorse is /out
or in summer,	4 ^ or in /*summer
when the heather's out,	4 ^ when the /*heather's /out
it's really one of the most	13 ^ it's /really /one of the /most
\0\0\0\0delightful areas in the	\0\0\0\0de/*lightful /*areas in the
whole country.	1 whole /*country
.sp
The moorland is	4 ^ the /*moorland is
rather high up, and	1 rather /high /*up and
fairly flat \(em a	1 fairly /*flat a
sort of plateau.	1 sort of /*plateau ...
At least,	1 ^ at /*least
it isn't really flat,	13 ^ it /*isn't /really /*flat
when you get up on the top;	\-3 ^ when you /get up on the /*top
it's rolling moorland	1 ^ it's /rolling /*moorland
cut across by steep valleys.  But	1 cut across by /steep /*valleys but
seen from the coast it's	4 seen from the /*coast it's ...
"up there on the moors", and you	1 up there on the /*moors and you
always think of it as a	_4 always /*think of it as a
kind of tableland.	1 kind of /*tableland
\l'\n(x0u\(ul'
.ta 0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i
.in 0
.FG "Table 9.2  Example of intonation and rhythm analysis (from Halliday, 1970)"
.[
Halliday 1970 Course in spoken English: Intonation
.]
.pp
These extremely crude and simplistic rules are really the most that one can do
without subjecting the utterance to a complicated semantic analysis.
In statistical terms, they are actually remarkably effective.
Table 9.2 shows part of a spontaneous monologue which was transcribed by
Halliday and appears in his teaching text on intonation
(Halliday, 1970, p 133).
.[
Halliday 1970 Course in Spoken English: Intonation
.]
Among the prosodic markers are some that were not introduced in Chapter 8.
Firstly, each tone group has secondary contours which are identified
by "1+", "1\-" (for tone group 1), and so on.
Secondly, the mark "..." is used to indicate a pause which disrupts
the speech rhythm.
Notice that its positioning belies the advice of the old elocutionists:
.br
.ev2
.in 0
.LB
.fi
A Comma stops the Voice while we may privately tell
.NI
.ul
one,
a Semi-colon
.ul
two;
a Colon
.ul
three:\c
  and a Period
.ul
four.
.br
.nr x0 \w'\fIone,\fR a Semi-colon \fItwo;\fR a Colon \fIthree:\fR  and a Period \fIfour.'-\w'(Mason,\fR 1748)'
.NI
\h'\n(x0u'(Mason, 1748)
.nf
.LE
.br
.ev
Thirdly, compound tone groups such as "13" appear which contain
.ul
two
tonic syllables.
This differs from a simple concatenation of tone groups
(with contours 1 and 3 in this case) because the second is in some sense subsidiary to
the first.
Typically it forms an adjunct clause, while the first clause gives the
main information.  Halliday provides many examples, such as
.LB
.NI
/Jane goes /shopping in /*town /every /*Friday
.NI
/^ I /met /*Arthur on the /*train.
.LE
But he does not comment on the
.ul
acoustic
difference between a compound tone group and a concatenation of simple ones \(em
which is, after all, the information needed for synthesis.
A final, minor, difference between Halliday's scheme and that outlined earlier
is that he compels tone group boundaries to occur at the beginning
of a foot.
.RF
.nr x0 3.3i+1.3i+\w'complete'
.nr x1 (\n(.l-\n(x0)/2
.in \n(x1u
.ta 3.3i +1.3i
	excerpt in	complete
	Table 9.2	passage
\l'\n(x0u\(ul'
.sp
number of tone groups	25	74
.sp
number of boundaries correctly	19 (76%)	47 (64%)
placed
.sp
number of boundaries incorrectly	\00	\01 (\01%)
placed
.sp
number of tone groups having a	22 (88%)	60 (81%)
tonic syllable at the beginning
of the final foot
.sp
number of tone groups whose	17 (68%)	51 (69%)
contours are correctly assigned
\l'\n(x0u\(ul'
.sp
number of compound tone groups	\02 (\08%)	\06 (\08%)
.sp
number of secondary intonation	\07 (28%)	13 (17%)
contours
\l'\n(x0u\(ul'
.ta 0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i
.in 0
.FG "Table 9.3  Success of simple intonation assignment rules"
.pp
Applying the simple rules given above to the text of Table 9.2 leads to
the results in the first column of Table 9.3.
Three-quarters of the foot boundaries are flagged by
punctuation marks, with no extraneous ones being included.
88% of tone groups have a tonic syllable at the start of the final foot.
However, the compound tone groups each have two tonic syllables,
and of course only the second one is predicted by the final-foot rule.
Assigning intonation contours on the extremely simple basis of using
contour 4 for the first tone group in a paragraph, and contour 1 thereafter,
also seems to work quite well.  Secondary contours such as "1+" and "1\-"
have been mapped into the appropriate primary contour (1, in this case)
for the present purpose, and compound tone groups have been assigned the first
contour of the pair.
The result is that 68% of contours are given correctly.
.pp
In order to give some idea of the reliability of these figures, the results
for the whole passage transcribed by Halliday \(em of which Table 9.2 is an
excerpt \(em are shown in the second column of Table 9.3.  Although it
looks as though the rules may have been slightly lucky with the excerpt,
the general trends are the same, with 65% to 80% of features being assigned
correctly.
It could be argued, though, that the complete text is punctuated fairly liberally by
present-day standards, so that the tone-group boundary rule is unusually
successful.
.pp
These results are really astonishingly good, considering the crudeness of
the rules.  However, they should be interpreted with caution.
What is missed by the rules, although appearing to comprise only
20% to 35% of the features, is certain to include the important,
information-bearing, and variety-producing features that give the utterance
its liveliness and interest.
It would be rash to assume that all tone-group boundaries,
all tonic positions, and all intonation contours, are equally
important for intelligibility and naturalness.
It is much more likely that the rules predict a
default pattern, while most information is borne by deviations from
them.
To give an engineering analogy, it may be as though the carrier waveform
of a modulated transmission is being simulated, instead of the
information-bearing signal!
Certainly the utterance will, if synthesized with intonation given by these
rules, sound extremely dull and repetitive, mainly because of the
overwhelming predominance of tone group 1 and the universal placement
of tonic stress on the final foot.
.pp
There are certainly many different ways to orate any particular text,
and that given by Halliday and reproduced in Table 9.2 is only one possible
version.
However, it is fair to say that the default intonation discussed above
could only occur naturally under very unusual circumstances \(em such as
a petulant child, unwilling and sulky, having been forced to read aloud.
This is hardly how we want our computers to speak!
.rh "Rhythm analysis."
Consider now how to decide where foot boundaries should be placed
in English text.
Clearly semantic considerations sometimes play a part in this \(em one could
say
.LB
/^ is /this /train /going /*to /London
.LE
instead of the more usual
.LB
/^ is /this /train /going to /*London
.LE
in circumstances where the train might be going
.ul
to
or
.ul
from
London.
Such effects are ignored here, although it is worth noting in passing that the
rogue words will often be marked by underscoring or italicizing
(as in the previous sentence).
If the text is liberally underlined, semantic analysis may
be unnecessary for the purposes of rhythm.
.pp
A rough and ready rule for placing foot boundaries is to insert one before
each word which is not in a small closed set of "function words".
The set includes, for example, "a", "and", "but", "for", "is", "the", "to".
If a verb or adjective begins with a prefix, the boundary should be moved
between it and the root \(em but not for a noun.
This will give the distinction between
.ul
con\c
vert (noun) and con\c
.ul
vert
(verb),
.ul
ex\c
tract and ex\c
.ul
tract,
and for many North American speakers,
will help to distinguish
.ul
in\c
quiry from in\c
.ul
quire.
However, detecting prefixes by a simple splitting algorithm is dangerous.
For example, "predate" is a verb with stress on what appears to be a prefix,
contrary to the rule; while the "pre" in "predator" is not a prefix \(em at
least, it is not pronounced as the prefix "pre" normally is.
Moreover, polysyllabic words like "/diplomat", "dip/lomacy", "diplo/matic";
or "/telegraph", "te/legraphy", "tele/graphic" cannot be handled on such a simple
basis.
.pp
In 1968, a remarkable work on English sound structure was published
(Chomsky and Halle, 1968) which proposes a system of rules to transform
English text into a phonetic representation in terms of distinctive features,
with the aid of a lexicon.
.[
Chomsky Halle 1968
.]
A great deal of attention is paid to stress, and rules are given which
perform well in many tricky cases.
.pp
It uses the American system of levels of stress, marking
so-called primary stress with a superscript 1, secondary stress with a
superscript 2, and so on.
The superscripts are written on the vowel of the stressed
syllable:  completely unstressed syllables receive no annotation.
For example, the sentence "take John's blackboard eraser" is written
.LB
ta\u2\dke Jo\u3\dhn's bla\u1\dckboa\u5\drd era\u4\dser.
.LE
In foot notation this utterance
is
.LB
/take /John's /*blackboard e/raser.
.LE
It undoubtedly contains less information than the stress-level version.
For example, the second syllable of "blackboard" and the first one of "erase"
are both unstressed, although the rhythm rules given in Chapter 8
will cause them
to be treated differently because they occupy different places in the
syllable pattern of the foot.
"Take", "John's", and the second syllable of "erase" are all non-tonic
foot-initial syllables and hence are not distinguished in the notation;
although the pitch contours schematized in Figure 8.9 will give them different
intonations.
.pp
An indefinite number of levels of stress can be used.  For example, according
to the rules given by Chomsky and Halle, the word "sad" in
.LB
my friend can't help being shocked at anyone who would fail to consider
his sad plight
.LE
has level-8 stress, the final two words being annotated
as "sa\u8\dd pli\u1\dght".
However, only the first few levels are used regularly, and
it is doubtful whether acoustic distinctions are made in speech
between the weaker ones.
.pp
Chomsky and Halle are concerned to distinguish between such utterances as
.LB
.NI
bla\u2\dck boa\u1\drd-era\u3\dser    ("board eraser that is black")
.NI
bla\u1\dckboa\u3\drd era\u2\dser     ("eraser for a blackboard")
.NI
bla\u3\dck boa\u1\drd era\u2\dser    ("eraser of a black board"),
.LE
and their stress assignment rules do indeed produce each version when
appropriate.
In foot notation the distinctions can still be made:
.LB
.NI
/black /*board-eraser/
.NI
/*blackboard e/raser/
.NI
/black /*board e/raser/
.LE
.pp
The rules operate on a grammatical derivation tree
of the text.
For instance, input for the three examples would be written
.LB
.NI
[\dNP\u[\dA\u black ]\dA\u [\dN\u[\dN\u board]\dN\u
[\dN\u eraser ]\dN\u]\dN\u]\dNP\u
.NI
[\dN\u[\dN\u[\dA\u black ]\dA\u [\dN\u board ]\dN\u]\dN\u [\dN\u eraser ]\dN\u]\dN\u
.NI
[\dN\u[\dNP\u[\dA\u black ]\dA\u [\dN\u board ]\dN\u]\dNP\u [\dN\u eraser ]\dN\u]\dN\u,
.LE
representing the trees shown in Figure 9.2.
.FC "Figure 9.2"
Here, N stands for a noun, NP for a noun phrase, and A for an adjective.
These categories appear explicitly as nodes in the tree.
In the linearized textual representation they are used to label
brackets which represent the tree structure.
An additional piece of information which is needed is the lexical entry for
"eraser", which would show that it has only one accented
(that is, potentially stressed) syllable, namely, the second.
.pp
Consider now how to account for stress in prefixed and
suffixed words, and those polysyllabic ones with more than one potential
stress point.
For these, the morphological structure must appear in the input.
.pp
Now
.ul
morphemes
are well-defined minimal units of grammatical analysis from which a word
may be composed.
For example,  [went]\ =\ [go]\ +\ [ed]  is
a morphemic decomposition, where "[ed]" denotes the
past-tense morpheme.
This representation is not particularly suitable for speech synthesis
for the obvious reason that the result bears no phonetic resemblance to
the input.
What is needed is a decomposition into
.ul
morphs,
which occur only when the lexical or phonetic representation of a word may
easily be segmented into parts.
Thus  [wanting]\ =\ [want]\ +\ [ing]  and  [bigger]\ =\ [big]\ +\ [er]  are
simultaneously morphic and morphemic decompositions.
Notice that in the second example, a rule about final consonant doubling has
been applied at the lexical level (although it is not needed in
a phonetic representation):  this comes into the sphere
of "easy" segmentation.
Contrast this with  [went]\ =\ [go]\ +\ [ed]  which
is certainly not an easy segmentation and hence a
morphemic but not a morphic decomposition.
But between these extremes there are some difficult
cases:  [specific]\ =\ [specify]\ +\ [ic]  is probably morphic
as well as morphemic, but it is not clear
that  [galactic]\ =\ [galaxy]\ +\ [ic]  is.
.pp
Assuming that the input is given as a derivation tree with morphological
structure made explicit, Chomsky and Halle present rules which assign stress
correctly in nearly all cases.  For example, their rules give
.LB
.NI
[\dA\u[\dN\u incident ]\dN\u + al]\dA\u  \(em>  i\u2\dncide\u1\dntal;
.LE
and if the stem is marked by  [\dS\u\ ...\ ]\dS\u  in prefixed words,
they can deduce
.LB
.NI
[\dN\u tele [\dS\u graph ]\dS\u]\dN\u		\(em>  te\u1\dlegra\u3\dph
.NI
[\dN\u[\dN\u tele [\dS\u graph ]\dS\u]\dN\u y ]\dN\u	\(em>  tele\u1\dgraphy
.NI
[\dA\u[\dN\u tele [\dS\u graph ]\dS\u]\dN\u ic ]\dA\u	\(em>  te\u3\dlegra\u1\dphi\u2\dc.
.LE
.pp
There are two rules which account for the word-level stress
on such examples:  the "main stress"
rule and the "alternating stress" rule.
In essence, the main stress rule emphasizes the last strong syllable
of a stem.
A syllable is "strong" either if it contains one of a class of so-called
"long" vowels, or if there is a cluster of two or more consonants
following the vowel; otherwise it is "weak".
(If you are exceptionally observant you will notice that this strong\(emweak
distinction has been used before, when discussing the rhythm of feet in
syllables.)  Thus the verb "torment" receives stress on the second syllable,
for it is a strong one.
A noun like "torment" is treated as being derived from the corresponding verb,
and the rule assigns stress to the verb first and then modifies it for the noun.
The second, "alternating stress", rule gives some stress to alternate
syllables of polysyllabic words like "form\c
.ul
al\c
de\c
.ul
hyde\c
".
.pp
It is quite easy to incorporate the word-level rules into a computer
program which uses feet rather than stress levels as the basis for prosodic
description.
A foot boundary is simply placed before the primary-stressed (level-1) syllable,
except for function words, which do not begin a foot.
The other stress levels should be ignored,
except that for slow, deliberate speech, secondary (level-2) stress is
mapped into a foot boundary too, if it precedes the primary stress.
There is also a rule which reduces vowels in unstressed
syllables.
.pp
The stress assignment rules can work on phonemic script, as well as English.
For example, starting from the phonetic
form  [\d\V\u\ \c
.ul
aa\ s\ t\ o\ n\ i\ sh\ \c
]\dV\u,
the stress assignment rules
produce  \c
.ul
aa\ s\ t\ o\u1\d\ n\ i\ sh\ ;\c
  the
vowel reduction rule
generates  \c
.ul
uh\ s\ t\ o\u1\d\ n\ i\ sh\ ;\c
  and
the foot conversion process
gives  \c
.ul
uh\ s/t\ o\ n\ i\ sh.
This appears to provide a fairly reliable algorithm for foot boundary
placement.
.rh "Speech synthesis from concept."
I argued earlier that in order to derive prosodic features
of an utterance from text it
is necessary to understand its role in the dialogue, its semantics,
its syntax, and \(em as we have just seen \(em its morphological structure.
This is a very tall order, and the problem of natural language comprehension
by machine is a vast research area in its own right.
However, in many applications requiring speech output,
utterances are generated by the computer from internally stored data
rather than being read aloud from pre-prepared text.
Then the problem of comprehending text may be evaded, for
presumably the language-generation module can provide a semantic,
syntactic, and even morphological decomposition of the utterance,
as well as some indication of its role in the dialogue
(that is, why it is necessary to say it).
.pp
This forms the basis of the appealing notion of "speech synthesis from concept".
It has some advantages over speech generation from text, and in principle
should provide more natural-sounding speech.
Every word produced by the system can have a complete lexical entry which
shows its morphological decomposition and potential stress points.
The full syntactic history of each utterance is known.
The Chomsky-Halle rules described above can therefore be used to place
foot boundaries accurately, without the need for a complex parsing program
and without the risk of having to make guesses about unknown words.
.pp
However, it is not clear how to take advantage of any semantic information
which is available.  Ideally, it should be possible to place tone group
boundaries and tonic stress points, and assign intonation contours, in
a natural-sounding way.
But look again at the example text of Table 9.2 and imagine that you have
at your disposal as much semantic information as is needed.
It is
.ul
still
far from obvious how the intonation features could be assigned!
It is, in the ultimate analysis, interpretive and stylistic
.ul
choices
that add variety and interest to speech.
.pp
Take the problem of determining pitch contours, for instance.
Some of them may be explicable.
Contour 4 on
.LB
.NI
except the parts that are covered in caravans of course
.LE
is due to its being a contrastive clause, for it presents
essentially new information.
Similarly, the succession
.LB
.NI
if you go in spring
.NI
when the gorse is out
.NI
or in summer
.NI
when the heather's out
.LE
could be considered contrastive, being in the subjunctive voice, and
this could explain why contour 4's were used.
But this is all conjecture, and it is difficult to apply throughout the
passage.
Halliday (1970) explains the contexts in which each tone group is typically
used, but in an extremely high-level manner which would be impossible
to embody directly in a computer program.
.[
Halliday 1970 Course in spoken English: Intonation
.]
At the other end of the spectrum, computer systems for written
discourse production do not seem to provide the subtle information needed
to make intonation decisions (see, for example, Davey, 1978, for a fairly
complete description of such a system).
.[
Davey 1978
.]
.pp
One project which uses such a method for generating speech has been
described (Young and Fallside, 1980).
.[
Young Fallside 1980
.]
Although some attention is paid to rhythm, the intonation contours
which are generated are disappointingly repetitive and lacking in
richness.
In fact, very little semantic information is used to assign contours; really
just that inferred by the crude punctuation-driven method described
earlier.
.pp
The higher-level semantic problems associated with speech output were
studied some years go under the
title "synthetic elocution" (Vanderslice, 1968).
.[
Vanderslice 1968
.]
A set of rules was generated and tested by hand on a sample passage,
the first part of which is shown in Table 9.4.
However, no attempt was made to formalize the rules in a computer program,
and indeed it was recognized that a number of important questions,
such as the form of the semantic information assumed at the input,
had been left unanswered.
.RF
.nr x0 \w'\0\0  psychologist   '+\w'emphasis assigned because of antithesis with  '
.nr x1 (\n(.l-\n(x0)/2
.in \n(x1u
.ta \w'\0\0  psychologist   'u
\l'\n(x0u\(ul'
.sp
Human experience and human behaviour are accessible to
observation by everyone.  The psychologist tries to bring
them under systematic study.  What he perceives, however,
anyone can perceive; for his task he requires no microscope
or electronic gear.
.sp2
\0\0  word	comments
\l'\n(x0u\(ul'
.sp
\01  Human	special treatment because paragraph-initial
\04  human	accent deleted because it echoes word 1
13  psychologist	emphasis assigned because of antithesis with
	"everyone"
17  them	anaphoric to "Human experience and human
	behaviour"
19  systematic	emphasis assigned because of contrast with
	"observation"
20  study	emphasis? \(em text is ambiguous whether
	"observation" is a kind of study that is
	nonsystematic, or an activity contrasting
	with the entire concept of "systematic study"
21  What	increase in pitch for "What he perceives"
	because it is not the subject
22  he	accented although anaphoric to word 13
	because of antithesis with word 25
24  however	decrease in pitch because it is parenthetical
25  anyone	emphasized by antithesis with word 22
27  perceive	unaccented because it echoes word 23,
	"perceives"
\0\0  ;	semicolon assigns falling intonation
30  task	unaccented because it is anaphoric with
	"tries to bring them under systematic study"
\l'\n(x0u\(ul'
.ta 0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i
.in 0
.FG "Table 9.4  Sample passage and comments pertinent to synthetic elocution"
.pp
The comments in the table, which are selected and slightly edited versions
of those appearing in the original work (Vanderslice, 1968), are intended
as examples of the nature and subtlety of the prosodic influences which
were examined.
.[
Vanderslice 1968
.]
The concepts of "accent" and "emphasis" are used; these relate to stress
but are not easy to define precisely in our tone-group terminology.
Fortunately we do not need an exact characterization of them for the present
purpose.
Roughly speaking, "accent" encompasses both foot-initial stress and
tonic stress, whereas "emphasis" is something more than this,
typically being realized by the fall-rise or rise-fall contours of
Halliday's tone groups 4 and 5 (Figure 8.5).
.pp
Particular attention is paid to anaphora and antithesis (amongst other things).
The first term means the repetition of a word or phrase in the text,
and is often applied to pronoun references.
In the example, the word "human" is repeated in the first few words;
"them" in the second sentence refers to "human experience and human
behaviour"; "he" in the third sentence is the previously-mentioned
psychologist; and "task" is anaphoric with "tries to bring them under
systematic study".
Other things being equal, anaphoric references are unaccented.
In our terms this means that they certainly do not receive tonic stress
and may not even receive foot stress.
.pp
Antithesis is defined as the contrast of ideas expressed by parallelism of
strongly contrasting words or phrases; and the second element taking part
in it is generally emphasized.
"Psychologist" in the passage is an antithesis of "everyone";
"systematic" and possibly "study" of "observation".
Thus
.LB
.NI
/^ the psy/*chologist
.LE
would probably receive intonation contour 4, since it is also introducing
a new actor; while
.LB
.NI
/tries to /bring them /under /system/*matic /study
.LE
could receive contour 5.
"He" and "everyone" are antithetical; not only does the latter receive
emphasis but the former has its accent restored \(em for otherwise
it would have been removed because of anaphora with "psychologist".
Hence it will certainly begin a foot, possibly a tonic foot.
.pp
A factor that does not affect the sample passage is the accentuation
of unusual syllables of similar words to bring out a contrast.
For example,
.LB
.NI
he went
.ul
out\c
side, not
.ul
in\c
side.
.LE
Although this may seem to be just another facet of antithesis,
Vanderslice points out that it is phonetic rather than structural
similarity that is contrasted:
.LB
.NI
I said
.ul
de\c
plane, not
.ul
com\c
plain.
.LE
This introduces an interesting interplay between the phonetic and
prosodic levels.
.pp
Anaphora and antithesis provide an ideal domain for speech synthesis from
concept.
Determining them from plain text is a very difficult problem,
requiring a great deal of real-world knowledge.
The first has received some attention in the field of natural language
understanding.
Finding pronoun referents is an important problem for language translation,
for their gender is frequently distinguished in, say, French where it is not
in English.
Examples such as
.LB
.NI
I bought the wine, sat on a table, and drank it
.NI
I bought the wine, sat on a table, and broke it
.LE
have been closely studied (Wilks, 1975); for if they were to be translated
into French the pronoun "it" would be rendered differently in each case
(\c
.ul
le
vin,
.ul
la
table).
.[
Wilks 1975 An intelligent analyzer and understander of English
.]
.pp
In spoken language, emphasis is used to indicate the referent of a pronoun
when it would not otherwise be obvious.
Vanderslice gives the example
.LB
.NI
Bill saw John across the room and he ran over to him
.NI
Bill saw John across the room and
.ul
he
ran over to
.ul
him,
.LE
where the emphasis reverses the pronoun referents
(so that John did the running).
He suggests accenting a personal pronoun whenever the true
antecedent is not the same as the "unmarked" or default one.
Unfortunately he does not elaborate on what is meant by "unmarked".
Does it mean that the referent cannot be predicted from
knowledge of the words alone \(em as in the second example above?
If so, this is a clear candidate for speech synthesis from concept,
for the distinction cannot be made from text! 
.sh "9.2  Pronunciation"
.pp
English pronunciation is notoriously irregular.
A poem by Charivarius, the pseudonym of a Dutch high school teacher
and linguist G.N.Trenite (1870\-1946), surveys the problems in an amusing
way and is worth quoting in full.
.br
.ev2
.in 0
.LB "nnnnnnnnnnnnnnnn"
.ul
              The Chaos
.sp2
.ne4
Dearest creature in Creation
Studying English pronunciation,
.in +5n
I will teach you in my verse
Sounds like corpse, corps, horse and worse.
.ne4
.in -5n
It will keep you, Susy, busy,
Make your head with heat grow dizzy;
.in +5n
Tear in eye your dress you'll tear.
So shall I!  Oh, hear my prayer:
.ne4
.in -5n
Pray, console your loving poet,
Make my coat look new, dear, sew it.
.in +5n
Just compare heart, beard and heard,
Dies and diet, lord and word.
.ne4
.in -5n
Sword and sward, retain and Britain,
(Mind the latter, how it's written).
.in +5n
Made has not the sound of bade,
Say \(em said, pay \(em paid, laid, but plaid.
.ne4
.in -5n
Now I surely will not plague you
With such words as vague and ague,
.in +5n
But be careful how you speak:
Say break, steak, but bleak and streak,
.ne4
.in -5n
Previous, precious; fuchsia, via;
Pipe, shipe, recipe and choir;
.in +5n
Cloven, oven; how and low;
Script, receipt; shoe, poem, toe.
.ne4
.in -5n
Hear me say, devoid of trickery;
Daughter, laughter and Terpsichore;
.in +5n
Typhoid, measles, topsails, aisles;
Exiles, similes, reviles;
.ne4
.in -5n
Wholly, holly; signal, signing;
Thames, examining, combining;
.in +5n
Scholar, vicar and cigar,
Solar, mica, war and far.
.ne4
.in -5n
Desire \(em desirable, admirable \(em admire;
Lumber, plumber; bier but brier;
.in +5n
Chatham, brougham; renown but known,
Knowledge; done, but gone and tone,
.ne4
.in -5n
One, anemone; Balmoral,
Kitchen, lichen; laundry, laurel;
.in +5n
Gertrude, German; wind and mind;
Scene, Melpemone, mankind;
.ne4
.in -5n
Tortoise, turquoise, chamois-leather,
Reading, Reading; heathen, heather.
.in +5n
This phonetic labyrinth
Gives:  moss, gross; brook, brooch; ninth, plinth.
.ne4
.in -5n
Billet does not end like ballet;
Bouquet, wallet, mallet, chalet;
.in +5n
Blood and flood are not like food,
Nor is mould like should and would.
.ne4
.in -5n
Banquet is not nearly parquet,
Which is said to rime with darky
.in +5n
Viscous, viscount; load and broad;
Toward, to forward, to reward.
.ne4
.in -5n
And your pronunciation's O.K.
When you say correctly:  croquet;
.in +5n
Rounded, wounded; grieve and sieve;
Friend and fiend, alive and live
.ne4
.in -5n
Liberty, library; heave and heaven;
Rachel, ache, moustache; eleven.
We say hallowed, but allowed;
People, leopard; towed, but vowed.
.in +5n
Mark the difference moreover
Between mover, plover, Dover;
.ne4
.in -5n
Leeches, breeches; wise, precise;
Chalice, but police and lice.
.in +5n
Camel, constable, unstable,
Principle, discipline, label;
.ne4
.in -5n
Petal, penal and canal;
Wait, surmise, plait, promise; pal.
.in +5n
Suit, suite, ruin; circuit, conduit,
Rime with:  "shirk it" and "beyond it";
.ne4
.in -5n
But it is not hard to tell
Why it's pall, mall, but Pall Mall.
.in +5n
Muscle, muscular; goal and iron;
Timber, climber; bullion, lion;
.ne4
.in -5n
Worm and storm; chaise, chaos, chair;
Senator, spectator, mayor.
.in +5n
Ivy, privy; famous, clamour
and enamour rime with "hammer".
.ne4
.in -5n
Pussy, hussy and possess,
Desert, but dessert, address.
.in +5n
Golf, wolf; countenants; lieutenants
Hoist, in lieu of flags, left pennants.
.ne4
.in -5n
River, rival; tomb, bomb, comb;
Doll and roll, and some and home.
.in +5n
Stranger does not rime with anger,
Neither does devour with clangour.
.ne4
.in -5n
Soul, but foul; and gaunt, but aunt;
Font, front, won't; want, grand and grant;
.in +5n
Shoes, goes, does.  Now first say:  finger,
And then; singer, ginger, linger.
.ne4
.in -5n
Real, zeal; mauve, gauze and gauge;
Marriage, foliage, mirage, age.
.in +5n
Query does not rime with very,
Nor does fury sound like bury.
.ne4
.in -5n
Dost, lost, post; and doth, cloth, loth;
Job, Job; blossom, bosom, oath.
.in +5n
Though the difference seems little
We say actual, but victual;
.ne4
.in -5n
Seat, sweat; chaste, caste; Leigh, eight, height;
Put, nut; granite but unite.
.in +5n
Reefer does not rime with deafer,
Feoffer does, and zephyr, heifer.
.ne4
.in -5n
Dull, bull; Geoffrey, George; ate, late;
Hint, pint; senate, but sedate.
.in +5n
Scenic, Arabic, Pacific;
Science, conscience, scientific.
.ne4
.in -5n
Tour, but our, and succour, four;
Gas, alas and Arkansas!
.in +5n
Sea, idea, guinea, area,
Psalm, Maria, but malaria.
.ne4
.in -5n
Youth, south, southern; cleanse and clean;
Doctrine, turpentine, marine.
.in +5n
Compare alien with Italian.
Dandelion with battalion,
.ne4
.in -5n
Sally with ally, Yea, Ye,
Eye, I, ay, aye, whey, key, quay.
Say aver, but ever, fever,
Neither, leisure, skein, receiver.
.in +5n
Never guess \(em it is not safe;
We say calves, valves, half, but Ralf.
.ne4
.in -5n
Heron, granary, canary;
Crevice and device and eyrie;
.in +5n
Face, preface, but efface,
Phlegm, phlegmatic; ass, glass, bass;
.ne4
.in -5n
Large, but target, gin, give, verging;
Ought, out, joust and scour, but scourging;
.in +5n
Ear, but earn; and wear and tear
Do not rime with "here", but "ere".
.ne4
.in -5n
Seven is right, but so is even;
Hyphen, roughen, nephew, Stephen;
.in +5n
Monkey, donkey; clerk and jerk;
Asp, grasp, wasp; and cork and work.
.ne4
.in -5n
Pronunciation \(em think of psyche -
Is a paling, stout and spikey;
.in +5n
Won't it make you lose your wits,
Writing groats and saying "groats"?
.ne4
.in -5n
It's a dark abyss or tunnel,
Strewn with stones, like rowlock, gunwale,
.in +5n
Islington and Isle of Wight,
Housewife, verdict and indict.
.ne4
.in -5n
Don't you think so, reader, rather
Saying lather, bather, father?
.in +5n
Finally:  which rimes with "enough",
Though, through, plough, cough, hough or tough?
.ne4
.in -5n
Hiccough has the sound of "cup",
My advice is ... give it up!
.LE "nnnnnnnnnnnnnnnn"
.br
.ev
.rh "Letter-to-sound rules."
Despite such irregularities, it is surprising how much can be done
with simple letter-to-sound rules.
These specify phonetic equivalents of word fragments and single letters.
The longest stored fragment which matches the current word is translated,
and then the same strategy is adopted on the remainder of the word.
Table 9.5 shows some English fragments and their pronunciations.
.RF
.nr x0 1.5i+\w'pronunciation  '
.nr x1 (\n(.l-\n(x0)/2
.in \n(x1u
.ta 1.5i
fragment	pronunciation
\l'\n(x0u\(ul'
.sp
-p-	\fIp\fR
-ph-	\fIf\fR
-phe|	\fIf ee\fR
-phe|s	\fIf ee z\fR
-phot-	\fIf uh u t\fR
-place|-	\fIp l e i s\fR
-plac|i-	\fIp l e i s i\fR
-ple|ment-	\fIp l i m e n t\fR
-plie|-	\fIp l aa i y\fR
-post	\fIp uh u s t\fR
-pp-	\fIp\fR
-pp|ly-	\fIp l ee\fR
-preciou-	\fIp r e s uh\fR
-proce|d-	\fIp r uh u s ee d\fR
-prope|r-	\fIp r o p uh r\fR
-prov-	\fIp r uu v\fR
-purpose-	\fIp er p uh s\fR
-push-	\fIp u sh\fR
-put	\fIp u t\fR
-puts	\fIp u t s\fR
\l'\n(x0u\(ul'
.ta 0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i
.in 0
.FG "Table 9.5  Word fragments and their pronunciations"
.pp
It is sometimes important to specify that a rule applies only when
the fragment is matched at the beginning or end of a word.
In the Table "-" means that other fragments can precede or follow this
one.
The "|" sign is used to separate suffixes from a word stem,
as will be explained
shortly.
.pp
An advantage of the longest-string search strategy is that it is easy
to account for exceptions simply by incorporating them into the fragment
table.
If they occur in the input, the complete word will automatically be
matched first, before any fragment of it is translated.
The exception list of complete words can be surprisingly small for
quite respectable performance.
Table 9.6 shows the entire dictionary for an excellent early pronunciation
system written at Bell Laboratories (McIlroy, 1974).
.[
McIlroy 1974
.]
Some of the words are notorious exceptions in English, while others are
included simply because the rules would run amok on them.
Notice that the exceptions are all quite short, with only a few of them
having more than two syllables.
.RF
.nr x1 0.9i+0.9i+0.9i+0.9i+0.9i+0.9i
.nr x1 (\n(.l-\n(x1)/2
.in \n(x1u
.ta 0.9i +0.9i +0.9i +0.9i +0.9i
a	doesn't	guest	meant	reader	those
alkali	doing	has	moreover	refer	to
always	done	have	mr	says	today
any	dr	having	mrs	seven	tomorrow
april	early	heard	nature	shall	tuesday
are	earn	his	none	someone	two
as	eleven	imply	nothing	something	upon
because	enable	into	nowhere	than	very
been	engine	is	nuisance	that	water
being	etc	island	of	the	wednesday
below	evening	john	on	their	were
body	every	july	once	them	who
both	everyone	live	one	there	whom
busy	february	lived	only	thereby	whose
copy	finally	living	over	these	woman
do	friday	many	people	they	women
does	gas	maybe	read	this	yes
.ta 0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i
.in 0
.FG "Table 9.6  Exception table for a simple pronunciation program"
.pp
Special action has to be taken with final "e"'s.
These lengthen and alter the quality
of the preceding vowel, so that "bit" becomes "bite" and so on.
Unfortunately, if the word has a suffix the "e" must be detected even though
it is no longer final, as in "lonely", and it is even dropped sometimes
("biting") \(em otherwise these would be pronounced "lonelly", "bitting".
To make matters worse the suffix may be another word:  we do not
want "kiteflying" to have an extra syllable which rhymes with "deaf"!
Although simple procedures can be developed to take care of common
word endings like "-ly", "-ness", "-d", it is difficult to decompose
compound words like "wisecrack" and "bumblebee" reliably \(em but this must
be done if they are not to be articulated with three syllables instead of two.
Of course, there are exceptions to the final "e" rule.
Many common words ("some", "done", "[live]\dV\u") disobey the rule by not
lengthening the main vowel, while in other, rarer, ones ("anemone",
"catastrophe", "epitome") the final "e" is actually pronounced.
There are also some complete anomalies ("fete").
.pp
McIlroy's (1974) system is a superb example of a robust program which takes
a pragmatic approach to these problems, accepting that they will never be
fully solved, and which is careful to degrade
gracefully when stumped.
.[
McIlroy 1974
.]
The pronunciation of each word is found by a succession of increasingly
desperate trials:
.LB
.NP
replace upper- by lower-case letters, strip punctuation, and try again;
.NP
remove final "-s", replace final "ie" by "y", and try again;
.NP
reject a word without a vowel;
.NP
repeatedly mark any suffixes with "|";
.NP
mark with "|" probable morph divisions in compound words;
.NP
mark potential long vowels indicated by "e|",
and long vowels elsewhere in the word;
.NP
mark voiced medial "s" as in "busy", "usual";
replace final "-s" if stripped;
.NP
scanning the word from left to right, apply letter-to-sound rules
to word fragments;
.NP
when all else fails spell the word, punctuation and all
(burp on letters for which no spelling rule exists).
.LE
.RF
.nr x0 \w'| ment\0\0\0'+\w'replace final ie by y\0\0\0'+\w'except when no vowel would remain in  '
.nr x1 (\n(.l-\n(x0)/2
.in \n(x1u
.ta \w'| ment\0\0\0'u +\w'replace final ie by y\0\0\0'u
suffix	action	notes and exceptions
\l'\n(x0u\(ul'
.sp
s	strip off final s	except in context us
\&'	strip off final '
ie	replace final ie by y
e	replace final e by E	when it is the only vowel in a word
	(long "e")

| able	place suffix mark as	except when no vowel would remain in
| ably	shown	the rest of the word
e | d
e | n
e | r
e | ry
e | st
e | y
| ful
| ing
| less
| ly
| ment
| ness
| or

| ic	place suffix mark as
| ical	shown and terminate
e |	final e processing
\l'\n(x0u\(ul'
.ta 0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i
.in 0
.FG "Table 9.7  Rules for detecting suffixes for final 'e' processing"
.pp
Table 9.7 shows the suffixes which the program recognizes, with some comments
on their processing.
Multiple suffixes are detected and marked in words like
"force|ful|ly" and "spite|ful|ness".
This allows silent "e"'s to be spotted even when they occur far back in a
word.
Notice that the suffix marks are available to the word-fragment
rules of Table 9.5, and are frequently used by them.
.pp
The program has some
.ul
ad hoc
rules for dealing with compound words like "race|track", "house|boat";
these are applied as well as normal suffix splitting so that multiple
decompositions like "pace|make|r" can be accomplished.
The rules look for short letter sequences which do not
usually appear in monomorphemic words.
It is impossible, however, to detect every morph boundary
by such rules, and the program inevitably makes mistakes.
Examples of boundaries which go undetected are
"edge|ways", "fence|post", "horse|back", "large|mouth", "where|in";
while boundaries are incorrectly inserted into "comple|mentary",
"male|volent", "prole|tariat", "Pame|la".
.pp
We now seem to have presented two opposing points of view on the pronunciation
problem.
Charivarius, the Dutch poet, shows that an enormous number of
exceptional words exist; whereas McIlroy's program makes do with a tiny
exception dictionary.
These views can be reconciled by noting that most of Charivarius' words
are relatively uncommon.
McIlroy tested his program against the 2000 most frequent words in a large
corpus (Kucera and Francis, 1967),
and found that 97% were pronounced correctly if word frequencies were
taken into account.
.[
Kucera Francis 1967
.]
(The notion of "correctness" is of course a rather subjective one.)  However,
he estimated that on the remaining words the success rate was only 88%.
.pp
The system is particularly impressive in that it is prepared to say
anything:  if used, for example, on source programs in a high-level
computer language it will say the keywords and pronouncable
identifiers, spell the other identifiers, and even give the names of special
symbols (like +, <, =) correctly!
.rh "Morphological analysis."
The use of letter-to-sound rules provides a cheap and fast technique
for pronunciation \(em the fragment table and exception dictionary for the
program described above occupy only 11 Kbyte of storage, and can easily
be kept in solid-state read-only memory.
It produces reasonable results if careful attention is paid to rules
for suffix-splitting.
However, it is inherently limited because it is not possible in general
to detect compound words by simple rules which operate on the lexical
structure of the word.
.pp
Compounds can only be found reliably by using a morph dictionary.
This gives the added advantage that syntactic information
can be stored with the morphs to assist with rhythm assignment according
to the Chomsky-Halle theory.
However, it was noted earlier that morphs, unlike the grammatically-determined
morphemes, are not very well defined from a linguistic point of view.
Some morphemic decompositions are obviously not morphic because the
constituents do not in any way resemble the final word;
while others, where the word is simply a concatenation
of its components, are clearly morphic.
Between these extremes lies a hazy region where what one considers
to be a morph depends upon how complex one is prepared to make the
concatenation rules.
The following description draws on techniques used in a project at MIT
in which a morph-based pronunciation system has been implemented
(Lee, 1969; Allen, 1976).
.[
Lee 1969
.]
.[
Allen 1976 Synthesis of speech from unrestricted text
.]
.pp
Estimates of the number of morphs in English vary from 10,000 to 30,000.
Although these seem to be very large numbers, they are considerably less
than the number of words in the language.
For example, Webster's
.ul
New Collegiate Dictionary
(7'th edition) contains about 100,000 entries.
If all forms of the words were included, this number would probably
double.
.pp
There are several classes of morphs, with restrictions on the combinations
that occur.
A general word has prefixes, a root, and suffixes, as shown in Figure 9.3;
only the root is mandatory.
.FC "Figure 9.3"
Suffixes usually perform a grammatical role, affecting the
conjugation of a verb or declension of a noun; or transforming one
part of speech into another
("-al" can make a noun into an adjective, while "-ness" performs the reverse
transformation.)  Other
suffixes, such as "-dom" or "-ship", only apply to certain parts of
speech (nouns, in this case), but do not change the grammatical
role of the word.  Such suffixes, and all prefixes, alter the meaning
of a word.
.pp
Some root morphs cannot combine with other morphs but always stand
alone \(em for instance, "this".
Others, called free morphs, can either occur on their own or combine
with further morphs t